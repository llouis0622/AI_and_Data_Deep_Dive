{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 0. NLP이냐 텍스트 분석이냐\n",
    "\n",
    "- NLP(National Language Processing) : 머신이 인간의 언어를 이해하고 해석하는 데 중점을 두고 발전\n",
    "- 텍스트 분석(Text Analysis) : 텍스트 마이닝(Text Mining), 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 중점을 두고 발전\n",
    "- 텍스트 분류(Text Classification) : 문서가 특정 분류 또는 카테고리에 속하는 것을 예측하는 기법\n",
    "- 감성 분석(Sentiment Analysis) : 텍스트에서 나타나는 감정/판단/믿음/의견/기분 등의 주관적인 요소를 분석하는 기법\n",
    "- 텍스트 요약(Summarization) : 텍스트 내에서 중요한 주제나 중심 사상을 추출하는 기법\n",
    "- 텍스트 군집화(Clustering)와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화를 수행하는 기법\n",
    "\n",
    "# 1. 텍스트 분석 이해\n",
    "\n",
    "- 비정형 데이터인 텍스트를 분석하는 것\n",
    "- 피처 벡터화(Feature Vectorization) : 텍스트를 word 기반의 다수의 피처로 추출, 피처에 단어 빈도수와 같은 숫자 값 부여\n",
    "- BOW(Bag of Words), Word2Vec\n",
    "\n",
    "## 1. 텍스트 분석 수행 프로세스\n",
    "\n",
    "- 텍스트 사전 준비작업(텍스트 전처리) : 클렌징, 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업, 의미 없는 단어 제거 작업, 어근 추출 등의 텍스트 정규화\n",
    "- 피처 벡터화/추출 : 가공된 텍스트에서 피처를 추출하고 벡터값 할당\n",
    "- ML 모델 수립 및 학습/예측/평가 : 피처 벡터화된 데이터 세트에 ML 모델을 적용해 학습/예측 및 평가 수행\n",
    "\n",
    "## 2. 파이썬 기반의 NLP, 텍스트 분석 패키지\n",
    "\n",
    "- NLTK(Natural Language Toolkit for Python) : 가장 대표적인 NLP 패키지, 방대한 데이터 세트와 서브 모듈 보유, 거의 모든 영역 커버 가능\n",
    "- Gensim : 토픽 모델링 분야에서 가장 두각을 나타내는 패키지, Word2Vec 구현 가능\n",
    "- SpaCy : 최근 가장 주목을 받는 패키지, 뛰어난 수행 성능\n",
    "\n",
    "# 2. 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화\n",
    "\n",
    "- 텍스트를 머신러닝 알고리즘과 NLP 애플리케이션에 입력 데이터로 사용하기 위해 클렌징, 정제, 토큰화, 어근화 등의 다양한 텍스트 데이터의 사전 작업 수행\n",
    "    - 클렌징(Cleansing)\n",
    "    - 토큰화(Tokenization)\n",
    "    - 필터링/스톱 워드 제거/철자 수정\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "\n",
    "## 1. 클렌징\n",
    "\n",
    "- 텍스트에서 분석에 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업\n",
    "\n",
    "## 2. 텍스트 토큰화\n",
    "\n",
    "- 문장 토큰화(Sentence Tokenization) : 문장의 마침표, 개행문자 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것\n",
    "- `sent_tokenize()` : 문장 토큰화\n",
    "- 단어 토큰화(Word Tokenization) : 문자를 공백, 콤마, 마침표, 개행문자 등으로 단어 분리\n",
    "- `word_tokenize()` : 단어 토큰화\n",
    "\n",
    "## 3. 스톱 워드 제거\n",
    "\n",
    "- 분석에 큰 의미가 없는 단어를 제거하는 것\n",
    "- 문장을 구성하는 필수 문법 요소지만 문맥적으로 큰 의미가 없는 단어\n",
    "\n",
    "## 4. Stemming과 Lemmatization\n",
    "\n",
    "- 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 것\n",
    "- Stemming : 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법 적용 → 원래 단어에서 일부 철자가 훼손된 어근 단어 추출\n",
    "- Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한철자로 된 어근 단어 찾기\n",
    "\n",
    "# 3. Bag of Words - BOW\n",
    "\n",
    "- 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델\n",
    "- 단순히 단어 발생 횟수에 기반 → 예상보다 문서의 특징을 잘 나타냄\n",
    "- 문맥 의미 반영 부족 : 순서 미고려 → 문장 내에서 단어의 문맥적인 의미 무시\n",
    "- 희소 행렬 문제 : 피처 벡터화 수행 → 희소 행렬 형태의 데이터 세트 생성\n",
    "\n",
    "## 1. BOW 피처 벡터화\n",
    "\n",
    "- 텍스트를 특정 의미를 가지는 숫자형 값인 벡터 값으로 변환\n",
    "- 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것\n",
    "- 카운트 기반 벡터화 : 해당 단어가 나타나는 횟수를 부여\n",
    "- TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화 : 개별 문서에서 자주 나타나는 단어에 높은 가중치를 두되 모든 문서에서 전반적으로 자주 나타나는 단어는 페널티를 주는 것\n",
    "\n",
    "## 2. 사이킷런의 Count 및 TF-IDF 벡터화 구현 : CountVectorizer, TfidfVectorizer\n",
    "\n",
    "- `CountVectorizer` 클래스\n",
    "    - `max_df` : 전체 문서에 걸쳐 너무 높은 빈도수를 가지는 단어 피처 제외\n",
    "    - `min_df` : 전체 문서에 걸쳐 너무 낮은 빈도수를 가지는 단어 피처 제외\n",
    "    - `max_features` : 추출하는 피처의 개수 제한\n",
    "    - `stop_words` : 스톱 워드로 지정된 단어는 추출에서 제외\n",
    "    - `n_gram_range` : 단어 순서를 어느정도 보강하기 위한 범위 설정\n",
    "    - `anaylzer` : 피처 추출을 수행한 단위 지정\n",
    "    - `token_pattern` : 토큰화를 수행하는 정규 표현식 패턴 지정\n",
    "    - `tokenizer` : 토큰화를 별도의 커스텀 함수로 이용\n",
    "- `TfidfVectorizer` 클래스\n",
    "\n",
    "## 3. 희소 행렬 - COO 형식\n",
    "\n",
    "- 0이 아닌 데이터만 별도의 데이터 배열에 저장, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장\n",
    "- `sparse` 패키지\n",
    "- `coo_matrix()`\n",
    "\n",
    "## 4. 희소 행렬 - CSR(Compressed Sparse Row) 형식\n",
    "\n",
    "- COO 형식이 행과 열의 위치를 나타내기 위해서 반복적인 위치 데이터를 사용해야 하는 문제점 해결\n",
    "- 행 위치 배열 내에 있는 고유한 값의 시작 위치만 다시 별도의 위치 배열로 가지는 변환 방식\n",
    "- `csr_matrix`\n",
    "\n",
    "# 4. 감성 분석(Sentiment Analysis)\n",
    "\n",
    "- 문서의 주관적인 감성/의견/감정/기분 등을 파악하기 위한 방법\n",
    "- 문서 내 텍스트가 나타내는 여러 가지 주관적인 단어와 문맥을 기반으로 감성 수치를 계산\n",
    "- 지도학습 → 학습 데이터와 타깃 레이블 값을 기반으로 감성 분석 학습 수행\n",
    "- 비지도학습 → Lexicon 감성 어휘 사전 이용\n",
    "    - 긍정 감성 + 부정 감성 수치 보유\n",
    "    - WordNet : 시맨틱 분석을 제공하는 어휘 사전\n",
    "- 감성 사전\n",
    "    - SentiWordNet : 감성 단어 전용의 WordNet 구현\n",
    "    - VADER : 소셜 미디어의 텍스트에 대한 감성 분석을 제공하기 위한 패키지\n",
    "    - Pattern : 예측 성능 측면에서 가장 주목받는 패키지\n",
    "\n",
    "# 5. 토픽 모델링(Topic Modeling)\n",
    "\n",
    "- 문서 집합에 숨어 있는 주제를 찾아내는 것\n",
    "- 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출\n",
    "- LSA(Latent Semantic Analysis)\n",
    "- LDA(Latent Dirichlet Allocation)\n",
    "\n",
    "# 6. 문서 군집화(Document Clustering)\n",
    "\n",
    "- 비슷한 텍스트 구성의 문서를 군집화하는 것\n",
    "- 동일한 군집에 속하는 문서를 같은 카테고리 소속으로 분류 가능\n",
    "- 학습 데이터 세트가 필요없는 비지도학습 기반\n",
    "- 핵심 단어를 주축으로 군집화\n",
    "\n",
    "# 7. 문서 유사도\n",
    "\n",
    "- 코사인 유사도(Cosine Similarity) : 벡터와 벡터 간의 유사도를 비교할 때 벡터의 크기보다 벡터의 상호 방향성이 얼마나 유사한지 기반으로 비교\n",
    "- 두 벡터 사이의 사잇각을 구해 얼마나 유사한지 수치로 적용한 것\n",
    "    - 유사 벡터\n",
    "    - 관련성이 없는 벡터\n",
    "    - 반대 관계 벡터\n",
    "- 두 벡터 내적을 총 벡터 크기의 합으로 나눈 것\n",
    "\n",
    "# 8. 한글 텍스트 처리\n",
    "\n",
    "- 띄어쓰기 + 다양한 조사 → 한글 NLP 처리 어려움\n",
    "- KoNLPy : 파이썬의 대표적인 한글 형태소 패키지"
   ],
   "id": "a5f75fd44ea2fa9"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-08T01:39:55.532057Z",
     "start_time": "2024-06-08T01:39:55.528637Z"
    }
   },
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/llouis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:41:27.831424Z",
     "start_time": "2024-06-08T01:41:27.827856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ],
   "id": "f05cde24df2b5757",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:41:46.946475Z",
     "start_time": "2024-06-08T01:41:46.942818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ],
   "id": "570c11a69e0951b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:41:55.546244Z",
     "start_time": "2024-06-08T01:41:55.107408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "id": "6f9119fc497bcd7f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/llouis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:42:05.951585Z",
     "start_time": "2024-06-08T01:42:05.947371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('영어 stop words 갯수 : ', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ],
   "id": "649ebfd9a3f8bac7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수 :  179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:42:25.014337Z",
     "start_time": "2024-06-08T01:42:25.010674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "print(all_tokens)"
   ],
   "id": "5ff188dca1e3db22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:42:32.791622Z",
     "start_time": "2024-06-08T01:42:32.787964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ],
   "id": "7ede62ccd4194b7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:42:44.072235Z",
     "start_time": "2024-06-08T01:42:43.035093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ],
   "id": "df03d4f36a8e61fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/llouis/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:42:52.602152Z",
     "start_time": "2024-06-08T01:42:52.599694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])"
   ],
   "id": "72afac8af0640b05",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:43:06.924127Z",
     "start_time": "2024-06-08T01:43:06.920851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy import sparse\n",
    "\n",
    "data = np.array([3, 1, 2])\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ],
   "id": "6bf0b4a3ecda77bd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:43:13.896471Z",
     "start_time": "2024-06-08T01:43:13.893310Z"
    }
   },
   "cell_type": "code",
   "source": "sparse_coo.toarray()",
   "id": "d8fe667d253ab71a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:43:33.281634Z",
     "start_time": "2024-06-08T01:43:33.276122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                   [1, 4, 0, 3, 2, 5],\n",
    "                   [0, 6, 0, 3, 0, 0],\n",
    "                   [2, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 7, 0, 8],\n",
    "                   [1, 0, 0, 0, 0, 0]])\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ],
   "id": "6a25bd5fdba8c8db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T01:43:41.753538Z",
     "start_time": "2024-06-08T01:43:41.749931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dense3 = np.array([[0, 0, 1, 0, 0, 5],\n",
    "                   [1, 4, 0, 3, 2, 5],\n",
    "                   [0, 6, 0, 3, 0, 0],\n",
    "                   [2, 0, 0, 0, 0, 0],\n",
    "                   [0, 0, 0, 7, 0, 8],\n",
    "                   [1, 0, 0, 0, 0, 0]])\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ],
   "id": "afe3b7292cf175ff",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:39:08.249603Z",
     "start_time": "2024-06-09T13:38:46.756253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='train', random_state=156)"
   ],
   "id": "a5aa94e41169bff8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:39:12.194904Z",
     "start_time": "2024-06-09T13:39:12.192039Z"
    }
   },
   "cell_type": "code",
   "source": "print(news_data.keys())",
   "id": "cf6a214a0d933455",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:39:30.633709Z",
     "start_time": "2024-06-09T13:39:29.666205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도 ', pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('target 클래스의 이름들 ', news_data.target_names)"
   ],
   "id": "4c9643c864d6bc70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도  0     480\n",
      "1     584\n",
      "2     591\n",
      "3     590\n",
      "4     578\n",
      "5     593\n",
      "6     585\n",
      "7     594\n",
      "8     598\n",
      "9     597\n",
      "10    600\n",
      "11    595\n",
      "12    591\n",
      "13    594\n",
      "14    593\n",
      "15    599\n",
      "16    546\n",
      "17    564\n",
      "18    465\n",
      "19    377\n",
      "Name: count, dtype: int64\n",
      "target 클래스의 이름들  ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:39:36.288345Z",
     "start_time": "2024-06-09T13:39:36.285947Z"
    }
   },
   "cell_type": "code",
   "source": "print(news_data.data[0])",
   "id": "b7fc09a4325e4877",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dlc@umcc.umcc.umich.edu (David Claytor)\n",
      "Subject: Re: When is Apple going to ship CD300i's?\n",
      "Organization: UMCC, Ann Arbor, MI\n",
      "Lines: 43\n",
      "NNTP-Posting-Host: umcc.umcc.umich.edu\n",
      "\n",
      "In article <1r00fdINNddt@senator-bedfellow.MIT.EDU> thewho@athena.mit.edu (Derek A Fong) writes:\n",
      ">\n",
      ">Interestingly enough, the CDROM 300i that came with my Quadra 800 has \n",
      ">only 8 disks:\n",
      ">\n",
      ">1. System Install\n",
      ">2. Kodak Photo CD sampler\n",
      ">3. Alice to Ocean\n",
      ">4. CDROM Titles\n",
      ">5. Application Demos\n",
      ">6. Mozart: Dissonant Quartet\n",
      ">7. Nautilus\n",
      ">8. Apple Chronicles\n",
      ">\n",
      ">Has anyone else noticed that they got less than everyone seems to be\n",
      ">getting with the external?  What I really feel I missed out on is what\n",
      ">is supposed to a fantastic Games demo disk.\n",
      ">\n",
      ">I have heard that people have gotten up to 9-10 disks with their drive.\n",
      ">I assume they get the 8 titles above plus Cinderella and the Games Demo CDROM.\n",
      ">\n",
      ">any comments and experiences?  Should I call Apple to complain? =)\n",
      ">\n",
      ">Derek\n",
      ">\n",
      ">\n",
      ">thewho@plume.mit.edu\n",
      "\n",
      "\n",
      "What I did NOT get with my drive (CD300i) is the System Install CD you\n",
      "listed as #1.  Any ideas about how I can get one?  I bought my IIvx 8/120\n",
      "from Direct Express in Chicago (no complaints at all -- good price & good\n",
      "service).\n",
      "\n",
      "BTW, I've heard that the System Install CD can be used to boot the mac;\n",
      "however, my drive will NOT accept a CD caddy is the machine is off.  How can\n",
      "you boot with it then?\n",
      "\n",
      "--Dave\n",
      "\n",
      "-- \n",
      "                           dlc@umcc.ais.org  313.485.3394\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:39:54.640880Z",
     "start_time": "2024-06-09T13:39:52.985589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "print(type(X_train))\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print('학습 데이터 크기 {0} , 테스트 데이터 크기 {1}'.format(len(train_news.data), len(test_news.data)))"
   ],
   "id": "481d0123e1747f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "학습 데이터 크기 11314 , 테스트 데이터 크기 7532\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:40:15.703571Z",
     "start_time": "2024-06-09T13:40:13.423069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt_vect = CountVectorizer()\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "print('학습 데이터 Text의 CountVectorizer Shape : ', X_train_cnt_vect.shape)"
   ],
   "id": "f1204eaa7583ea61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 Text의 CountVectorizer Shape :  (11314, 101631)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:41:27.525806Z",
     "start_time": "2024-06-09T13:40:25.952011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ],
   "id": "2e9543214a7ec236",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression 의 예측 정확도는 0.617\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:41:36.196062Z",
     "start_time": "2024-06-09T13:41:27.529886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ],
   "id": "f3ee2733d655f9dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression 의 예측 정확도는 0.678\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:41:59.688269Z",
     "start_time": "2024-06-09T13:41:36.207595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect, y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ],
   "id": "2b1135b8a7f25ece",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.690\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:46:18.452925Z",
     "start_time": "2024-06-09T13:41:59.692446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'C': [0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_lr.fit(X_train_tfidf_vect, y_train)\n",
    "print('Logistic Regression best C parameter : ', grid_cv_lr.best_params_)\n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ],
   "id": "c47e241b1b040647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Logistic Regression best C parameter :  {'C': 10}\n",
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T13:46:47.041717Z",
     "start_time": "2024-06-09T13:46:18.454229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ],
   "id": "55260d066ce1807b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline을 통한 Logistic Regression 의 예측 정확도는 0.704\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:10:07.564570Z",
     "start_time": "2024-06-09T13:46:47.042562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "params = {'tfidf_vect__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "          'tfidf_vect__max_df': [100, 300, 700],\n",
    "          'lr_clf__C': [1, 5, 10]\n",
    "          }\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv_pipe.fit(X_train, y_train)\n",
    "print(grid_cv_pipe.best_params_, grid_cv_pipe.best_score_)\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test, pred)))"
   ],
   "id": "8c7ea622f468703d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "{'lr_clf__C': 10, 'tfidf_vect__max_df': 700, 'tfidf_vect__ngram_range': (1, 2)} 0.7550828826229531\n",
      "Pipeline을 통한 Logistic Regression 의 예측 정확도는 0.702\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:10:07.934283Z",
     "start_time": "2024-06-09T14:10:07.565776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_df = pd.read_csv('./data/labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)\n",
    "review_df.head(3)"
   ],
   "id": "9c6a731dc1694122",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:10:07.937266Z",
     "start_time": "2024-06-09T14:10:07.935103Z"
    }
   },
   "cell_type": "code",
   "source": "print(review_df['review'][0])",
   "id": "7446f95513a4feb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:10:09.214254Z",
     "start_time": "2024-06-09T14:10:07.939233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "review_df['review'] = review_df['review'].str.replace('<br />', ' ')\n",
    "review_df['review'] = review_df['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", x))"
   ],
   "id": "4509fd627242f86b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:10:09.226570Z",
     "start_time": "2024-06-09T14:10:09.218504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df.drop(['id', 'sentiment'], axis=1, inplace=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_df, class_df, test_size=0.3, random_state=156)\n",
    "X_train.shape, X_test.shape"
   ],
   "id": "68323a603a8b4322",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:10:26.654703Z",
     "start_time": "2024-06-09T14:10:09.227361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))])\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test, pred), roc_auc_score(y_test, pred_probs)))"
   ],
   "id": "9431a8df65d71db8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8860, ROC-AUC는 0.9503\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:10:36.675588Z",
     "start_time": "2024-06-09T14:10:26.655457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1, 2))),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))])\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:, 1]\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test, pred), roc_auc_score(y_test, pred_probs)))"
   ],
   "id": "da0536da7d123f18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8936, ROC-AUC는 0.9598\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:07.051844Z",
     "start_time": "2024-06-09T14:10:36.676377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('all')"
   ],
   "id": "c5ef3922e5da1f8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     /Users/llouis/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:07.902952Z",
     "start_time": "2024-06-09T14:12:07.054029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "term = 'present'\n",
    "synsets = wn.synsets(term)\n",
    "print('synsets() 반환 type : ', type(synsets))\n",
    "print('synsets() 반환 값 갯수 : ', len(synsets))\n",
    "print('synsets() 반환 값 : ', synsets)"
   ],
   "id": "c2f6a5fc8051b0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets() 반환 type :  <class 'list'>\n",
      "synsets() 반환 값 갯수 :  18\n",
      "synsets() 반환 값 :  [Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:07.906360Z",
     "start_time": "2024-06-09T14:12:07.903746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for synset in synsets:\n",
    "    print('##### Synset name : ', synset.name(), '#####')\n",
    "    print('POS : ', synset.lexname())\n",
    "    print('Definition : ', synset.definition())\n",
    "    print('Lemmas : ', synset.lemma_names())"
   ],
   "id": "9d863f2e18581077",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Synset name :  present.n.01 #####\n",
      "POS :  noun.time\n",
      "Definition :  the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
      "Lemmas :  ['present', 'nowadays']\n",
      "##### Synset name :  present.n.02 #####\n",
      "POS :  noun.possession\n",
      "Definition :  something presented as a gift\n",
      "Lemmas :  ['present']\n",
      "##### Synset name :  present.n.03 #####\n",
      "POS :  noun.communication\n",
      "Definition :  a verb tense that expresses actions or states at the time of speaking\n",
      "Lemmas :  ['present', 'present_tense']\n",
      "##### Synset name :  show.v.01 #####\n",
      "POS :  verb.perception\n",
      "Definition :  give an exhibition of to an interested audience\n",
      "Lemmas :  ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "##### Synset name :  present.v.02 #####\n",
      "POS :  verb.communication\n",
      "Definition :  bring forward and present to the mind\n",
      "Lemmas :  ['present', 'represent', 'lay_out']\n",
      "##### Synset name :  stage.v.01 #####\n",
      "POS :  verb.creation\n",
      "Definition :  perform (a play), especially on a stage\n",
      "Lemmas :  ['stage', 'present', 'represent']\n",
      "##### Synset name :  present.v.04 #####\n",
      "POS :  verb.possession\n",
      "Definition :  hand over formally\n",
      "Lemmas :  ['present', 'submit']\n",
      "##### Synset name :  present.v.05 #####\n",
      "POS :  verb.stative\n",
      "Definition :  introduce\n",
      "Lemmas :  ['present', 'pose']\n",
      "##### Synset name :  award.v.01 #####\n",
      "POS :  verb.possession\n",
      "Definition :  give, especially as an honor or reward\n",
      "Lemmas :  ['award', 'present']\n",
      "##### Synset name :  give.v.08 #####\n",
      "POS :  verb.possession\n",
      "Definition :  give as a present; make a gift of\n",
      "Lemmas :  ['give', 'gift', 'present']\n",
      "##### Synset name :  deliver.v.01 #####\n",
      "POS :  verb.communication\n",
      "Definition :  deliver (a speech, oration, or idea)\n",
      "Lemmas :  ['deliver', 'present']\n",
      "##### Synset name :  introduce.v.01 #####\n",
      "POS :  verb.communication\n",
      "Definition :  cause to come to know personally\n",
      "Lemmas :  ['introduce', 'present', 'acquaint']\n",
      "##### Synset name :  portray.v.04 #####\n",
      "POS :  verb.creation\n",
      "Definition :  represent abstractly, for example in a painting, drawing, or sculpture\n",
      "Lemmas :  ['portray', 'present']\n",
      "##### Synset name :  confront.v.03 #####\n",
      "POS :  verb.communication\n",
      "Definition :  present somebody with something, usually to accuse or criticize\n",
      "Lemmas :  ['confront', 'face', 'present']\n",
      "##### Synset name :  present.v.12 #####\n",
      "POS :  verb.communication\n",
      "Definition :  formally present a debutante, a representative of a country, etc.\n",
      "Lemmas :  ['present']\n",
      "##### Synset name :  salute.v.06 #####\n",
      "POS :  verb.communication\n",
      "Definition :  recognize with a gesture prescribed by a military regulation; assume a prescribed position\n",
      "Lemmas :  ['salute', 'present']\n",
      "##### Synset name :  present.a.01 #####\n",
      "POS :  adj.all\n",
      "Definition :  temporal sense; intermediate between past and future; now existing or happening or in consideration\n",
      "Lemmas :  ['present']\n",
      "##### Synset name :  present.a.02 #####\n",
      "POS :  adj.all\n",
      "Definition :  being or existing in a specified place\n",
      "Lemmas :  ['present']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:07.917630Z",
     "start_time": "2024-06-09T14:12:07.907104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog = wn.synset('dog.n.01')\n",
    "entities = [tree, lion, tiger, cat, dog]\n",
    "similarities = []\n",
    "entity_names = [entity.name().split('.')[0] for entity in entities]\n",
    "for entity in entities:\n",
    "    similarity = [round(entity.path_similarity(compared_entity), 2) for compared_entity in entities]\n",
    "    similarities.append(similarity)\n",
    "similarity_df = pd.DataFrame(similarities, columns=entity_names, index=entity_names)\n",
    "similarity_df"
   ],
   "id": "d7645b89670bf156",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       tree  lion  tiger   cat   dog\n",
       "tree   1.00  0.07   0.07  0.08  0.12\n",
       "lion   0.07  1.00   0.33  0.25  0.17\n",
       "tiger  0.07  0.33   1.00  0.25  0.17\n",
       "cat    0.08  0.25   0.25  1.00  0.20\n",
       "dog    0.12  0.17   0.17  0.20  1.00"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:08.303489Z",
     "start_time": "2024-06-09T14:12:07.919073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "senti_synsets = list(swn.senti_synsets('slow'))\n",
    "print('senti_synsets() 반환 type : ', type(senti_synsets))\n",
    "print('senti_synsets() 반환 값 갯수 : ', len(senti_synsets))\n",
    "print('senti_synsets() 반환 값 : ', senti_synsets)"
   ],
   "id": "8a823b13d95007c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senti_synsets() 반환 type :  <class 'list'>\n",
      "senti_synsets() 반환 값 갯수 :  11\n",
      "senti_synsets() 반환 값 :  [SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:08.307635Z",
     "start_time": "2024-06-09T14:12:08.304325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "father = swn.senti_synset('father.n.01')\n",
    "print('father 긍정감성 지수 : ', father.pos_score())\n",
    "print('father 부정감성 지수 : ', father.neg_score())\n",
    "print('father 객관성 지수 : ', father.obj_score())\n",
    "fabulous = swn.senti_synset('fabulous.a.01')\n",
    "print('fabulous 긍정감성 지수 : ', fabulous.pos_score())\n",
    "print('fabulous 부정감성 지수 : ', fabulous.neg_score())"
   ],
   "id": "3c93dc19602bc2c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father 긍정감성 지수 :  0.0\n",
      "father 부정감성 지수 :  0.0\n",
      "father 객관성 지수 :  1.0\n",
      "fabulous 긍정감성 지수 :  0.875\n",
      "fabulous 부정감성 지수 :  0.125\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:08.310712Z",
     "start_time": "2024-06-09T14:12:08.308275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return"
   ],
   "id": "4489514f252420bf",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:12:08.314737Z",
     "start_time": "2024-06-09T14:12:08.311436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "def swn_polarity(text):\n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    for raw_sentence in raw_sentences:\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())\n",
    "            tokens_count += 1\n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    if sentiment >= 0:\n",
    "        return 1\n",
    "    return 0"
   ],
   "id": "17119cd2d882ca72",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:14:40.792259Z",
     "start_time": "2024-06-09T14:12:08.315363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "review_df['preds'] = review_df['review'].apply(lambda x: swn_polarity(x))\n",
    "y_target = review_df['sentiment'].values\n",
    "preds = review_df['preds'].values"
   ],
   "id": "7853ea9cb2660806",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:14:40.814415Z",
     "start_time": "2024-06-09T14:14:40.793157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "print(confusion_matrix(y_target, preds))\n",
    "print(\"정확도 : \", np.round(accuracy_score(y_target, preds), 4))\n",
    "print(\"정밀도 : \", np.round(precision_score(y_target, preds), 4))\n",
    "print(\"재현율 : \", np.round(recall_score(y_target, preds), 4))"
   ],
   "id": "13f2d7f8a15d746c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7668 4832]\n",
      " [3636 8864]]\n",
      "정확도 :  0.6613\n",
      "정밀도 :  0.6472\n",
      "재현율 :  0.7091\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:14:40.835643Z",
     "start_time": "2024-06-09T14:14:40.819912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "senti_scores = senti_analyzer.polarity_scores(review_df['review'][0])\n",
    "print(senti_scores)"
   ],
   "id": "28072549b0380b0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.13, 'neu': 0.743, 'pos': 0.127, 'compound': -0.7943}\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:36.606490Z",
     "start_time": "2024-06-09T14:14:40.836368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def vader_polarity(review, threshold=0.1):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 1 if agg_score >= threshold else 0\n",
    "    return final_sentiment\n",
    "\n",
    "\n",
    "review_df['vader_preds'] = review_df['review'].apply(lambda x: vader_polarity(x, 0.1))\n",
    "y_target = review_df['sentiment'].values\n",
    "vader_preds = review_df['vader_preds'].values\n",
    "print(confusion_matrix(y_target, vader_preds))\n",
    "print(\"정확도 : \", np.round(accuracy_score(y_target, vader_preds), 4))\n",
    "print(\"정밀도 : \", np.round(precision_score(y_target, vader_preds), 4))\n",
    "print(\"재현율 : \", np.round(recall_score(y_target, vader_preds), 4))"
   ],
   "id": "75ed59cf9a63a888",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6747  5753]\n",
      " [ 1858 10642]]\n",
      "정확도 :  0.6956\n",
      "정밀도 :  0.6491\n",
      "재현율 :  0.8514\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:40.244526Z",
     "start_time": "2024-06-09T14:16:36.607431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
    "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
    "news_df = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'),\n",
    "                             categories=cats, random_state=0)\n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1, 2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print('CountVectorizer Shape : ', feat_vect.shape)"
   ],
   "id": "36988591abf26cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Shape :  (7862, 1000)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:50.915824Z",
     "start_time": "2024-06-09T14:16:40.245451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)"
   ],
   "id": "e2826fa71bd864ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=8, random_state=0)"
      ],
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(n_components=8, random_state=0)</pre></div></div></div></div></div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:50.919759Z",
     "start_time": "2024-06-09T14:16:50.916704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ],
   "id": "efaeea29ba0bc22d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
       "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
       "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
       "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
       "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
       "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
       "       ...,\n",
       "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
       "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
       "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
       "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
       "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
       "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:16:50.974162Z",
     "start_time": "2024-06-09T14:16:50.920462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #', topic_index)\n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes = topic_word_indexes[:no_top_words]\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])\n",
    "        print(feature_concat)\n",
    "\n",
    "\n",
    "feature_names = count_vect.get_feature_names()\n",
    "display_topics(lda, feature_names, 15)"
   ],
   "id": "ae1251ce9e304635",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 10\u001B[0m\n\u001B[1;32m      6\u001B[0m         feature_concat \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([feature_names[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m top_indexes])\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mprint\u001B[39m(feature_concat)\n\u001B[0;32m---> 10\u001B[0m feature_names \u001B[38;5;241m=\u001B[39m count_vect\u001B[38;5;241m.\u001B[39mget_feature_names()\n\u001B[1;32m     11\u001B[0m display_topics(lda, feature_names, \u001B[38;5;241m15\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:47.033139Z",
     "start_time": "2024-06-09T14:17:47.026895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 700)\n",
    "\n",
    "path = r'/Users/llouis/Desktop/LLouis/AI_and_Data_Deep_Dive/파이썬 머신러닝 완벽 가이드/data/OpinosisDataset1.0'\n",
    "all_files = glob.glob(os.path.join(path, \"*.data\"))\n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "document_df = pd.DataFrame({'filename': filename_list, 'opinion_text': opinion_text})\n",
    "document_df.head()"
   ],
   "id": "5006f765d57b3d84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, opinion_text]\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:48.600998Z",
     "start_time": "2024-06-09T14:17:48.598211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ],
   "id": "b81ef723dcd00b8b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:50.056184Z",
     "start_time": "2024-06-09T14:17:49.890055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english', ngram_range=(1, 2), min_df=0.05, max_df=0.85)\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])"
   ],
   "id": "674f47bd5f3ff461",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TfidfVectorizer\n\u001B[1;32m      3\u001B[0m tfidf_vect \u001B[38;5;241m=\u001B[39m TfidfVectorizer(tokenizer\u001B[38;5;241m=\u001B[39mLemNormalize, stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m, ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m), min_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, max_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.85\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m feature_vect \u001B[38;5;241m=\u001B[39m tfidf_vect\u001B[38;5;241m.\u001B[39mfit_transform(document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopinion_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2133\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   2126\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[1;32m   2127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[1;32m   2128\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[1;32m   2129\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[1;32m   2130\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[1;32m   2131\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[1;32m   2132\u001B[0m )\n\u001B[0;32m-> 2133\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit_transform(raw_documents)\n\u001B[1;32m   2134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[1;32m   2135\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[1;32m   2136\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1388\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1380\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1381\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1382\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1383\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1384\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1385\u001B[0m             )\n\u001B[1;32m   1386\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1388\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_count_vocab(raw_documents, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfixed_vocabulary_)\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[1;32m   1391\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1294\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1292\u001B[0m     vocabulary \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(vocabulary)\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m vocabulary:\n\u001B[0;32m-> 1294\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1295\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1296\u001B[0m         )\n\u001B[1;32m   1298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indptr[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39miinfo(np\u001B[38;5;241m.\u001B[39mint32)\u001B[38;5;241m.\u001B[39mmax:  \u001B[38;5;66;03m# = 2**31 - 1\u001B[39;00m\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _IS_32BIT:\n",
      "\u001B[0;31mValueError\u001B[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:51.479511Z",
     "start_time": "2024-06-09T14:17:51.311766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_cluster = KMeans(n_clusters=5, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_"
   ],
   "id": "1748f24505fb6b8f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[39], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcluster\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KMeans\n\u001B[1;32m      3\u001B[0m km_cluster \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m km_cluster\u001B[38;5;241m.\u001B[39mfit(feature_vect)\n\u001B[1;32m      5\u001B[0m cluster_label \u001B[38;5;241m=\u001B[39m km_cluster\u001B[38;5;241m.\u001B[39mlabels_\n\u001B[1;32m      6\u001B[0m cluster_centers \u001B[38;5;241m=\u001B[39m km_cluster\u001B[38;5;241m.\u001B[39mcluster_centers_\n",
      "\u001B[0;31mNameError\u001B[0m: name 'feature_vect' is not defined"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:52.299464Z",
     "start_time": "2024-06-09T14:17:52.288693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.head()"
   ],
   "id": "8ae1166db4fd082d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster_label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m cluster_label\n\u001B[1;32m      2\u001B[0m document_df\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cluster_label' is not defined"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:54.426457Z",
     "start_time": "2024-06-09T14:17:53.196174Z"
    }
   },
   "cell_type": "code",
   "source": "document_df[document_df['cluster_label'] == 0].sort_values(by='filename')",
   "id": "dfec6ad194b4d0f2",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster_label'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m document_df[document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:55.277027Z",
     "start_time": "2024-06-09T14:17:55.244795Z"
    }
   },
   "cell_type": "code",
   "source": "document_df[document_df['cluster_label'] == 1].sort_values(by='filename')",
   "id": "630c739f8eddbef2",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster_label'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m document_df[document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:55.960555Z",
     "start_time": "2024-06-09T14:17:55.929037Z"
    }
   },
   "cell_type": "code",
   "source": "document_df[document_df['cluster_label'] == 2].sort_values(by='filename')",
   "id": "3dddc492f4145a3",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster_label'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m document_df[document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:56.466313Z",
     "start_time": "2024-06-09T14:17:56.433753Z"
    }
   },
   "cell_type": "code",
   "source": "document_df[document_df['cluster_label'] == 3].sort_values(by='filename')",
   "id": "55ec39dbb55c2f9e",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster_label'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m document_df[document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:56.936150Z",
     "start_time": "2024-06-09T14:17:56.903299Z"
    }
   },
   "cell_type": "code",
   "source": "document_df[document_df['cluster_label'] == 4].sort_values(by='filename')",
   "id": "1893d74b153fbafb",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster_label'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m document_df[document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m4\u001B[39m]\u001B[38;5;241m.\u001B[39msort_values(by\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:57.439950Z",
     "start_time": "2024-06-09T14:17:57.427080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.sort_values(by='cluster_label')"
   ],
   "id": "b493f80e3e3189e9",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcluster\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KMeans\n\u001B[1;32m      3\u001B[0m km_cluster \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m km_cluster\u001B[38;5;241m.\u001B[39mfit(feature_vect)\n\u001B[1;32m      5\u001B[0m cluster_label \u001B[38;5;241m=\u001B[39m km_cluster\u001B[38;5;241m.\u001B[39mlabels_\n\u001B[1;32m      6\u001B[0m document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m cluster_label\n",
      "\u001B[0;31mNameError\u001B[0m: name 'feature_vect' is not defined"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:58.096537Z",
     "start_time": "2024-06-09T14:17:58.085183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cluster_centers = km_cluster.cluster_centers_\n",
    "print('cluster_centers shape : ', cluster_centers.shape)\n",
    "print(cluster_centers)"
   ],
   "id": "c1394787f5124ca1",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMeans' object has no attribute 'cluster_centers_'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cluster_centers \u001B[38;5;241m=\u001B[39m km_cluster\u001B[38;5;241m.\u001B[39mcluster_centers_\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_centers shape : \u001B[39m\u001B[38;5;124m'\u001B[39m, cluster_centers\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(cluster_centers)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'KMeans' object has no attribute 'cluster_centers_'"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:17:58.683264Z",
     "start_time": "2024-06-09T14:17:58.680111Z"
    }
   },
   "cell_type": "code",
   "source": [
    " def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10):\n",
    "    cluster_details = {}\n",
    "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:, ::-1]\n",
    "    for cluster_num in range(clusters_num):\n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
    "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
    "        top_features = [feature_names[ind] for ind in top_feature_indexes]\n",
    "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
    "        cluster_details[cluster_num]['top_features'] = top_features\n",
    "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
    "        filenames = cluster_data[cluster_data['cluster_label'] == cluster_num]['filename']\n",
    "        filenames = filenames.values.tolist()\n",
    "        cluster_details[cluster_num]['filenames'] = filenames\n",
    "    return cluster_details"
   ],
   "id": "bf3c958fc5dfbd8a",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:00.011922Z",
     "start_time": "2024-06-09T14:18:00.009439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_cluster_details(cluster_details):\n",
    "    for cluster_num, cluster_detail in cluster_details.items():\n",
    "        print('####### Cluster {0}'.format(cluster_num))\n",
    "        print('Top features : ', cluster_detail['top_features'])\n",
    "        print('Reviews 파일명 : ', cluster_detail['filenames'][:7])\n",
    "        print('==================================================')"
   ],
   "id": "81669978e9c9a1c3",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:00.746879Z",
     "start_time": "2024-06-09T14:18:00.735970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_names = tfidf_vect.get_feature_names()\n",
    "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df, feature_names=feature_names,\n",
    "                                      clusters_num=3, top_n_features=10)\n",
    "print_cluster_details(cluster_details)"
   ],
   "id": "1c5e4f3f63eb66af",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m feature_names \u001B[38;5;241m=\u001B[39m tfidf_vect\u001B[38;5;241m.\u001B[39mget_feature_names()\n\u001B[1;32m      2\u001B[0m cluster_details \u001B[38;5;241m=\u001B[39m get_cluster_details(cluster_model\u001B[38;5;241m=\u001B[39mkm_cluster, cluster_data\u001B[38;5;241m=\u001B[39mdocument_df, feature_names\u001B[38;5;241m=\u001B[39mfeature_names,\n\u001B[1;32m      3\u001B[0m                                       clusters_num\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, top_n_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      4\u001B[0m print_cluster_details(cluster_details)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:01.462375Z",
     "start_time": "2024-06-09T14:18:01.460478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)"
   ],
   "id": "fbcd2bedacca1e99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.2\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:02.047495Z",
     "start_time": "2024-06-09T14:18:02.045019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    dot_product = np.dot(v1, v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm\n",
    "    return similarity"
   ],
   "id": "d0c7865764e33d11",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:02.687533Z",
     "start_time": "2024-06-09T14:18:02.684076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "doc_list = ['if you take the blue pill, the story ends',\n",
    "            'if you take the red pill, you stay in Wonderland',\n",
    "            'if you take the red pill, I show you how deep the rabbit hole goes']\n",
    "tfidf_vect_simple = TfidfVectorizer()\n",
    "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
    "print(feature_vect_simple.shape)"
   ],
   "id": "6acd9d83d76fd08d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 18)\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:03.339611Z",
     "start_time": "2024-06-09T14:18:03.336754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_vect_dense = feature_vect_simple.todense()\n",
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n",
    "similarity_simple = cos_similarity(vect1, vect2)\n",
    "print('문장 1, 문장 2 Cosine 유사도 : {0:.3f}'.format(similarity_simple))"
   ],
   "id": "cc8dcbee62a6eb88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 1, 문장 2 Cosine 유사도 : 0.402\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:03.936620Z",
     "start_time": "2024-06-09T14:18:03.933585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1, )\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n",
    "similarity_simple = cos_similarity(vect1, vect3)\n",
    "print('문장 1, 문장 3 Cosine 유사도 : {0:.3f}'.format(similarity_simple))\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1, )\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1, )\n",
    "similarity_simple = cos_similarity(vect2, vect3)\n",
    "print('문장 2, 문장 3 Cosine 유사도 : {0:.3f}'.format(similarity_simple))"
   ],
   "id": "eedc286c4ac81db7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 1, 문장 3 Cosine 유사도 : 0.404\n",
      "문장 2, 문장 3 Cosine 유사도 : 0.456\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:04.557755Z",
     "start_time": "2024-06-09T14:18:04.554168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple)\n",
    "print(similarity_simple_pair)"
   ],
   "id": "3359e38c6db89ff8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.40207758 0.40425045]]\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:05.112079Z",
     "start_time": "2024-06-09T14:18:05.109189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(feature_vect_simple[0], feature_vect_simple[1:])\n",
    "print(similarity_simple_pair)"
   ],
   "id": "384f5bf6911b88f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40207758 0.40425045]]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:05.686872Z",
     "start_time": "2024-06-09T14:18:05.684224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "similarity_simple_pair = cosine_similarity(feature_vect_simple, feature_vect_simple)\n",
    "print(similarity_simple_pair)\n",
    "print('shape : ', similarity_simple_pair.shape)"
   ],
   "id": "39de8417e1f14fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.40207758 0.40425045]\n",
      " [0.40207758 1.         0.45647296]\n",
      " [0.40425045 0.45647296 1.        ]]\n",
      "shape :  (3, 3)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:06.279808Z",
     "start_time": "2024-06-09T14:18:06.276974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ],
   "id": "fb9d2b94ba317170",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:06.877473Z",
     "start_time": "2024-06-09T14:18:06.837479Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path = r'OpinosisDataset1.0\\topics'\n",
    "all_files = glob.glob(os.path.join(path, \"*.data\"))\n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split('.')[0]\n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "document_df = pd.DataFrame({'filename': filename_list, 'opinion_text': opinion_text})\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english', ngram_range=(1, 2), min_df=0.05, max_df=0.85)\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "document_df['cluster_label'] = cluster_label"
   ],
   "id": "8188a5e045e1443d",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[60], line 21\u001B[0m\n\u001B[1;32m     19\u001B[0m document_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m: filename_list, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopinion_text\u001B[39m\u001B[38;5;124m'\u001B[39m: opinion_text})\n\u001B[1;32m     20\u001B[0m tfidf_vect \u001B[38;5;241m=\u001B[39m TfidfVectorizer(tokenizer\u001B[38;5;241m=\u001B[39mLemNormalize, stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m, ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m), min_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m, max_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.85\u001B[39m)\n\u001B[0;32m---> 21\u001B[0m feature_vect \u001B[38;5;241m=\u001B[39m tfidf_vect\u001B[38;5;241m.\u001B[39mfit_transform(document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mopinion_text\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     22\u001B[0m km_cluster \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     23\u001B[0m km_cluster\u001B[38;5;241m.\u001B[39mfit(feature_vect)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2133\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   2126\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[1;32m   2127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[1;32m   2128\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[1;32m   2129\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[1;32m   2130\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[1;32m   2131\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[1;32m   2132\u001B[0m )\n\u001B[0;32m-> 2133\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mfit_transform(raw_documents)\n\u001B[1;32m   2134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[1;32m   2135\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[1;32m   2136\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1388\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1380\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1381\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1382\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1383\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1384\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1385\u001B[0m             )\n\u001B[1;32m   1386\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1388\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_count_vocab(raw_documents, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfixed_vocabulary_)\n\u001B[1;32m   1390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[1;32m   1391\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1294\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1292\u001B[0m     vocabulary \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(vocabulary)\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m vocabulary:\n\u001B[0;32m-> 1294\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1295\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1296\u001B[0m         )\n\u001B[1;32m   1298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m indptr[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39miinfo(np\u001B[38;5;241m.\u001B[39mint32)\u001B[38;5;241m.\u001B[39mmax:  \u001B[38;5;66;03m# = 2**31 - 1\u001B[39;00m\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _IS_32BIT:\n",
      "\u001B[0;31mValueError\u001B[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:07.550731Z",
     "start_time": "2024-06-09T14:18:07.516554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "hotel_indexes = document_df[document_df['cluster_label'] == 2].index\n",
    "print('호텔로 클러스터링 된 문서들의 DataFrame Index : ', hotel_indexes)\n",
    "comparison_docname = document_df.iloc[hotel_indexes[0]]['filename']\n",
    "print('##### 비교 기준 문서명 ', comparison_docname, ' 와 타 문서 유사도######')\n",
    "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]], feature_vect[hotel_indexes])\n",
    "print(similarity_pair)"
   ],
   "id": "15d3133eb8494bb1",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cluster_label'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpairwise\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cosine_similarity\n\u001B[0;32m----> 3\u001B[0m hotel_indexes \u001B[38;5;241m=\u001B[39m document_df[document_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcluster_label\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mindex\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m호텔로 클러스터링 된 문서들의 DataFrame Index : \u001B[39m\u001B[38;5;124m'\u001B[39m, hotel_indexes)\n\u001B[1;32m      5\u001B[0m comparison_docname \u001B[38;5;241m=\u001B[39m document_df\u001B[38;5;241m.\u001B[39miloc[hotel_indexes[\u001B[38;5;241m0\u001B[39m]][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mget_loc(key)\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'cluster_label'"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:08.939208Z",
     "start_time": "2024-06-09T14:18:08.417339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sorted_index = similarity_pair.argsort()[:, ::-1]\n",
    "sorted_index = sorted_index[:, 1:]\n",
    "hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1)]\n",
    "hotel_1_sim_value = np.sort(similarity_pair.reshape(-1))[::-1]\n",
    "hotel_1_sim_value = hotel_1_sim_value[1:]\n",
    "hotel_1_sim_df = pd.DataFrame()\n",
    "hotel_1_sim_df['filename'] = document_df.iloc[hotel_sorted_indexes]['filename']\n",
    "hotel_1_sim_df['similarity'] = hotel_1_sim_value\n",
    "fig1 = plt.gcf()\n",
    "sns.barplot(x='similarity', y='filename', data=hotel_1_sim_df)\n",
    "plt.title(comparison_docname)\n",
    "fig1.savefig('p553_hotel.tif', format='tif', dpi=300, bbox_inches='tight')"
   ],
   "id": "6843bcce867b027",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarity_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[62], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      4\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmatplotlib\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minline\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 6\u001B[0m sorted_index \u001B[38;5;241m=\u001B[39m similarity_pair\u001B[38;5;241m.\u001B[39margsort()[:, ::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m      7\u001B[0m sorted_index \u001B[38;5;241m=\u001B[39m sorted_index[:, \u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m      8\u001B[0m hotel_sorted_indexes \u001B[38;5;241m=\u001B[39m hotel_indexes[sorted_index\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'similarity_pair' is not defined"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:10.276315Z",
     "start_time": "2024-06-09T14:18:09.997313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('./data/ratings_train.txt', sep='\\t')\n",
    "train_df.head(3)"
   ],
   "id": "a51758dc6004be42",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         id                           document  label\n",
       "0   9976970                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                  너무재밓었다그래서보는것을추천한다      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:10.892012Z",
     "start_time": "2024-06-09T14:18:10.888419Z"
    }
   },
   "cell_type": "code",
   "source": "train_df['label'].value_counts()",
   "id": "d2af02451609f0d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    75173\n",
       "1    74827\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:11.974159Z",
     "start_time": "2024-06-09T14:18:11.521358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "train_df = train_df.fillna(' ')\n",
    "train_df['document'] = train_df['document'].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
    "test_df = pd.read_csv('ratings_test.txt', sep='\\t')\n",
    "test_df = test_df.fillna(' ')\n",
    "test_df['document'] = test_df['document'].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n",
    "train_df.drop('id', axis=1, inplace=True)\n",
    "test_df.drop('id', axis=1, inplace=True)"
   ],
   "id": "5663bd49be759371",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ratings_test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[65], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m train_df \u001B[38;5;241m=\u001B[39m train_df\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m train_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m train_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, x))\n\u001B[0;32m----> 5\u001B[0m test_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mratings_test.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m test_df \u001B[38;5;241m=\u001B[39m test_df\u001B[38;5;241m.\u001B[39mfillna(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m test_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m test_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124md+\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m, x))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1881\u001B[0m     f,\n\u001B[1;32m   1882\u001B[0m     mode,\n\u001B[1;32m   1883\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1884\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1885\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1886\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1887\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1888\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1889\u001B[0m )\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    876\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    877\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'ratings_test.txt'"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:12.773173Z",
     "start_time": "2024-06-09T14:18:12.540519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from konlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "\n",
    "\n",
    "def tw_tokenizer(text):\n",
    "    tokens_ko = twitter.morphs(text)\n",
    "    return tokens_ko"
   ],
   "id": "ca5a981b61825302",
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (libjli.dylib) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJVMNotFoundException\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkonlpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Twitter\n\u001B[0;32m----> 3\u001B[0m twitter \u001B[38;5;241m=\u001B[39m Twitter()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtw_tokenizer\u001B[39m(text):\n\u001B[1;32m      7\u001B[0m     tokens_ko \u001B[38;5;241m=\u001B[39m twitter\u001B[38;5;241m.\u001B[39mmorphs(text)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/konlpy/tag/_okt.py:18\u001B[0m, in \u001B[0;36mTwitter\u001B[0;34m(jvmpath)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m warn\n\u001B[1;32m     17\u001B[0m warn(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTwitter\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m has changed to \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOkt\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m since KoNLPy v0.4.5.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Okt(jvmpath)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/konlpy/tag/_okt.py:51\u001B[0m, in \u001B[0;36mOkt.__init__\u001B[0;34m(self, jvmpath, max_heap_size)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, jvmpath\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, max_heap_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m jpype\u001B[38;5;241m.\u001B[39misJVMStarted():\n\u001B[0;32m---> 51\u001B[0m         jvm\u001B[38;5;241m.\u001B[39minit_jvm(jvmpath, max_heap_size)\n\u001B[1;32m     53\u001B[0m     oktJavaPackage \u001B[38;5;241m=\u001B[39m jpype\u001B[38;5;241m.\u001B[39mJPackage(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkr.lucypark.okt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     54\u001B[0m     OktInterfaceJavaClass \u001B[38;5;241m=\u001B[39m oktJavaPackage\u001B[38;5;241m.\u001B[39mOktInterface\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/konlpy/jvm.py:55\u001B[0m, in \u001B[0;36minit_jvm\u001B[0;34m(jvmpath, max_heap_size)\u001B[0m\n\u001B[1;32m     52\u001B[0m args \u001B[38;5;241m=\u001B[39m [javadir, os\u001B[38;5;241m.\u001B[39msep]\n\u001B[1;32m     53\u001B[0m classpath \u001B[38;5;241m=\u001B[39m [f\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39margs) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m folder_suffix]\n\u001B[0;32m---> 55\u001B[0m jvmpath \u001B[38;5;241m=\u001B[39m jvmpath \u001B[38;5;129;01mor\u001B[39;00m jpype\u001B[38;5;241m.\u001B[39mgetDefaultJVMPath()\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdarwin\u001B[39m\u001B[38;5;124m'\u001B[39m\\\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m jvmpath\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1.8.0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\\\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m jvmpath\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlibjvm.dylib\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/jpype/_jvmfinder.py:74\u001B[0m, in \u001B[0;36mgetDefaultJVMPath\u001B[0;34m()\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     73\u001B[0m     finder \u001B[38;5;241m=\u001B[39m LinuxJVMFinder()\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m finder\u001B[38;5;241m.\u001B[39mget_jvm_path()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/jpype/_jvmfinder.py:212\u001B[0m, in \u001B[0;36mJVMFinder.get_jvm_path\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m jvm_notsupport_ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m jvm_notsupport_ext\n\u001B[0;32m--> 212\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m JVMNotFoundException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo JVM shared library file (\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    213\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound. Try setting up the JAVA_HOME \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    214\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment variable properly.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    215\u001B[0m                            \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_libfile))\n",
      "\u001B[0;31mJVMNotFoundException\u001B[0m: No JVM shared library file (libjli.dylib) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:13.344830Z",
     "start_time": "2024-06-09T14:18:13.331259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1, 2), min_df=3, max_df=0.9)\n",
    "tfidf_vect.fit(train_df['document'])\n",
    "tfidf_matrix_train = tfidf_vect.transform(train_df['document'])"
   ],
   "id": "b6d95e2e194f1caa",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tw_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinear_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LogisticRegression\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GridSearchCV\n\u001B[0;32m----> 5\u001B[0m tfidf_vect \u001B[38;5;241m=\u001B[39m TfidfVectorizer(tokenizer\u001B[38;5;241m=\u001B[39mtw_tokenizer, ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m), min_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, max_df\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m)\n\u001B[1;32m      6\u001B[0m tfidf_vect\u001B[38;5;241m.\u001B[39mfit(train_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      7\u001B[0m tfidf_matrix_train \u001B[38;5;241m=\u001B[39m tfidf_vect\u001B[38;5;241m.\u001B[39mtransform(train_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tw_tokenizer' is not defined"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:14.027781Z",
     "start_time": "2024-06-09T14:18:14.015172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lg_clf = LogisticRegression(random_state=0, solver='liblinear')\n",
    "params = {'C': [1, 3.5, 4.5, 5.5, 10]}\n",
    "grid_cv = GridSearchCV(lg_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_cv.fit(tfidf_matrix_train, train_df['label'])\n",
    "print(grid_cv.best_params_, round(grid_cv.best_score_, 4))"
   ],
   "id": "1fcc15ef02483d00",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[68], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC\u001B[39m\u001B[38;5;124m'\u001B[39m: [\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3.5\u001B[39m, \u001B[38;5;241m4.5\u001B[39m, \u001B[38;5;241m5.5\u001B[39m, \u001B[38;5;241m10\u001B[39m]}\n\u001B[1;32m      3\u001B[0m grid_cv \u001B[38;5;241m=\u001B[39m GridSearchCV(lg_clf, param_grid\u001B[38;5;241m=\u001B[39mparams, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m grid_cv\u001B[38;5;241m.\u001B[39mfit(tfidf_matrix_train, train_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(grid_cv\u001B[38;5;241m.\u001B[39mbest_params_, \u001B[38;5;28mround\u001B[39m(grid_cv\u001B[38;5;241m.\u001B[39mbest_score_, \u001B[38;5;241m4\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'tfidf_matrix_train' is not defined"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:14.642837Z",
     "start_time": "2024-06-09T14:18:14.630606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tfidf_matrix_test = tfidf_vect.transform(test_df['document'])\n",
    "best_estimator = grid_cv.best_estimator_\n",
    "preds = best_estimator.predict(tfidf_matrix_test)\n",
    "print('Logistic Regression 정확도 : ', accuracy_score(test_df['label'], preds))"
   ],
   "id": "38dafe8623d073a2",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[69], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy_score\n\u001B[0;32m----> 3\u001B[0m tfidf_matrix_test \u001B[38;5;241m=\u001B[39m tfidf_vect\u001B[38;5;241m.\u001B[39mtransform(test_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      4\u001B[0m best_estimator \u001B[38;5;241m=\u001B[39m grid_cv\u001B[38;5;241m.\u001B[39mbest_estimator_\n\u001B[1;32m      5\u001B[0m preds \u001B[38;5;241m=\u001B[39m best_estimator\u001B[38;5;241m.\u001B[39mpredict(tfidf_matrix_test)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:15.241786Z",
     "start_time": "2024-06-09T14:18:15.192888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "mercari_df = pd.read_csv('./data/mercari_train.tsv', sep='\\t')\n",
    "print(mercari_df.shape)\n",
    "mercari_df.head(3)"
   ],
   "id": "be1e404f1ba0a7a8",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/mercari_train.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[70], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CountVectorizer, TfidfVectorizer\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m mercari_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./data/mercari_train.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(mercari_df\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m      8\u001B[0m mercari_df\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m3\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m   1014\u001B[0m     dialect,\n\u001B[1;32m   1015\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m   1023\u001B[0m )\n\u001B[1;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_engine(f, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m   1881\u001B[0m     f,\n\u001B[1;32m   1882\u001B[0m     mode,\n\u001B[1;32m   1883\u001B[0m     encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1884\u001B[0m     compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcompression\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1885\u001B[0m     memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmemory_map\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m   1886\u001B[0m     is_text\u001B[38;5;241m=\u001B[39mis_text,\n\u001B[1;32m   1887\u001B[0m     errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding_errors\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrict\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1888\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[1;32m   1889\u001B[0m )\n\u001B[1;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[1;32m    876\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[1;32m    877\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/mercari_train.tsv'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:15.996217Z",
     "start_time": "2024-06-09T14:18:15.984995Z"
    }
   },
   "cell_type": "code",
   "source": "print(mercari_df.info())",
   "id": "9773d4f54dac19d6",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[71], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(mercari_df\u001B[38;5;241m.\u001B[39minfo())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:16.498198Z",
     "start_time": "2024-06-09T14:18:16.486284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_train_df = mercari_df['price']\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.histplot(y_train_df, bins=100)\n",
    "plt.show()"
   ],
   "id": "b768feb73d165c4d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[72], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m y_train_df \u001B[38;5;241m=\u001B[39m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m6\u001B[39m, \u001B[38;5;241m4\u001B[39m))\n\u001B[1;32m      6\u001B[0m sns\u001B[38;5;241m.\u001B[39mhistplot(y_train_df, bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:17.042356Z",
     "start_time": "2024-06-09T14:18:17.030615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train_df = np.log1p(y_train_df)\n",
    "sns.histplot(y_train_df, bins=50)\n",
    "plt.show()"
   ],
   "id": "fd295af7f4767573",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[73], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m y_train_df \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog1p(y_train_df)\n\u001B[1;32m      4\u001B[0m sns\u001B[38;5;241m.\u001B[39mhistplot(y_train_df, bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m)\n\u001B[1;32m      5\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_train_df' is not defined"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:17.659878Z",
     "start_time": "2024-06-09T14:18:17.647569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mercari_df['price'] = np.log1p(mercari_df['price'])\n",
    "mercari_df['price'].head(3)"
   ],
   "id": "aec7a4e663d79694",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[74], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog1p(mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      2\u001B[0m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprice\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m3\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:18.361539Z",
     "start_time": "2024-06-09T14:18:18.349850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Shipping 값 유형 : ', mercari_df['shipping'].value_counts())\n",
    "print('item_condition_id 값 유형 : ', mercari_df['item_condition_id'].value_counts())"
   ],
   "id": "a56fce7d6dfcc001",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[75], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mShipping 값 유형 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshipping\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts())\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_condition_id 값 유형 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_condition_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:19.052039Z",
     "start_time": "2024-06-09T14:18:19.040980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "boolean_cond = mercari_df['item_description'] == 'No description yet'\n",
    "mercari_df[boolean_cond]['item_description'].count()"
   ],
   "id": "cc97159dd5011e87",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[76], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m boolean_cond \u001B[38;5;241m=\u001B[39m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo description yet\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      2\u001B[0m mercari_df[boolean_cond][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcount()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:19.632380Z",
     "start_time": "2024-06-09T14:18:19.617768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_cat(category_name):\n",
    "    try:\n",
    "        return category_name.split('/')\n",
    "    except:\n",
    "        return ['Other_Null', 'Other_Null', 'Other_Null']\n",
    "\n",
    "\n",
    "mercari_df['cat_dae'], mercari_df['cat_jung'], mercari_df['cat_so'] = zip(\n",
    "    *mercari_df['category_name'].apply(lambda x: split_cat(x)))\n",
    "print('대분류 유형 : ', mercari_df['cat_dae'].value_counts())\n",
    "print('중분류 갯수 : ', mercari_df['cat_jung'].nunique())\n",
    "print('소분류 갯수 : ', mercari_df['cat_so'].nunique())"
   ],
   "id": "9ee3b1cdfd81bd6f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[77], line 9\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m      5\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOther_Null\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOther_Null\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOther_Null\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      8\u001B[0m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcat_dae\u001B[39m\u001B[38;5;124m'\u001B[39m], mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcat_jung\u001B[39m\u001B[38;5;124m'\u001B[39m], mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcat_so\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\n\u001B[0;32m----> 9\u001B[0m     \u001B[38;5;241m*\u001B[39mmercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: split_cat(x)))\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m대분류 유형 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcat_dae\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts())\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m중분류 갯수 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcat_jung\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnunique())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:20.095599Z",
     "start_time": "2024-06-09T14:18:20.083393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mercari_df['brand_name'] = mercari_df['brand_name'].fillna(value='Other_Null')\n",
    "mercari_df['category_name'] = mercari_df['category_name'].fillna(value='Other_Null')\n",
    "mercari_df['item_description'] = mercari_df['item_description'].fillna(value='Other_Null')\n",
    "mercari_df.isnull().sum()"
   ],
   "id": "765e5ba081ab38af",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[78], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrand_name\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrand_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfillna(value\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOther_Null\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory_name\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategory_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfillna(value\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOther_Null\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfillna(value\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOther_Null\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:20.601866Z",
     "start_time": "2024-06-09T14:18:20.590745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('brand name 의 유형 건수 : ', mercari_df['brand_name'].nunique())\n",
    "print('brand name sample 5건 : ', mercari_df['brand_name'].value_counts()[:5])"
   ],
   "id": "8c6a08649c7b7375",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[79], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrand name 의 유형 건수 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrand_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnunique())\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrand name sample 5건 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrand_name\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts()[:\u001B[38;5;241m5\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:21.150273Z",
     "start_time": "2024-06-09T14:18:21.138870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('name 의 종류 갯수 : ', mercari_df['name'].nunique())\n",
    "print('name sample 7건 : ', mercari_df['name'][:7])"
   ],
   "id": "467d354d5f6d5f07",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[80], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname 의 종류 갯수 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnunique())\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname sample 7건 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m][:\u001B[38;5;241m7\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:21.615776Z",
     "start_time": "2024-06-09T14:18:21.603861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "print('item_description 평균 문자열 개수 : ', mercari_df['item_description'].str.len().mean())\n",
    "mercari_df['item_description'][:2]"
   ],
   "id": "e89de8d36bf06a2b",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[81], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m pd\u001B[38;5;241m.\u001B[39mset_option(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_colwidth\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;241m200\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description 평균 문자열 개수 : \u001B[39m\u001B[38;5;124m'\u001B[39m, mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mstr\u001B[38;5;241m.\u001B[39mlen()\u001B[38;5;241m.\u001B[39mmean())\n\u001B[1;32m      3\u001B[0m mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description\u001B[39m\u001B[38;5;124m'\u001B[39m][:\u001B[38;5;241m2\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:22.052208Z",
     "start_time": "2024-06-09T14:18:22.039236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "X_name = cnt_vec.fit_transform(mercari_df.name)\n",
    "tfidf_descp = TfidfVectorizer(max_features=50000, ngram_range=(1, 3), stop_words='english')\n",
    "X_descp = tfidf_descp.fit_transform(mercari_df['item_description'])\n",
    "print('name vectorization shape : ', X_name.shape)\n",
    "print('item_description vectorization shape : ', X_descp.shape)"
   ],
   "id": "db1f56a7c45fc741",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[82], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m cnt_vec \u001B[38;5;241m=\u001B[39m CountVectorizer()\n\u001B[0;32m----> 2\u001B[0m X_name \u001B[38;5;241m=\u001B[39m cnt_vec\u001B[38;5;241m.\u001B[39mfit_transform(mercari_df\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m      3\u001B[0m tfidf_descp \u001B[38;5;241m=\u001B[39m TfidfVectorizer(max_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50000\u001B[39m, ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m), stop_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m X_descp \u001B[38;5;241m=\u001B[39m tfidf_descp\u001B[38;5;241m.\u001B[39mfit_transform(mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_description\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:22.488846Z",
     "start_time": "2024-06-09T14:18:22.475104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb_brand_name = LabelBinarizer(sparse_output=True)\n",
    "X_brand = lb_brand_name.fit_transform(mercari_df['brand_name'])\n",
    "lb_item_cond_id = LabelBinarizer(sparse_output=True)\n",
    "X_item_cond_id = lb_item_cond_id.fit_transform(mercari_df['item_condition_id'])\n",
    "lb_shipping = LabelBinarizer(sparse_output=True)\n",
    "X_shipping = lb_shipping.fit_transform(mercari_df['shipping'])\n",
    "lb_cat_dae = LabelBinarizer(sparse_output=True)\n",
    "X_cat_dae = lb_cat_dae.fit_transform(mercari_df['cat_dae'])\n",
    "lb_cat_jung = LabelBinarizer(sparse_output=True)\n",
    "X_cat_jung = lb_cat_jung.fit_transform(mercari_df['cat_jung'])\n",
    "lb_cat_so = LabelBinarizer(sparse_output=True)\n",
    "X_cat_so = lb_cat_so.fit_transform(mercari_df['cat_so'])"
   ],
   "id": "f9632c6d1ad5c60f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mercari_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[83], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LabelBinarizer\n\u001B[1;32m      3\u001B[0m lb_brand_name \u001B[38;5;241m=\u001B[39m LabelBinarizer(sparse_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 4\u001B[0m X_brand \u001B[38;5;241m=\u001B[39m lb_brand_name\u001B[38;5;241m.\u001B[39mfit_transform(mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrand_name\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      5\u001B[0m lb_item_cond_id \u001B[38;5;241m=\u001B[39m LabelBinarizer(sparse_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m X_item_cond_id \u001B[38;5;241m=\u001B[39m lb_item_cond_id\u001B[38;5;241m.\u001B[39mfit_transform(mercari_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_condition_id\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mercari_df' is not defined"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:23.098037Z",
     "start_time": "2024-06-09T14:18:23.085937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(X_brand), type(X_item_cond_id), type(X_shipping))\n",
    "print('X_brand_shape : {0}, X_item_cond_id shape : {1}'.format(X_brand.shape, X_item_cond_id.shape))\n",
    "print('X_shipping shape : {0}, X_cat_dae shape : {1}'.format(X_shipping.shape, X_cat_dae.shape))\n",
    "print('X_cat_jung shape : {0}, X_cat_so shape : {1}'.format(X_cat_jung.shape, X_cat_so.shape))"
   ],
   "id": "ec524d693976096f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_brand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[84], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(X_brand), \u001B[38;5;28mtype\u001B[39m(X_item_cond_id), \u001B[38;5;28mtype\u001B[39m(X_shipping))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX_brand_shape : \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, X_item_cond_id shape : \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(X_brand\u001B[38;5;241m.\u001B[39mshape, X_item_cond_id\u001B[38;5;241m.\u001B[39mshape))\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mX_shipping shape : \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, X_cat_dae shape : \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(X_shipping\u001B[38;5;241m.\u001B[39mshape, X_cat_dae\u001B[38;5;241m.\u001B[39mshape))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_brand' is not defined"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:23.554482Z",
     "start_time": "2024-06-09T14:18:23.542510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.sparse import hstack\n",
    "import gc\n",
    "\n",
    "sparse_matrix_list = (X_name, X_descp, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "X_features_sparse = hstack(sparse_matrix_list).tocsr()\n",
    "print(type(X_features_sparse), X_features_sparse.shape)\n",
    "del X_features_sparse\n",
    "gc.collect()"
   ],
   "id": "e0b1c1b5755ef06c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[85], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hstack\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgc\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m sparse_matrix_list \u001B[38;5;241m=\u001B[39m (X_name, X_descp, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n\u001B[1;32m      5\u001B[0m X_features_sparse \u001B[38;5;241m=\u001B[39m hstack(sparse_matrix_list)\u001B[38;5;241m.\u001B[39mtocsr()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mtype\u001B[39m(X_features_sparse), X_features_sparse\u001B[38;5;241m.\u001B[39mshape)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_name' is not defined"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:23.555175Z",
     "start_time": "2024-06-09T14:18:23.555116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y_pred), 2)))\n",
    "\n",
    "\n",
    "def evaluate_org_price(y_test, preds):\n",
    "    preds_exmpm = np.expm1(preds)\n",
    "    y_test_exmpm = np.expm1(y_test)\n",
    "    rmsle_result = rmsle(y_test_exmpm, preds_exmpm)\n",
    "    return rmsle_result"
   ],
   "id": "7e2b2e380072a133",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:23.981275Z",
     "start_time": "2024-06-09T14:18:23.978351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "def model_train_predict(model, matrix_list):\n",
    "    X = hstack(matrix_list).tocsr()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, mercari_df['price'], test_size=0.2, random_state=156)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    del X, X_train, X_test, y_train\n",
    "    gc.collect()\n",
    "    return preds, y_test"
   ],
   "id": "c2d1e865fdf34548",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:24.405602Z",
     "start_time": "2024-06-09T14:18:24.391999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear_model = Ridge(solver=\"lsqr\", fit_intercept=False)\n",
    "sparse_matrix_list = (X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "linear_preds, y_test = model_train_predict(model=linear_model, matrix_list=sparse_matrix_list)\n",
    "print('Item Description을 제외했을 때 rmsle 값 : ', evaluate_org_price(y_test, linear_preds))\n",
    "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "linear_preds, y_test = model_train_predict(model=linear_model, matrix_list=sparse_matrix_list)\n",
    "print('Item Description을 포함한 rmsle 값 : ', evaluate_org_price(y_test, linear_preds))"
   ],
   "id": "8cb3dda089d7f786",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[87], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m linear_model \u001B[38;5;241m=\u001B[39m Ridge(solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlsqr\u001B[39m\u001B[38;5;124m\"\u001B[39m, fit_intercept\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m----> 2\u001B[0m sparse_matrix_list \u001B[38;5;241m=\u001B[39m (X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n\u001B[1;32m      3\u001B[0m linear_preds, y_test \u001B[38;5;241m=\u001B[39m model_train_predict(model\u001B[38;5;241m=\u001B[39mlinear_model, matrix_list\u001B[38;5;241m=\u001B[39msparse_matrix_list)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mItem Description을 제외했을 때 rmsle 값 : \u001B[39m\u001B[38;5;124m'\u001B[39m, evaluate_org_price(y_test, linear_preds))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_name' is not defined"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T14:18:26.199425Z",
     "start_time": "2024-06-09T14:18:24.954846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "sparse_matrix_list = (X_descp, X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n",
    "lgbm_model = LGBMRegressor(n_estimators=200, learning_rate=0.5, num_leaves=125, random_state=156)\n",
    "lgbm_preds, y_test = model_train_predict(model=lgbm_model, matrix_list=sparse_matrix_list)\n",
    "print('LightGBM rmsle 값 : ', evaluate_org_price(y_test, lgbm_preds))"
   ],
   "id": "7a98d72a4aa0fa95",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_descp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[88], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mlightgbm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LGBMRegressor\n\u001B[0;32m----> 3\u001B[0m sparse_matrix_list \u001B[38;5;241m=\u001B[39m (X_descp, X_name, X_brand, X_item_cond_id, X_shipping, X_cat_dae, X_cat_jung, X_cat_so)\n\u001B[1;32m      4\u001B[0m lgbm_model \u001B[38;5;241m=\u001B[39m LGBMRegressor(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, num_leaves\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m125\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m156\u001B[39m)\n\u001B[1;32m      5\u001B[0m lgbm_preds, y_test \u001B[38;5;241m=\u001B[39m model_train_predict(model\u001B[38;5;241m=\u001B[39mlgbm_model, matrix_list\u001B[38;5;241m=\u001B[39msparse_matrix_list)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'X_descp' is not defined"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preds = lgbm_preds * 0.45 + linear_preds * 0.55\n",
    "print('LightGBM과 Ridge를 ensemble한 최종 rmsle 값 : ', evaluate_org_price(y_test, preds))"
   ],
   "id": "84788181cf42d474",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
