# 1. 언제 신경망과 딥러닝을 사용하는가

- 신경망(Neural Network) : 입력 변수와 출력 변수 사이 가중치, 편향, 비선형 함수로 이루어진 층을 쌓아 구성
- 딥러닝(Deep Learning) : 가중치와 편향을 가진 노드로 구성된 여러 개의 은닉층 사용, 신경망의 한 종류
- 분류, 회귀에 전부 사용 가능
- 합성곱 신경망 → 이미지 인식
- LSTM(Long Short-Term Memory) → 시계열 예측
- 순환 신경망(Recurrent Neural Network) → 텍스트-투-스피치 애플리케이션

# 2. 간단한 신경망

## 1. 활성화 함수

- 노드에서 가중치를 곱해 더한 값을 변환하거나 압축하는 비선형 함수
- 신경망이 데이터를 효과적으로 분리해 분류 작업을 수행할 수 있도록 함
- 선형(Linear) : 값을 그대로 둠
    - 출력층에 사용
- 로지스틱 : S자형 시그모이드 곡선
    - 출력층에 사용
    - 0과 1 사이로 값 압축, 이진 분류에 주로 사용
- 하이퍼볼릭 탄젠트(Hyperbolic Tangent) : Tanh, -1에서 1 사이의 S자형 시그모이드 곡선
    - 은닉층에 사용
    - 평균을 0에 가깝게 만들어 데이터를 중앙에 맞춤
- 렐루(ReLU) : 음수값을 0으로 만듦
    - 은닉층에 사용
    - 시그모이드나 Tanh보다 빠른 활성화 함수
    - 그레이디언트 소실 문제 완화, 계산 비용 저렴
- 리키 렐루(Leaky ReLU) : 음수값에 0.01 곱함
    - 은닉층에 사용
    - 음수값을 제거하지 않고 줄이는 렐루의 변형
- 소프트맥스(Softmax) : 모든 출력 노드를 더해 1.0이 되도록 만듦
    - 출력층에 사용
    - 다중 분류에 유용, 출력의 크기를 조정해 합을 1.0으로 만듦

## 2. 정방향 계산

- 피드 포워드 신경망(Feed-Forward Neural Network) : 신경망에 각 픽셀의 색상값을 입력한 후 각 층의 결과를 신경망의 앞쪽으로 계속 전달해 출력 결과를 얻는 것

# 3. 역전파

## 1. 가중치와 편향에 대한 도함수 구하기

- 편도함수를 서로 연결해 가중치와 편향에 대한 새로운 편도함수 생성
- 그레이디언트 → 비용 함수의 기울기 계산

## 2. 확률적 경사 하강법

- 반복할 때마다 하나의 훈련 레코드만 사용
- 연쇄 법칙 통합

# 4. 신경망과 딥러닝의 한계

- 층, 노드, 활성화 함수의 유연성 → 비선형적인 방식으로 데이터에 유연하게 맞추기 가능
- 데이터에 과도하게 맞추는 문제 발생