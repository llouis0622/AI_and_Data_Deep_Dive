# 1. 기본 선형 회귀

- 회귀(Regression) : 관측 데이터에서 어떤 함수를 훈련한 다음 새로운 데이터에 대한 예측을 만드는 기법
- 선형 회귀(Linear Regression) : 관측 데이터에 맞는 직선 훈련

# 2. 잔차와 제곱 오차

- 머신러닝 훈련 질문
    - 최적을 정의하는 기준은 무엇인가
    - 최적에 도달하려면 어떻게 해야 하는가
- 잔차(Residual) : 제곱한 합을 최소화하는 것, 오차(Error)
- 총 잔차 값 : 각 잔차의 제곱 합

# 3. 최적의 직선 찾기

## 1. 닫힌 형식 방정식(Closed Form Equation)

- 정확한 계산으로 선형 회귀 풀기
- 단순 선형 회귀(Simple Linear Regression)에서만 사용

## 2. 역행렬 기법

- 전치와 역행렬 연산 수행 → 행렬 곱셈 결합
- QR 분해 → 베타 계수 값
- 수치적 안정성 : 알고리즘이 근사값에서 오류를 증폭시키지 않고 최소화하는 정도

## 3. 경사 하강법(Gradient Descent)

- 미분과 반복을 사용해 목적 함수에 대한 파라미터 집합을 최소화/최대화하는 최적화 기법
- 학습률(Learning Rate) : 경사 크기에 비례해 보폭의 크기 계산
- 도함수 → 경사 하강법 수행

# 4. 과대적합 및 분산

- 과대적합(Overfitting) : 훈련 데이터에만 너무 정확하게 맞아서 새로운 데이터에 대해 제대로 예측하지 못함
- 단순한 포인트 연결 모델 → 이상치 민감, 예측 분산 높음
- 모델 편향 → 데이터의 내용을 정확하게 맞추는 것이 아닌 특정 방법에 우선순위를 두는 것
- 과소적합(Underfitting)

# 5. 확률적 경사 하강법(Stochastic Gradient Descent)

- 각 반복마다 데이터셋에 있는 표본 하나만 사용해 훈련하는 것
- 배치 경사 하강법(Batch Gradient Descent) : 한 번에 모든 훈련 데이터를 사용해 훈련하는 것
- 미니배치 경사 하강법(Mini-batch Gradient Descent) : 각 반복마다 데이터셋에 있는 여러 개의 표본 사용

# 6. 상관 계수(Correlation Coefficient)

- 두 변수 사이 관계 강도를 -1 ~ 1 사이 값으로 측정
- 상관 계수 0에 가까움 → 상관관계가 없음
- 상관 계수 1에 가까움 → 강한 양의 상관관계
- 상관 계수 -1에 가까움 → 강한 음의 상관관계
- 피어슨 상관 계수(Pearson Correlation Coefficient)
- 상관 행렬(Correlation Matrix) : 데이터셋의 모든 변수 쌍 간의 상관 계수 확인

# 7. 통계적 유의성

- 상관 계수가 우연에 의한 것이 아니라는 확신 정량화 필요
- 상관 관계가 높고 p 값이 작음 → 정확도 증가

# 8. 결정 계수(Coefficient of Determination)

- 한 변수의 변동이 다른 변수의 변동으로 얼마나 설명 가능한지를 측정
- 상관 계수의 제곱

# 9. 추정 표준 오차(Standard Error of the Estimate)

- SSE 제곱근의 평균
- 제곱 오차 합(SSE, Sum of Squared Error) : 각 잔차를 제곱하고 합산한 것

# 10. 예측 구간(Prediction Interval)

- 각각의 y 예측 주위에 존재하는 신뢰 구간

# 11. 훈련/테스트 분할

- 과대적합 완화
- 데이터의 1/3 → 테스트용, 2/3 → 훈련용
- 훈련 데이터셋 → 선형 회귀 모델 훈련
- 테스트 데이터셋 → 이전에 본 적 없는 데이터에 대한 선형 회귀 모델 성능 측정
- 교차 검증 : 데이터의 일부가 번갈아 가며 테스트 데이터셋이 되는 것
- 랜덤 폴드 교차 검증(Random-Fold Cross Validation) : 데이터를 여러 번 반복해 섞어 훈련/테스트 분할 진행 → 테스트 결과 집계

# 12. 다중 선형 회귀

- 다변수 선형 회귀 적용 가능
- 변수 증가 → 데이터 희소 증가