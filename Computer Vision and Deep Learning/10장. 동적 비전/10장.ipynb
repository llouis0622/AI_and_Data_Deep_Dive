{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "\n",
    "def draw_OpticalFlow(img, flow, step=16):\n",
    "    for y in range(step // 2, frame.shape[0], step):\n",
    "        for x in range(step // 2, frame.shape[1], step):\n",
    "            dx, dy = flow[y, x].astype(np.int)\n",
    "            if (dx * dx + dy * dy) > 1:\n",
    "                cv.line(img, (x, y), (x + dx, y + dy), (0, 0, 255), 2)\n",
    "            else:\n",
    "                cv.line(img, (x, y), (x + dx, y + dy), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    sys.exit('카메라 연결 실패')\n",
    "\n",
    "prev = None\n",
    "\n",
    "while (1):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        sys('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "\n",
    "    if prev is None:\n",
    "        prev = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "        continue\n",
    "\n",
    "    curr = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    flow = cv.calcOpticalFlowFarneback(prev, curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    draw_OpticalFlow(frame, flow)\n",
    "    cv.imshow('Optical flow', frame)\n",
    "\n",
    "    prev = curr\n",
    "\n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture('slow_traffic_small.mp4')\n",
    "\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "color = np.random.randint(0, 255, (100, 3))\n",
    "\n",
    "ret, old_frame = cap.read()\n",
    "old_gray = cv.cvtColor(old_frame, cv.COLOR_BGR2GRAY)\n",
    "p0 = cv.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "mask = np.zeros_like(old_frame)\n",
    "\n",
    "while (1):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    new_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    p1, match, err = cv.calcOpticalFlowPyrLK(old_gray, new_gray, p0, None, **lk_params)\n",
    "\n",
    "    if p1 is not None:\n",
    "        good_new = p1[match == 1]\n",
    "        good_old = p0[match == 1]\n",
    "\n",
    "    for i in range(len(good_new)):\n",
    "        a, b = int(good_new[i][0]), int(good_new[i][1])\n",
    "        c, d = int(good_old[i][0]), int(good_old[i][1])\n",
    "        mask = cv.line(mask, (a, b), (c, d), color[i].tolist(), 2)\n",
    "        frame = cv.circle(frame, (a, b), 5, color[i].tolist(), -1)\n",
    "\n",
    "    img = cv.add(frame, mask)\n",
    "    cv.imshow('LTK tracker', img)\n",
    "    cv.waitKey(30)\n",
    "\n",
    "    old_gray = new_gray.copy()\n",
    "    p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "cv.destroyAllWindows()"
   ],
   "id": "f83cc7077363a564"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "from sort import Sort\n",
    "\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f = open('coco_names.txt', 'r')\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    model = cv.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "    layer_names = model.getLayerNames()\n",
    "    out_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "\n",
    "    return model, out_layers, class_names\n",
    "\n",
    "\n",
    "def yolo_detect(img, yolo_model, out_layers):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    test_img = cv.dnn.blobFromImage(img, 1.0 / 256, (448, 448), (0, 0, 0), swapRB=True)\n",
    "\n",
    "    yolo_model.setInput(test_img)\n",
    "    output3 = yolo_model.forward(out_layers)\n",
    "\n",
    "    box, conf, id = [], [], []\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores = vec85[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                centerx, centery = int(vec85[0] * width), int(vec85[1] * height)\n",
    "                w, h = int(vec85[2] * width), int(vec85[3] * height)\n",
    "                x, y = int(centerx - w / 2), int(centery - h / 2)\n",
    "                box.append([x, y, x + w, y + h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "\n",
    "    ind = cv.dnn.NMSBoxes(box, conf, 0.5, 0.4)\n",
    "    objects = [box[i] + [conf[i]] + [id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "\n",
    "model, out_layers, class_names = construct_yolo_v3()\n",
    "colors = np.random.uniform(0, 255, size=(100, 3))\n",
    "\n",
    "sort = Sort()\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    sys.exit('카메라 연결 실패')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "\n",
    "    res = yolo_detect(frame, model, out_layers)\n",
    "    persons = [res[i] for i in range(len(res)) if res[i][5] == 0]\n",
    "\n",
    "    if len(persons) == 0:\n",
    "        tracks = sort.update()\n",
    "    else:\n",
    "        tracks = sort.update(np.array(persons))\n",
    "\n",
    "    for i in range(len(tracks)):\n",
    "        x1, y1, x2, y2, track_id = tracks[i].astype(int)\n",
    "        cv.rectangle(frame, (x1, y1), (x2, y2), colors[track_id], 2)\n",
    "        cv.putText(frame, str(track_id), (x1 + 10, y1 + 40), cv.FONT_HERSHEY_PLAIN, 3, colors[track_id], 2)\n",
    "\n",
    "    cv.imshow('Person tracking by SORT', frame)\n",
    "\n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "b761bc3986742a96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "img = cv.imread('BSDS_376001.jpg')\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "res = face_detection.process(cv.cvtColor(img, cv.COLOR_BGR2RGB))\n",
    "\n",
    "if not res.detections:\n",
    "    print('얼굴 검출에 실패했습니다. 다시 시도하세요.')\n",
    "else:\n",
    "    for detection in res.detections:\n",
    "        mp_drawing.draw_detection(img, detection)\n",
    "    cv.imshow('Face detection by MediaPipe', img)\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "1678d547390322bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "\n",
    "    res = face_detection.process(cv.cvtColor(frame, cv.COLOR_BGR2RGB))\n",
    "\n",
    "    if res.detections:\n",
    "        for detection in res.detections:\n",
    "            mp_drawing.draw_detection(frame, detection)\n",
    "\n",
    "    cv.imshow('MediaPipe Face Detection from video', cv.flip(frame, 1))\n",
    "    if cv.waitKey(5) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "6e457252bd9fba5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "dice = cv.imread('dice.png', cv.IMREAD_UNCHANGED)\n",
    "dice = cv.resize(dice, dsize=(0, 0), fx=0.1, fy=0.1)\n",
    "w, h = dice.shape[1], dice.shape[0]\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=1, min_detection_confidence=0.5)\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "\n",
    "    res = face_detection.process(cv.cvtColor(frame, cv.COLOR_BGR2RGB))\n",
    "\n",
    "    if res.detections:\n",
    "        for det in res.detections:\n",
    "            p = mp_face_detection.get_key_point(det, mp_face_detection.FaceKeyPoint.RIGHT_EYE)\n",
    "            x1, x2 = int(p.x * frame.shape[1] - w // 2), int(p.x * frame.shape[1] + w // 2)\n",
    "            y1, y2 = int(p.y * frame.shape[0] - h // 2), int(p.y * frame.shape[0] + h // 2)\n",
    "            if x1 > 0 and y1 > 0 and x2 < frame.shape[1] and y2 < frame.shape[0]:\n",
    "                alpha = dice[:, :, 3:] / 255\n",
    "                frame[y1:y2, x1:x2] = frame[y1:y2, x1:x2] * (1 - alpha) + dice[:, :, :3] * alpha\n",
    "\n",
    "    cv.imshow('MediaPipe Face AR', cv.flip(frame, 1))\n",
    "    if cv.waitKey(5) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "d633a61d905a6de3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "mesh = mp_mesh.FaceMesh(max_num_faces=2, refine_landmarks=True, min_detection_confidence=0.5,\n",
    "                        min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "\n",
    "    res = mesh.process(cv.cvtColor(frame, cv.COLOR_BGR2RGB))\n",
    "\n",
    "    if res.multi_face_landmarks:\n",
    "        for landmarks in res.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(image=frame, landmark_list=landmarks, connections=mp_mesh.FACEMESH_TESSELATION,\n",
    "                                      landmark_drawing_spec=None,\n",
    "                                      connection_drawing_spec=mp_styles.get_default_face_mesh_tesselation_style())\n",
    "            mp_drawing.draw_landmarks(image=frame, landmark_list=landmarks, connections=mp_mesh.FACEMESH_CONTOURS,\n",
    "                                      landmark_drawing_spec=None,\n",
    "                                      connection_drawing_spec=mp_styles.get_default_face_mesh_contours_style())\n",
    "            mp_drawing.draw_landmarks(image=frame, landmark_list=landmarks, connections=mp_mesh.FACEMESH_IRISES,\n",
    "                                      landmark_drawing_spec=None,\n",
    "                                      connection_drawing_spec=mp_styles.get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "    cv.imshow('MediaPipe Face Mesh', cv.flip(frame, 1))\n",
    "    if cv.waitKey(5) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "6a2c139965403ddb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_hand = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hand = mp_hand.Hands(max_num_hands=2, static_image_mode=False, min_detection_confidence=0.5,\n",
    "                     min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "\n",
    "    res = hand.process(cv.cvtColor(frame, cv.COLOR_BGR2RGB))\n",
    "\n",
    "    if res.multi_hand_landmarks:\n",
    "        for landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, landmarks, mp_hand.HAND_CONNECTIONS,\n",
    "                                      mp_styles.get_default_hand_landmarks_style(),\n",
    "                                      mp_styles.get_default_hand_connections_style())\n",
    "\n",
    "    cv.imshow('MediaPipe Hands', cv.flip(frame, 1))\n",
    "    if cv.waitKey(5) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "503f7f93818e9751"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_styles = mp.solutions.drawing_styles\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False, enable_segmentation=True, min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        break\n",
    "\n",
    "    res = pose.process(cv.cvtColor(frame, cv.COLOR_BGR2RGB))\n",
    "\n",
    "    mp_drawing.draw_landmarks(frame, res.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                              landmark_drawing_spec=mp_styles.get_default_pose_landmarks_style())\n",
    "\n",
    "    cv.imshow('MediaPipe pose', cv.flip(frame, 1))\n",
    "    if cv.waitKey(5) == ord('q'):\n",
    "        mp_drawing.plot_landmarks(res.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "7973af04d57c7e10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
