{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "n_class = 10\n",
    "img_siz = (32, 32, 3)\n",
    "\n",
    "patch_siz = 4\n",
    "p2 = (img_siz[0] // patch_siz) ** 2\n",
    "d_model = 64\n",
    "h = 8\n",
    "N = 6\n",
    "\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.p_siz = patch_size\n",
    "\n",
    "    def call(self, img):\n",
    "        batch_size = tf.shape(img)[0]\n",
    "        patches = tf.image.extract_patches(images=img, sizes=[1, self.p_siz, self.p_siz, 1],\n",
    "                                           strides=[1, self.p_siz, self.p_siz, 1], rates=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, p2, d_model):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.p2 = p2\n",
    "        self.projection = layers.Dense(units=d_model)\n",
    "        self.position_embedding = layers.Embedding(input_dim=p2, output_dim=d_model)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.p2, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def create_vit_classifier():\n",
    "    input = layers.Input(shape=(img_siz))\n",
    "    nor = layers.Normalization()(input)\n",
    "\n",
    "    patches = Patches(patch_siz)(nor)\n",
    "    x = PatchEncoder(p2, d_model)(patches)\n",
    "\n",
    "    for _ in range(N):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x2 = layers.MultiHeadAttention(num_heads=h, key_dim=d_model // h, dropout=0.1)(x1, x1)\n",
    "        x3 = layers.Add()([x2, x])\n",
    "        x4 = layers.LayerNormalization(epsilon=1e-6)(x3)\n",
    "        x5 = layers.Dense(d_model * 2, activation=tf.nn.gelu)(x4)\n",
    "        x6 = layers.Dropout(0.1)(x5)\n",
    "        x7 = layers.Dense(d_model, activation=tf.nn.gelu)(x6)\n",
    "        x8 = layers.Dropout(0.1)(x7)\n",
    "        x = layers.Add()([x8, x3])\n",
    "\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(2048, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(1024, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        output = layers.Dense(n_class, activation='softmax')(x)\n",
    "\n",
    "        model = keras.Model(inputs=input, outputs=output)\n",
    "        return model\n",
    "\n",
    "\n",
    "model = create_vit_classifier()\n",
    "model.layers[1].adapt(x_train)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('정확률 = ', res[1] * 100)\n",
    "\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Accuracy graph')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Loss graph')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "n_class = 10\n",
    "img_siz = (32, 32, 3)\n",
    "img_expanded_siz = (72, 72, 3)\n",
    "\n",
    "patch_siz = 6\n",
    "p2 = (img_expanded_siz[0] // patch_siz) ** 2\n",
    "d_model = 64\n",
    "h = 8\n",
    "N = 6\n",
    "\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.p_siz = patch_size\n",
    "\n",
    "    def call(self, img):\n",
    "        batch_size = tf.shape(img)[0]\n",
    "        patches = tf.image.extract_patches(images=img, sizes=[1, self.p_siz, self.p_siz, 1],\n",
    "                                           strides=[1, self.p_siz, self.p_siz, 1], rates=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, p2, d_model):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.p2 = p2\n",
    "        self.projection = layers.Dense(units=d_model)\n",
    "        self.position_embedding = layers.Embedding(input_dim=p2, output_dim=d_model)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.p2, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "def create_vit_classifier():\n",
    "    input = layers.Input(shape=(img_siz))\n",
    "    nor = layers.Normalization()(input)\n",
    "    exp = layers.Resizing(img_expanded_siz[0], img_expanded_siz[1])(nor)\n",
    "\n",
    "    x = layers.RandomFlip('horizontal')(exp)\n",
    "    x = layers.RandomRotation(factor=0.02)(x)\n",
    "    x = layers.RandomZoom(height_factor=0.2, width_factor=0.2)(x)\n",
    "\n",
    "    patches = Patches(patch_siz)(x)\n",
    "    x = PatchEncoder(p2, d_model)(patches)\n",
    "\n",
    "    for _ in range(N):\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x2 = layers.MultiHeadAttention(num_heads=h, key_dim=d_model // h, dropout=0.1)(x1, x1)\n",
    "        x3 = layers.Add()([x2, x])\n",
    "        x4 = layers.LayerNormalization(epsilon=1e-6)(x3)\n",
    "        x5 = layers.Dense(d_model * 2, activation=tf.nn.gelu)(x4)\n",
    "        x6 = layers.Dropout(0.1)(x5)\n",
    "        x7 = layers.Dense(d_model, activation=tf.nn.gelu)(x6)\n",
    "        x8 = layers.Dropout(0.1)(x7)\n",
    "        x = layers.Add()([x8, x3])\n",
    "\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(2048, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(1024, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        output = layers.Dense(n_class, activation='softmax')(x)\n",
    "\n",
    "        model = keras.Model(inputs=input, outputs=output)\n",
    "        return model\n",
    "\n",
    "\n",
    "model = create_vit_classifier()\n",
    "model.layers[1].adapt(x_train)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "res = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('정확률 = ', res[1] * 100)\n",
    "\n",
    "plt.plot(hist.history['accuracy'])\n",
    "plt.plot(hist.history['val_accuracy'])\n",
    "plt.title('Accuracy graph')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Loss graph')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "6aa4b621a98c9b0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import ViTFeatureExtractor, TFViTForImageClassification\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = [Image.open('BSDS_242078.jpg'), Image.open('BSDS_361010.jpg'), Image.open('BSDS_376001.jpg')]\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "model = TFViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "inputs = feature_extractor(img, return_tensors='tf')\n",
    "res = model(**inputs)\n",
    "\n",
    "for i in range(res.logits.shape[0]):\n",
    "    plt.imshow(img[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    predicted_label = int(tf.math.argmax(res.logits[i], axis=-1))\n",
    "    prob = float(tf.nn.softmax(res.logits[i])[predicted_label] * 100.0)\n",
    "    print(i, '번째 영상의 1순위 부류 : ', model.config.id2label[predicted_label], prob)"
   ],
   "id": "c7250914993f919b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "img = Image.open('BSDS_361010.jpg')\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50')\n",
    "model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-50')\n",
    "\n",
    "inputs = feature_extractor(img, return_tensors='pt')\n",
    "res = model(**inputs)\n",
    "\n",
    "colors = np.random.uniform(0, 255, size=(100, 3))\n",
    "im = cv.cvtColor(np.array(img), cv.COLOR_BGR2RGB)\n",
    "for i in range(res.logits.shape[1]):\n",
    "    predicted_label = res.logits[0, i].argmax(-1).item()\n",
    "    if predicted_label != 91:\n",
    "        name = model.config.id2label[predicted_label]\n",
    "        prob = '{:.2f}'.format(float(res.logits[0, i].softmax(dim=0)[predicted_label]))\n",
    "        cx, cy = int(481 * res.pred_boxes[0, i, 0]), int(321 * res.pred_boxes[0, i, 1])\n",
    "        w, h = int(481 * res.pred_boxes[0, i, 2]), int(321 * res.pred_boxes[0, i, 3])\n",
    "        cv.rectangle(im, (cx - w // 2, cy - h // 2), (cx + w // 2, cy + h // 2), colors[predicted_label], 2)\n",
    "        cv.putText(im, name + str(prob), (cx - w // 2, cy - h // 2 - 5), cv.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                   colors[predicted_label], 1)\n",
    "\n",
    "cv.imshow('DETR', im)\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "a575f858106e5aa6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img = Image.open('BSDS_361010.jpg')\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "captions = ['Two horses are running on grass', 'Students are eating', 'Croquet playing on horses',\n",
    "            'Golf playing on horses']\n",
    "inputs = processor(text=captions, images=img, return_tensors='pt', padding=True)\n",
    "res = model(**inputs)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "logits = res.logits_per_image\n",
    "probs = logits.softmax(dim=1)\n",
    "for i in range(len(captions)):\n",
    "    print(captions[i], ' : ', '{:.2f}'.format(float(probs[0, i] * 100.0)))"
   ],
   "id": "43f2546af704b8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
