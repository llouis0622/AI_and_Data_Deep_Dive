{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f = open('coco_names.txt', 'r')\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    model = cv.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "    layer_names = model.getLayerNames()\n",
    "    out_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "\n",
    "    return model, out_layers, class_names\n",
    "\n",
    "\n",
    "def yolo_detect(img, yolo_model, out_layers):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    test_img = cv.dnn.blobFromImage(img, 1.0 / 256, (448, 448), (0, 0, 0), swapRB=True)\n",
    "\n",
    "    yolo_model.setInput(test_img)\n",
    "    output3 = yolo_model.forward(out_layers)\n",
    "\n",
    "    box, conf, id = [], [], []\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores = vec85[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                centerx, centery = int(vec85[0] * width), int(vec85[1] * height)\n",
    "                w, h = int(vec85[2] * width), int(vec85[3] * height)\n",
    "                x, y = int(centerx - w / 2), int(centery - h / 2)\n",
    "                box.append([x, y, x + w, y + h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "\n",
    "    ind = cv.dnn.NMSBoxes(box, conf, 0.5, 0.4)\n",
    "    objects = [box[i] + [conf[i]] + [id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "\n",
    "model, out_layers, class_names = construct_yolo_v3()\n",
    "colors = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "img = cv.imread('soccer.jpg')\n",
    "if img is None:\n",
    "    sys.exit('파일이 없습니다.')\n",
    "\n",
    "res = yolo_detect(img, model, out_layers)\n",
    "\n",
    "for i in range(len(res)):\n",
    "    x1, y1, x2, y2, confidence, id = res[i]\n",
    "    text = str(class_names[id]) + '%.3f' % confidence\n",
    "    cv.rectangle(img, (x1, y1), (x2, y2), colors[id], 2)\n",
    "    cv.putText(img, text, (x1, y1 + 30), cv.FONT_HERSHEY_PLAIN, 1.5, colors[id], 2)\n",
    "\n",
    "cv.imshow(\"Object detection by YOLO v.3\", img)\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f = open('coco_names.txt', 'r')\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    model = cv.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "    layer_names = model.getLayerNames()\n",
    "    out_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "\n",
    "    return model, out_layers, class_names\n",
    "\n",
    "\n",
    "def yolo_detect(img, yolo_model, out_layers):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    test_img = cv.dnn.blobFromImage(img, 1.0 / 256, (448, 448), (0, 0, 0), swapRB=True)\n",
    "\n",
    "    yolo_model.setInput(test_img)\n",
    "    output3 = yolo_model.forward(out_layers)\n",
    "\n",
    "    box, conf, id = [], [], []\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores = vec85[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                centerx, centery = int(vec85[0] * width), int(vec85[1] * height)\n",
    "                w, h = int(vec85[2] * width), int(vec85[3] * height)\n",
    "                x, y = int(centerx - w / 2), int(centery - h / 2)\n",
    "                box.append([x, y, x + w, y + h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "\n",
    "    ind = cv.dnn.NMSBoxes(box, conf, 0.5, 0.4)\n",
    "    objects = [box[i] + [conf[i]] + [id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "\n",
    "model, out_layers, class_names = construct_yolo_v3()\n",
    "colors = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    sys.exit('카메라 연결 실패')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "\n",
    "    res = yolo_detect(frame, model, out_layers)\n",
    "\n",
    "    for i in range(len(res)):\n",
    "        x1, y1, x2, y2, confidence, id = res[i]\n",
    "        text = str(class_names[id]) + '%.3f' % confidence\n",
    "        cv.rectangle(frame, (x1, y1), (x2, y2), colors[id], 2)\n",
    "        cv.putText(frame, text, (x1, y1 + 30), cv.FONT_HERSHEY_PLAIN, 1.5, colors[id], 2)\n",
    "\n",
    "    cv.imshow(\"Object detection from video by YOLO v.3\", frame)\n",
    "\n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q'): break\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "dc9780ac44baf96a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f = open('coco_names.txt', 'r')\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    model = cv.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "    layer_names = model.getLayerNames()\n",
    "    out_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "\n",
    "    return model, out_layers, class_names\n",
    "\n",
    "\n",
    "def yolo_detect(img, yolo_model, out_layers):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    test_img = cv.dnn.blobFromImage(img, 1.0 / 256, (448, 448), (0, 0, 0), swapRB=True)\n",
    "\n",
    "    yolo_model.setInput(test_img)\n",
    "    output3 = yolo_model.forward(out_layers)\n",
    "\n",
    "    box, conf, id = [], [], []\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores = vec85[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:\n",
    "                centerx, centery = int(vec85[0] * width), int(vec85[1] * height)\n",
    "                w, h = int(vec85[2] * width), int(vec85[3] * height)\n",
    "                x, y = int(centerx - w / 2), int(centery - h / 2)\n",
    "                box.append([x, y, x + w, y + h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "\n",
    "    ind = cv.dnn.NMSBoxes(box, conf, 0.5, 0.4)\n",
    "    objects = [box[i] + [conf[i]] + [id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "\n",
    "model, out_layers, class_names = construct_yolo_v3()\n",
    "colors = np.random.uniform(0, 255, size=(len(class_names), 3))\n",
    "\n",
    "cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "if not cap.isOpened():\n",
    "    sys.exit('카메라 연결 실패')\n",
    "\n",
    "start = time.time()\n",
    "n_frame = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "\n",
    "    res = yolo_detect(frame, model, out_layers)\n",
    "\n",
    "    for i in range(len(res)):\n",
    "        x1, y1, x2, y2, confidence, id = res[i]\n",
    "        text = str(class_names[id]) + '%.3f' % confidence\n",
    "        cv.rectangle(frame, (x1, y1), (x2, y2), colors[id], 2)\n",
    "        cv.putText(frame, text, (x1, y1 + 30), cv.FONT_HERSHEY_PLAIN, 1.5, colors[id], 2)\n",
    "\n",
    "    cv.imshow(\"Object detection from video by YOLO v.3\", frame)\n",
    "    n_frame += 1\n",
    "\n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "end = time.time()\n",
    "print('처리한 프레임 수 = ', n_frame, ', 경과 시간 = ', end - start, '\\n초당 프레임 수 = ', n_frame / (end - start))\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "62bbeec63db7faf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import random\n",
    "import cv2 as cv\n",
    "\n",
    "input_dir = './datasets/oxford_pets/images/images/'\n",
    "target_dir = './datasets/oxford_pets/annotations/annotations/trimaps/'\n",
    "img_siz = (160, 160)\n",
    "n_class = 3\n",
    "batch_siz = 32\n",
    "\n",
    "img_paths = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.jpg')])\n",
    "label_paths = sorted(\n",
    "    [os.path.join(target_dir, f) for f in os.listdir(target_dir) if f.endswith('.png') and not f.startswith('.')])\n",
    "\n",
    "\n",
    "class OxfordPets(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, img_size, img_paths, label_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_paths = img_paths\n",
    "        self.label_paths = label_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_img_paths = self.img_paths[i:i + self.batch_size]\n",
    "        batch_label_paths = self.label_paths[i:i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_label_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "            y[j] = np.expand_dims(img, 2)\n",
    "            y[j] -= 1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def make_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    previous_block_activation = x\n",
    "\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding='same')(previous_block_activation)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding='same')(residual)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation='softmax', padding='same')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_model(img_siz, n_class)\n",
    "\n",
    "random.Random(1).shuffle(img_paths)\n",
    "random.Random(1).shuffle(label_paths)\n",
    "test_samples = int(len(img_paths) * 0.1)\n",
    "train_img_paths = img_paths[:-test_samples]\n",
    "train_label_paths = label_paths[:-test_samples]\n",
    "test_img_paths = img_paths[-test_samples:]\n",
    "test_label_paths = label_paths[-test_samples:]\n",
    "\n",
    "train_gen = OxfordPets(batch_siz, img_siz, train_img_paths, train_label_paths)\n",
    "test_gen = OxfordPets(batch_siz, img_siz, test_img_paths, test_label_paths)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "cb = [keras.callbacks.ModelCheckpoint('oxford_seg.h5', save_best_only=True)]\n",
    "model.fit(train_gen, epochs=30, validation_data=test_gen, callbacks=cb)\n",
    "\n",
    "preds = model.predict(test_gen)\n",
    "\n",
    "cv.imshow('Sample image', cv.imread(test_img_paths[0]))\n",
    "cv.imshow('Segmentation label', cv.imread(test_label_paths[0]) * 64)\n",
    "cv.imshow('Segmentation prediction', preds[0])\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "7acd706159dbe6fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pixellib.semantic import semantic_segmentation\n",
    "import cv2 as cv\n",
    "\n",
    "seg = semantic_segmentation()\n",
    "seg.load_ade20k_model('deeplabv3_xception65_ade20k.h5')\n",
    "\n",
    "img_fname = 'busy_street.jpg'\n",
    "seg.segmentAsAde20k(img_fname, output_image_name='image_new.jpg')\n",
    "info1, img_segmented1 = seg.segmentAsAde20k(img_fname)\n",
    "info2, img_segmented2 = seg.segmentAsAde20k(img_fname, overlay=True)\n",
    "\n",
    "cv.imshow('Image original', cv.imread(img_fname))\n",
    "cv.imshow('Image segmention', img_segmented1)\n",
    "cv.imshow('Image segmention overlayed', img_segmented2)\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "1e45131eacaa851f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pixellib.semantic import semantic_segmentation\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "seg_video = semantic_segmentation()\n",
    "seg_video.load_ade20k_model('deeplabv3_xception65_ade20k.h5')\n",
    "\n",
    "seg_video.process_camera_ade20k(cap, overlay=True, frames_per_second=2, output_video_name='output_video.mp4',\n",
    "                                show_frames=True, frame_name='Pixellib')\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "82af50a4949952c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pixellib.instance import instance_segmentation\n",
    "import cv2 as cv\n",
    "\n",
    "seg = instance_segmentation()\n",
    "seg.load_model(\"mask_rcnn_coco.h5\")\n",
    "\n",
    "img_fname = 'busy_street.jpg'\n",
    "info, img_segmented = seg.segmentImage(img_fname, show_bboxes=True)\n",
    "\n",
    "cv.imshow('Image segmention overlayed', img_segmented)\n",
    "\n",
    "cv.waitKey()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "e5598a87ec2f05b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pixellib.instance import instance_segmentation\n",
    "import cv2 as cv\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "seg_video = instance_segmentation()\n",
    "seg_video.load_model(\"mask_rcnn_coco.h5\")\n",
    "\n",
    "target_class = seg_video.select_target_classes(person=True, book=True)\n",
    "seg_video.process_camera(cap, segment_target_classes=target_class, frames_per_second=2, show_frames=True,\n",
    "                         frame_name='Pixellib', show_bboxes=True)\n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()"
   ],
   "id": "efcd840cafdbf940"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PyQt5.QtWidgets import *\n",
    "import sys\n",
    "from pixellib.tune_bg import alter_bg\n",
    "\n",
    "\n",
    "class VideoSpecialEffect(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle('배경을 내 맘대로')\n",
    "        self.setGeometry(200, 200, 400, 100)\n",
    "\n",
    "        videoButton = QPushButton('배경 내 맘대로 켜기', self)\n",
    "        self.pickCombo = QComboBox(self)\n",
    "        self.pickCombo.addItems(['원래 영상', '흐릿(조금)', '흐릿(중간)', '흐릿(많이)', '빨강', '녹색', '파랑'])\n",
    "        quitButton = QPushButton('나가기', self)\n",
    "\n",
    "        videoButton.setGeometry(10, 10, 140, 30)\n",
    "        self.pickCombo.setGeometry(150, 10, 110, 30)\n",
    "        quitButton.setGeometry(280, 10, 100, 30)\n",
    "\n",
    "        videoButton.clicked.connect(self.videoSpecialEffectFunction)\n",
    "        quitButton.clicked.connect(self.quitFunction)\n",
    "\n",
    "    def videoSpecialEffectFunction(self):\n",
    "        self.cap = cv.VideoCapture(0, cv.CAP_DSHOW)\n",
    "        if not self.cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret: break\n",
    "\n",
    "            pick_effect = self.pickCombo.currentIndex()\n",
    "            if pick_effect == 0:\n",
    "                special_img = frame\n",
    "            elif pick_effect == 1:\n",
    "                special_img = change_bg.blur_frame(frame, low=True, detect='person')\n",
    "            elif pick_effect == 2:\n",
    "                special_img = change_bg.blur_frame(frame, moderate=True, detect='person')\n",
    "            elif pick_effect == 3:\n",
    "                special_img = change_bg.blur_frame(frame, extreme=True, detect='person')\n",
    "            elif pick_effect == 4:\n",
    "                special_img = change_bg.color_frame(frame, colors=(255, 0, 0), detect='person')\n",
    "            elif pick_effect == 5:\n",
    "                special_img = change_bg.color_frame(frame, colors=(0, 255, 0), detect='person')\n",
    "            elif pick_effect == 6:\n",
    "                special_img = change_bg.color_frame(frame, colors=(0, 0, 255), detect='person')\n",
    "\n",
    "            cv.imshow('Special effect', special_img)\n",
    "            cv.waitKey(1)\n",
    "\n",
    "    def quitFunction(self):\n",
    "        self.cap.release()\n",
    "        cv.destroyAllWindows()\n",
    "        self.close()\n",
    "\n",
    "\n",
    "change_bg = alter_bg(model_type=\"pb\")\n",
    "change_bg.load_pascalvoc_model('xception_pascalvoc.pb')\n",
    "\n",
    "app = QApplication(sys.argv)\n",
    "win = VideoSpecialEffect()\n",
    "win.show()\n",
    "app.exec_()"
   ],
   "id": "7e1230a9a8e725fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
