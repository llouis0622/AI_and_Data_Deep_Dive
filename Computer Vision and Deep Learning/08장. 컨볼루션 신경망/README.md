# 1. 발상과 전개

## 1. 발상

- 깊은 다층 퍼셉트론 → 완전연결 구조 층 구성
- 인간 시각의 수용장
- 네오코그니트론

## 2. 딥러닝의 성공 요인

- 데이터셋의 증가 → 학습 알고리즘이 추정해야 하는 매개변수가 늘어나 큰 데이터셋 필요
- GPU 병렬 처리 → 학습 시간 대폭 빨라짐
- 좋은 학습 알고리즘 개발

# 2. 컨볼루션 신경망의 구조

## 1. 컨볼루션층과 풀링층

- 컨볼루션층
    - 입력 특징 맵에 컨볼루션 적용해서 얻은 특징 맵 출력, 패딩, 스트라이드 추가 지정
    - 가중치 공유, 부분 연결성 특성 보유
- 풀링층 : 보통 최대 풀링 수행

## 2. 빌딩블록을 쌓아 만드는 컨볼루션 신경망

- 컨볼루션 신경망 : 컨볼루션층 + 풀링층 번갈아 쌓아 만듦
- LeNet-5 : C-P-C-P-C-FC-FC 구조
- 풀어야 하는 문제에 따라 다양한 모양으로 조립 가능 → 컨볼루션층, 풀링층, 완전연결층의 자유로움

# 3. 컨볼루션 신경망의 학습

## 1. 컨볼루션 신경망을 위한 역전파 알고리즘

- 전방 계산 : 컨볼루션층, 풀링층, 완전연결층을 거쳐 출력 벡터 출력
- 손실 함수 계산 : 손실 함수를 통해 출력 벡터와 참값 벡터의 오류 계산
- 역전파 : 오류를 줄이는 방향으로 가중치 갱신

## 2. 특징 학습과 통째 학습

- 특징 학습 : 컨볼루션 신경망 특징
- 통째 학습 : 특징 추출 + 분류를 같이 학습

## 3. 컨볼루션 신경망이 우수한 이유

- 데이터 원래 구조 유지
- 특징 학습을 통해 최적의 특징 추출
- 신경망의 깊이 깊게 가능

# 4. 컨볼루션 신경망 구현

## 1. LeNet-5의 재현

- 코드 구현

## 2. 자연 영상 인식

- 코드 구현

## 3. 텐서플로 프로그래밍

- models 모듈 : Sequential과 Functional API
- layers 모듈 : Dense, Conv2D, MaxPooling2D, Flatten, Dropout 등
- losses 모듈 : MSE, categorial_crossentropy 등
- optimizers 모듈 : SGD, Adam, AdaGrad, RMSprop 등

# 5. 딥러닝의 학습 알고리즘 향상

## 1. 손실 함수

- 크로스 엔트로피 : 두 확률 분포가 얼마나 다른지 측정
- Focal 손실 함수 : 부류 불균형이 심한 경우 주로 사용

## 2. 옵티마이저

- 모멘텀 : 이전 운동량을 현재에 반영
- 적응적 학습률 : 가중치 갱신 규칙, 세대와 그레이디언트에 따라 적응적으로 학습률 결정
    - AdaGrad : 이전 그레이디언트를 누적한 정보를 이용해 학습률을 적응적으로 결정
    - RMSprop : 이전 그레이디언트를 누적할 때 오래될수록 영향력을 줄여가는 정책으로 개선
    - Adam : RMSprop에 모멘텀 적용

## 3. 규제

- 과잉 적합 : 훈련에 참여하지 않은 새로운 데이터에 대해 아주 낮은 성능을 보이는 것, 일반화 성능 매우 낮음
- 데이터 증강 : 주어진 훈련 집합을 조금씩 변형하여 인위적으로 데이터 수 늘림
- 드롭아웃 : 특징 맵을 구성하는 요소 중 일부를 랜덤 선택하여 0으로 설정
- 조기 멈춤 : 과잉 적합이 나타날 시 조기에 학습을 멈추는 것

# 6. 전이 학습

## 1. 백본 모델

- 사전 학습 모델 : 대용량 데이터로 미리 학습되어 있어 전이 학습에 활용할 수 있는 모델
- 백본 모델 : 사전 학습 모델을 특정 응용에 전이 학습한 것
- VGGNet : 2014년 ILSVRC 대회 준우승
- GoogleLeNet : 2014년 ILSVRC 대회 우승
- ResNet : 2015년 ILSVRC 대회 우승

## 2. 사전 학습 모델로 자연 영상 인식

- 코드 구현

## 3. 사전 학습 모델로 견종 인식

- 코드 구현