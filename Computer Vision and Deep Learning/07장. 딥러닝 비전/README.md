# 1. 방법론의 대전환

## 1. 규칙 기반의 한계

- 규칙으로 알고리즘 표현
- 사람의 노력으로 일정 수준의 성능 달성 가능 → 그 이상 돌파 불가능

## 2. 딥러닝으로 대전환

- 기계 학습 : 주어진 문제 도메인에서 데이터를 수집하고 모델을 학습하는 과정을 거쳐 문제 해결
- 신경망 모델 : 얕은 신경망 모델 → 훨씬 많은 층을 배치한 깊은 신경망 모델로 발전
- 비신경망 모델 : SVM, 결정 트리, 랜덤 포레스트

# 2. 기계 학습 기초

## 1. 기계 학습의 단순한 예

- 모델 : 함수
- 학습 : 수집한 데이터로 방정식을 풀어 함수를 알아내는 일

## 2. 기계 학습의 4단계

- 데이터 수집
- 모델 선택
- 학습
- 예측

# 3. 딥러닝 소프트웨어 맛보기

## 1. 텐서플로 소개

- 딥러닝 구현 도구
- 텐서플로 + 케라스

## 2. 데이터와 텐서

- 텐서플로 → 다양한 데이터셋 제공

# 4. 인공 신경망의 태동

## 1. 신경망의 간략 역사

- 1900년대 인간 뇌에 대한 연구 → 뉴런의 정체 밝혀짐
- 1946년 최초의 전자식 컴퓨터 에니악 탄생
- 1943년 뉴런의 정보 처리를 모방한 수학 모델 발표
- 1958년 퍼셉트론 최초 구현
- 1960년대 신경망이 인공지능을 완성해줄 것처럼 과장된 매스컴 주목 받음
- 1969년 퍼셉트론 → XOR 문제조차 풀지 못함을 밝힘
- 1986년 퍼셉트론 → 다층 퍼셉트론과 역전파 알고리즘
- 1990년대 비신경망 모델 등장
- 2000년대 딥러닝 등장 → 신경망 주류 기술로 도약

## 2. 퍼셉트론

- 특징 벡터를 입력으로 받아 연산 수행 → 결과를 출력층으로 내보냄
- 입력 노드 : 특징값을 통과시키는 일
- 출력 노드 : 연산 수행
- 활성화 함수 : 계단 함수 사용
- 특징 공간 → 2개의 부분 공간으로 나누는 함수로 해석 가능

# 5. 깊은 다층 퍼셉트론

## 1. 다층 퍼셉트론

- 퍼셉트론 여러 개 사용
- 퍼셉트론 2개 사용 → 특징 공간 3개 영역으로 분할 가능
- 입력층 - 은닉층 - 출력층
- 입력층 : 특징 통과
- 은닉층, 출력층 : 연산 수행
- 전방 계산 : 특징 벡터가 입력층으로 들어가 은닉층과 출력층을 거치면서 순차적으로 연산을 수행하는 과정

## 2. 딥러닝의 서막 : 깊은 다층 퍼셉트론

- 깊은 다층 퍼셉트론 → 전방 계산
- 신경망 출력 → 부류 정보로 해석

## 3. 활성 함수

- Step Function
- Logistic Sigmoid
- Hyperbolic Tangent
- ReLU
- Leaky ReLU
- Softplus
- Softmax
- Swish

# 6. 학습 알고리즘

## 1. 발상

- 가중치 행렬 난수로 초기화
- 가중치 행렬로 전방 계산 수행 → 손실 함수 계산
- 개선 불가능 → 중지
- 그레이디언트가 낮아지는 방향으로 가중치 계산

## 2. 스토카스틱 경사하강법

- 경사하강법 : 미분값을 보고 함수가 낮아지는 방향을 찾아 이동하는 일 반복 → 최저점에 도달하는 알고리즘
- SGD : 미니 배치를 사용하는 확장된 경사하강법

# 7. 다층 퍼셉트론 구현하기

- 필기 숫자 인식
- 성능 시각화
- 하이퍼 매개변수 다루기
- 자연 영상 인식
- 우편 번호 인식기