# 1. 신경망

- 클래스(Class) : 머신 러닝에서 분류 문제의 범주(Category)
- 샘플(Sample) : 데이터 포인트
- 레이블(Label) : 특정 샘플의 클래스
- 훈련 세트(Train Set), 테스트 세트(Test Set)
- 층(Layer) : 데이터를 위한 필터, 주어진 문제에 더 의미 있는 표현을 입력된 데이터로부터 추출
- 옵티마이저(Optimizer) : 성능을 향상시키기 위해 입력된 데이터를 기반으로 모델 업데이트
- 손실 함수(Loss Function) : 훈련 데이터에서 모델의 성능을 측정하는 방법
- 훈련과 테스트 과정을 모니터링 할 지표

# 2. 신경망을 위한 데이터 표현

- 텐서(Tensor) : 다차원 넘파이 배열에 데이터를 저장하는 것, 데이터를 위한 컨테이너

## 1. 스칼라(랭크-0 텐서)

- 하나의 숫자만 담고 있는 텐서
- float32, float64 타입의 숫자

## 2. 벡터(랭크-1 텐서)

- 숫자의 배열
- 딱 하나의 축 가짐

## 3. 행렬(랭크-2 텐서)

- 벡터의 배열
- 2개의 축 → 행, 열

## 4. 랭크-3 텐서와 더 높은 랭크의 텐서

- 숫자로 채워진 직육면체 형태로 해석 가능
- 딥러닝 → 랭크 0에서 4까지의 텐서를 다룸

## 5. 핵심 속성

- 축의 개수(랭크) : ndim 속성에 저장
- 크기 : 텐서의 각 축을 따라 얼마나 많은 차원이 있는지를 나타낸 파이썬의 튜플
- 데이터 타입 : 텐서에 포함된 데이터의 타입

## 6. 넘파이로 텐서 조작하기

- 슬라이싱(Slicing) : 배열에 있는 특정 원소들을 선택하는 것
- 각 배열의 축을 따라 인덱스 사이 선택 가능

## 7. 배치 데이터

- 샘플 축(Sample Axis) : 딥러닝에서 사용하는 모든 데이터 텐서의 첫 번째 축
- 배치 축(Batch Axis) : 배치 데이터의 첫 번째 축, 배치 차원(Batch Dimension)

## 8. 텐서의 실제 사례

- 벡터 데이터 : (samples, features) 크기의 랭크-2 텐서, 수치 속성으로 구성된 벡터
- 시계열 데이터/시퀀스 데이터 : (samples, timesteps, features) 크기의 랭크-3 텐서, 특성 벡터의 시퀀스
- 이미지 : (samples, height, width, channels) or (samples, channels, height, width) 크기의 랭크-4 텐서, 샘플 → 픽셀의 2D 격자, 픽셀 → 수치 값의 벡터
- 동영상 : (samples, frames, height, width, channels) or (samples, frames, channels, height, width) 크기의 랭크-5 텐서, 샘플 → 이미지 시퀀스

## 9. 벡터 데이터

- 첫 번째 축 → 샘플 축
- 두 번째 축 → 특성 축

## 10. 시계열 데이터 또는 시퀀스 데이터

- 데이터에서 시간이 중요할 때 → 시간 축을 포함한 랭크-3 텐서
- 시간 축 → 관례적으로 항상 두 번째 축

## 11. 이미지 데이터

- 이미지 → 높이, 너비, 컬러 채널의 3차원
- 채널 마지막 방식(Channel-Last) : 컬러 채널의 깊이를 끝에 놓음
- 채널 우선 방식(Channel-First) : 컬러 채널의 깊이를 배치 축 바로 뒤에 놓음

## 12. 비디오 데이터

- 프레임 → 랭크-3 텐서
- 프레임의 연속 → 랭크-4 텐서
- 비디오 → 랭크-5 텐서

# 3. 신경망의 톱니바퀴 : 텐서 연산(Tensor Operation)

## 1. 원소별 연산(Element-wise Operation)

- 텐서에 있는 각 원소에 독립적으로 적용
- BLAS(Basic Linear Algebra Subporgram) : 고도로 병렬화되고 효율적인 저수준 텐서 조작 루틴

## 2. 브로드캐스팅(Broadcasting)

- 모호하지 않고 실행 가능할 때 작은 텐서가 큰 텐서의 크기에 맞추는 것

## 3. 텐서 곱셈(Tensor Product)

- `np.dot` 함수 적용
- 행렬 곱셈과 동일

## 4. 텐서 크기 변환(Tensor Reshaping)

- 특정 크기에 맞게 열과 행 재배열
- 원래 텐서와 원소 개수 동일
- 전치(Transposition) : 행과 열을 바꾸는 것

## 5. 텐서 연산의 기하학적 해석

- 어떤 기하학적 공간에 있는 좌표 포인트로 해석 가능
- 이동(Translation) : 한 점에 벡터를 더해 고정된 방향으로 고정된 양만큼 점 이동
- 회전(Rotation) : 특정 각도만큼 2D 벡터를 반시계 방향 회전
- 크기 변경(Scaling) : 점곱을 통해 수직, 수평 방향으로 크기 변경
- 기울이기(Skewing)
- 선형 변환(Linear Transform) : 임의의 행렬과 점곱, 변경, 회전
- 아핀 변환(Affine Transform) : 선형 변환과 이동의 조합

## 6. 딥러닝의 기하학적 해석

- 고차원 공간에서 복잡하고 심하게 꼬여 있는 데이터의 매니폴드에 대한 깔끔한 표현을 찾는 것

# 4. 신경망의 엔진 : 그레이디언트 기반 최적화

- 가중치(Weight) : 훈련 데이터를 모델에 노출시켜 학습된 정보 포함, 훈련되는 파라미터
- 훈련 반복 루프(Training Loop)
    - 훈련 샘플과 이에 상응하는 타깃의 배치 추출
    - x를 사용하여 모델 실행, 예측 구하기
    - 예측의 차이를 측정하여 배치에 대한 모델 손실 계산
    - 모델의 모든 가중치 업데이트
- 경사 하강법(Gradient Descent) : 모델이 사용하는 모든 함수는 입력을 매끄럽고 연속적인 방식으로 변환
- 그레이디언트(Gradient) : 모델 가중치를 여러 방향으로 이동했을 때 손실이 얼마나 변하는지 설명 가능

## 1. 도함수(Derivative)

- 함수에서 특정 위치의 기울기
- 최적화(Optimization) 가능

## 2. 텐서 연산의 도함수 : 그레이디언트

- 텐서 연산의 도함수 → 그레이디언트
- 다차원 표면의 곡률

## 3. 확률적 경사 하강법

- 랜덤한 배치 데이터에서 현재 손실 값을 토대로 하여 조금씩 파라미터를 수정하는 것
- 미니 배치 확률적 경사 하강법(Mini-batch Stochastic Gradient Descent)
- 배치 경사 하강법 : 가용한 모든 데이터를 사용하여 반복 실행
- 옵티마이저 : 최적화 방법(Optimization Method)

## 4. 도함수 연결 : 역전파 알고리즘(Backpropagation Algorithm)

- 역전파 : 간단한 연산의 도함수를 사용해서 기초적인 연산을 조합한 복잡한 연산의 그레이디언트 쉽게 계산하는 것
- 미적분의 연쇄 법칙 → 연결된 함수의 도함수
- 계산 그래프(Computation Graph) : 텐서플로와 일반적인 딥러닝 혁신의 중심에 있는 데이터 구조
- 최종 손실 값에서 시작하여 아래층에서 맨 위층까지 거꾸로 거슬러 올라가 각 파라미터가 손실 값에 기여한 정도를 계산하는 것