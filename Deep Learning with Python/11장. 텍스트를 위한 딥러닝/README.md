# 1. 자연어 처리(NLP, Natural Language Processing)

- 자연어 : 기계를 위해 고안된 언어와 구별하기 위해 이름 붙인 것
- 복잡함, 모호함, 혼란함, 불규칙적, 끊임없이 변화함
- 결정 트리 기반 → 통계적 방법 → 학습된 파라미터를 가진 모델
- 입력으로 언어를 받아 유용한 어떤 것을 반환하는 것
    - 텍스트 분류
    - 콘텐츠 필터링
    - 감성 분석
    - 언어 모델링
    - 번역
    - 요약
- 초 순환 신경망 → 양방향 LSTM 모델 : 요약, 질문-대답, 기계 번역
- 트랜스포머 → RNN 대체

# 2. 텍스트 데이터 준비

- 텍스트 벡터화(Vectorization) : 텍스트를 수치 텐서로 바꾸는 과정

## 1. 텍스트 표준화(Standardization)

- 모델이 인코딩 차이를 고려하지 않도록 이를 제거하기 위한 기초적인 특성 공학의 한 형태
- 소문자로 바꾸고 구두점 문자 삭제
- 특수 문자를 표준 형태로 변경
- 어간 추출(Stemming) : 어형이 변형된 단어 → 공통된 하나의 표현

## 2. 텍스트 분할(토큰화, Tokenization)

- 표준화 이후 벡터화할 단위로 나누는 것
- 단어 수준 토큰화 : 토큰이 공백으로 구분된 부분 문자열
- N-그램 토큰화 : 토큰이 N개의 연속된 단어 그룹, 모델에 국부적인 단어 순서에 대한 소량의 정보 주입
- 문자 수준 토큰화 : 각 문자가 하나의 토큰, 텍스트 생성, 음성 인식에 사용
- 시퀀스 모델(Sequence Model) : 단어 순서 고려, 단어 수준 토큰화 사용
- BoW 모델(Bag-of-Words Model) : 입력 단어의 순서를 무시하고 집합으로 다룸, N-그램 토큰화 사용

## 3. 어휘 사전 인덱싱(Indexing)

- 각 토큰을 수치 표현으로 인코딩
- 훈련 데이터에 있는 모든 토큰 인덱스 → 각 항목에 고유한 정수 할당
- 원-핫 벡터 → 벡터 인코딩
- 드문 토큰 인덱싱 → 특성 공간 과도하게 증가
- 예외 어휘(Out of Vocabulary) 인덱스 : 어휘 사전에 없는 모든 토큰에 대응
- 마스킹 토큰 : 단어가 아니라 무시할 수 있는 토큰

## 4. TextVectorization 층 사용

- `tf.data` 파이프라인에 넣기
- 모델의 일부로 만들기

# 3. 단어 그룹을 표현하는 두 가지 방법 : 집합과 시퀀스

## 1. IMDB 영화 리뷰 데이터 준비

- `text_dataset_from_directory`
- 훈련, 검증, 테스트 데이터셋 분리

## 2. 단어를 집합으로 처리하기 : BoW 방식

- 텍스트 처리 → 순서를 무시하고 토큰의 집합으로 다룸
- 이진 인코딩을 사용한 유니그램
    - 전체 텍스트 → 하나의 벡터로 표현 가능
    - 하나의 텍스트 → 어휘 사전에 있는 단어 개수만큼의 차원을 가진 벡터로 인코딩
- 이진 인코딩을 사용한 바이그램
    - N-그램을 사용하여 국부적인 순서 정보 추가
    - 바이그램, 트라이그램 등 임의의 N-그램 반환 가능
- TF-IDF 인코딩을 사용한 바이그램
    - 단어의 히스토그램 사용
    - 정규화 사용 → 나눗셈만 사용
    - TF-IDF(단어 빈도-역문서 빈도, Term Frequency-Inverse Document Frequency)
    - 단어 빈도로 해당 단어에 가중치 부여 → 문서 빈도로 나눔

## 3. 단어를 시퀀스로 처리하기 : 시퀀스 모델 방식

- 시퀀스 모델(Sequence Model) : 순서 기반 특성  → 수동으로 만드는 대신 원시 단어 시퀀스를 모델에 전달하여 스스로 특성 학습
- 입력 샘플 → 정수 인덱스의 시퀀스로 표현 → 각 정수 벡터 매핑
- 단어 임베딩(Word Embedding) : 사람의 언어를 구조적인 기하학적 공간에 매핑
    - 저차원의 부동 소수점 벡터
    - 많은 정보 → 더 적은 차원으로 압축
    - 데이터로부터 학습 → 비슷한 단어 가까운 위치에 임베딩
- Embedding 층으로 단어 임베딩 학습
    - 새로운 작업 → 새로운 임베딩 학습
    - 정수 인덱스 → 밀집 벡터로 매핑하는 딕셔너리로 이해
    - 랭크-2 정수 텐서 → 랭크-3 부동 소수점 텐서
- 마스킹(Masking) : 샘플의 타임스텝을 건너뛸지 말지 결정
    - 케라스 → 마스킹 처리 가능한 모든 층에 자동 전달
    - RNN → 마스킹된 스텝 건너뜀
- 사전 훈련된 단어 임베딩 사용
    - 단어 출현 통계 사용
    - Word2Vec 알고리즘 : 차원 → 구체적인 의미가 있는 속성 잡아냄
    - GloVe : 단어 동시 출현 통계 → 행렬 분해 기법 사용

# 4. 트랜스포머(Transformer) 아키텍처

## 1. 셀프 어텐션 이해하기

- 모델 → 어떤 특성은 조금 더 주의를 기울임, 다른 특성은 조금 덜 주의를 기울일 필요 존재
- 일련의 특성에 대한 중요도 점수 계산
- 시퀀스에 있는 관련된 토큰의 표현을 사용하여 한 토큰의 표현을 조절하는 것
- 어텐션 점수 : 벡터와 문장에 있는 다른 모든 단어 사이의 관련성 점수 계산
- 쿼리-키-값 모델
    - 쿼리에 있는 모든 원소가 키에 있는 모든 원소에 얼마나 관련되어 있는지 계산 → 이 점수로 값에 있는 모든 원소의 가중치 합 계산
    - 검색 엔진, 추천 시스템
    - 키, 값 → 같은 시퀀스

## 2. 멀티 헤드 어텐션

- 멀티 헤드 : 셀프 어텐션의 출력 공간이 독립적으로 학습되는 부분 공간으로 나뉘어진다는 것
- 초기 쿼리, 키, 값 → 3개의 밀집 투영 → 3개의 별개 벡터

## 3. 트랜스포머 인코더

- 표준 아키텍처 패턴 → 모든 복잡한 모델에서 활용
    - 출력 → 여러 개의 독립적인 공간으로 분해
    - 잔차 연결, 정규화 층 추가
- 트랜스포머 인코더 : 소스 시퀀스 처리
- 트랜스포머 디코더 : 소스 시퀀스를 사용하여 변환된 버전 생성
- `LayerNormalization` : 각 시퀀스 안에서 데이터를 개별적으로 구함
- 기술적으로 순서에 구애받지 않지만 모델이 처리하는 표현에 순서 정보를 주입하는 하이브리드 방식
- 위치 인코딩(Positional Encoding) : 모델에 단어 순서 정보를 제공하기 위해 문장의 단어 위치를 각 단어 임베딩에 추가

## 4. BoW 모델 대신 언제 시퀀스 모델을 사용하는가

- 훈련 데이터에 잇는 샘플 개수와 샘플에 있는 평균 단어 개수 사이의 비율에 주의
- 시퀀스 모델 → 훈련 데이터가 많고 비교적 샘플의 길이가 짧은 경우에 잘 동작

# 5. 텍스트 분류를 넘어 : 시퀀스-투-시퀀스 학습

- 시퀀스-투-시퀀스 모델(Sequence-to-Sequence Model) : 입력으로 시퀀스 → 다른 시퀀스로 변경
    - 기계 번역(Machine Translation) : 소스 언어에 있는 문단 → 타깃 언어의 문단
    - 텍스트 요약(Text Summarization) : 긴 문서 → 중요한 정보를 유지한 짧은 버전
    - 질문 답변(Question Answering) : 입력 질문에 대한 답변 생성
    - 챗봇(Chatbot) : 입력된 대화나 대화 이력에서 다음 응답 생성
    - 텍스트 생성 : 시작 텍스트 → 하나의 문단 완성
- 시퀀스-투-시퀀스 모델 구조
    - 인코더 : 소스 시퀀스를 중간 표현으로 바꿈
    - 디코더 : 이전 토큰과 인코딩된 소스 시퀀스 → 타깃 시퀀스에 있는 다음 토큰 예측

## 1. RNN을 사용한 시퀀스-투-시퀀스 모델

- 각 타임스텝의 RNN 출력을 그대로 유지
- 타깃 시퀀스 → 항상 소스 시퀀스와 동일한 길이
- 모델이 타깃 시퀀스에 있는 토큰을 에측하기 위해 소스 시퀀스에 있는 토큰만 참조

## 2. 트랜스포머를 사용한 시퀀스-투-시퀀스 모델

- 시퀀스 형태로 인코딩된 표현 유지 → 문맥을 고려한 임베딩 벡터 시퀀스
- 인코딩된 소스 문장에서 어떤 토큰이 현재 예측하려는 타깃 토큰에 가장 관련이 높은지 식별
- 코잘 패딩(Causal Padding) : 순서에 구애받지 않고 한 번에 타깃 시퀀스 전체를 바라봄