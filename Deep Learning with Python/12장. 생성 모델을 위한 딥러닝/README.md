# 1. 텍스트 생성

## 1. 시퀀스 생성을 위한 딥러닝 모델의 간단한 역사

- 1997년 LSTM 알고리즘 개발
- 2002년 LSTM → 음악 생성에 적용
- 2010년대 초반 순환 네트워크 → 시퀀스 데이터 생성
- 2017, 2018년 트랜스포머 아키텍처 → 순환 신경망 압도

## 2. 시퀀스 데이터 생성

- 언어 모델(Language Model) : 이전 토큰들이 주어졌을 때 다음 토큰의 확률을 모렐링할 수 있는 네트워크
- 초기 텍스트 문자열 주입 → 새로운 글자, 단어 생성

## 3. 샘플링 전략

- 탐욕적 샘플링(Greedy Sampling) : 항상 가장 높은 확률을 가진 글자 선택
- 확률적 샘플링(Stochastic Sampling) : 다음 단어를 확률 분포에서 샘플링하는 과정에 무작위성 주입
- 소프트맥스 온도(Softmax Temperature) : 샘플링에 사용되는 확률 분포의 엔트로피를 나타냄
    - 높은 온도 : 엔트로피가 높은 샘플링 분포 → 놀랍고 생소한 데이터 생성
    - 낮은 온도 : 무작위성 낮음 → 예상할 수 있는 데이터 생성

## 4. 케라스를 사용한 텍스트 생성 모델 구현

- 시퀀스-투-시퀀스 모델 사용
- 트랜스포머 기반 모델

## 5. 가변 온도 샘플링을 사용한 텍스트 생성 콜백

- 콜백 → 에포크가 끝날 때마다 다양한 온도로 텍스트 생성
- 소프트맥스 온도 → 다양한 온도 실험을 통해 적절한 값 찾기

# 2. 딥드림(DeepDream)

- 합성곱 신경망이 학습한 표현을 사용하여 예술적으로 이미지를 조작하는 기법
- ConvNet 필터 시각화 기법과 거의 동일
- 전체 층의 활성화 최대화
- 이미 가지고 있는 이미지 사용
- 시각 품질을 높이기 위해 여러 다른 스케일로 처리

## 1. 케라스 딥드림 구현

- 딥드림 → 인셉션 모델
- 네트워크가 학습한 표현 기반 → ConvNet 거꾸로 실행
- 이미지 모델, ConvNet에 국한되지 않음

# 3. 뉴럴 스타일 트랜스퍼(Neural Style Transfer)

- 딥러닝을 사용하여 이미지 변경
- 타깃 이미지의 콘텐츠를 보존하면서 참조 이미지의 스타일을 타깃 이미지에 적용
- 스타일 : 질감, 색깔, 이미지에 있는 다양한 크기의 시각 요소
- 목표를 표현한 손실 함수 정의, 손실 최소화

## 1. 콘텐츠 소실

- ConvNet 층 활성화 → 이미지를 다른 크기의 콘텐츠로 분해

## 2. 스타일 손실

- 활성화 출력의 그람 행렬(Gram Matrix) 사용 → 층의 특성맵의 내적
- 내재된 상관관계를 비슷하게 보존하는 것

## 3. 케라스로 뉴럴 스타일 트랜스퍼 구현하기

- 스타일 참조 이미지, 베이스 이미지, 생성된 이미지를 위해 층 활성화를 동시에 계산하는 네트워크 설정
- 층 활성화를 사용하여 손실 함수 정의
- 손실 함수 → 최소화할 경사 하강법 과정 설정
- 이미지의 텍스처 변경, 텍스처 전이

# 4. 변이형 오토인코더를 사용한 이미지 생성

## 1. 이미지의 잠재 공간에서 샘플링

- 각 포인트가 실제와 같은 이미지로 매핑될 수 있는 저차원 잠재 공간의 표현 생성
- 생성자, 디코더

## 2. 이미지 변형을 위한 개념 벡터

- 잠재 공간, 임베딩 공간 → 원본 데이터의 흥미로운 변화 인코딩

## 3. 변이형 오토인코더(VAE, Variational AutoEncoders)

- 생성 모델의 한 종류, 개념 벡터 → 이미지 변형
- 딥러닝 + 베이즈 추론 혼합 오토인코더
- 약간의 통계 기법 추가 → 연속적, 구조적인 잠재 공간 학습
- 재구성 손실(Reconstruction Loss) : 디코딩된 샘플이 원본 입력과 동일하도록 만듦
- 규제 손실(Regularization Loss) : 잠재 공간을 잘 형성하고 훈련 데이터에 과대적합을 줄임

## 4. 케라스로 VAE 구현

- 인코더 네트워크 → 실제 이미지를 잠재 공간의 평균과 분산으로 변환
- 샘플링 층 → 평균과 분산을 받아 잠재 공간에서 랜덤한 포인트 샘플링
- 디코더 네트워크 → 잠재 공간의 포인트 이미지로 변환
- 자기지도 학습(Self Supervised Learning) : 입력을 타깃으로 사용

# 5. 생성적 적대 신경망(GAN, Generative Adversarial Networks)

- 생성된 이미지가 실제 이미지와 통계적으로 거의 구분되지 않도록 강제하여 실제같은 합성 이미지 생성
- 생성자 네트워크(Generator Network) : 랜덤 벡터를 입력으로 받아 합성된 이미지로 디코딩
- 판별자 네트워크(Discriminator Network) : 이미지를 입력으로 받아 훈련 세트에서 온 이미지인지, 생성자 네트워크가 만든 이미지인지 판별

## 1. GAN 구현 방법

- 네트워크 → 벡터 일정 크기의 이미지로 매핑
- 일정 크기의 이미지가 진짜일 확률을 추정하여 이진 값으로 매핑
- 생성자와 판별자를 연결하는 GAN 네트워크 생성
- 진짜 이미지, 가짜 이미지 샘플로 판별자 훈련
- GAN 모델의 손실에 대한 생성자 가중치의 그레이디언트 사용

## 2. 훈련 방법

- 판별자에서 특성 맵을 다운샘플링하는 데 스트라이드 사용
- 정규 분포 → 잠재 공간에서 포인트 샘플링
- 무작위성 → 모델 견고하게 함
- 희소한 그레이디언트 → GAN 훈련 방해

## 3. 판별자

- 생성자가 노이즈 같은 이미지를 생성하는 데에서 멈춤
- 드롭아웃 사용

## 4. 생성자

- 벡터 → 후보 이미지로 변환

## 5. 적대 네트워크

- 잠재 공간에서 무작위로 포인트를 뽑음
- 랜덤 노이즈 → 이미지 생성
- 생성된 이미지 + 진짜 이미지
- 진짜와 가짜가 섞인 이미지와 대응하는 타깃으로 훈련
- 잠재 공간에서 무작위로 새로운 포인트를 뽑음
- 랜덤 벡터 → 훈련