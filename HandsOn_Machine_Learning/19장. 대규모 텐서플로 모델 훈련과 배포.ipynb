{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. 텐서플로 모델 서빙\n",
    "\n",
    "- SavedModel로 내보내기 → 하나 이상의 메타그래프 포함\n",
    "- REST API로 TF 서빙에 쿼리하기\n",
    "- gRPC API로 TF 서빙에 쿼리하기\n",
    "- 새로운 버전의 모델 배포하기\n",
    "- 버텍스 AI에서 예측 서비스 만들기\n",
    "- 버텍스 AI에서 배치 예측 작업 실행하기\n",
    "\n",
    "# 2. 모바일 또는 임베디드 디바이스에 모델 배포하기\n",
    "\n",
    "- 엣지 컴퓨팅(Edge Computing) : 데이터 소스에 더 가까운 곳에서도 실행\n",
    "- 다운로드 시간과 RAM 사용량을 줄이기 위해 모델 크기 감소\n",
    "- 응답 속도, 배터리 사용량, 발열 감소를 위해 예측에 필요한 계산량 감소\n",
    "- 모델을 특정 디바이스의 제약 조건에 맞춤\n",
    "- 양자화를 고려한 훈련(Quantization-Aware Training)\n",
    "\n",
    "# 3. 웹 페이지에서 모델 실행하기\n",
    "\n",
    "- 프로그레시브 웹 앱(PWA, Progressive Web App) : 여러 기준을 준수하는 웹 사이트\n",
    "- 서비스 워커(Service Worker) : PWA 오프라인 실행\n",
    "- 연합 학습(Federated Learning)\n",
    "\n",
    "# 4. 계산 속도를 높이기 위해 GPU 사용하기\n",
    "\n",
    "- GPU 구매하기\n",
    "- GPU RAM 관리하기\n",
    "- 디바이스에 연산과 변수 할당하기\n",
    "- 다중 장치에서 병렬 실행하기\n",
    "\n",
    "# 5. 다중 장치에서 모델 훈련하기\n",
    "\n",
    "- 모델 병렬화(Model Parallelism) : 모델 여러 장치에 분할\n",
    "- 데이터 병렬화(Data Parallelism) : 모델을 각 장치에 복사 → 복사본을 각기 다른 데이터의 일부분에서 훈련\n",
    "    - 미러드 전략 : 복제된 모든 모델은 항상 완벽하게 동일한 상태 유지\n",
    "    - 중앙 집중적인 파라미터 사용\n",
    "    - 대역폭 포화 : 파이프라인 병렬화\n",
    "- 분산 전략 API를 사용한 대규모 훈련\n",
    "- 텐서플로 클러스터에서 모델 훈련하기\n",
    "- 버텍스 AI에서 대규모 훈련 작업 실행하기\n",
    "- 버텍스 AI의 하이퍼파라미터 튜닝"
   ],
   "id": "dec72a9f98be4909"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-28T11:46:54.448230Z",
     "start_time": "2024-06-28T11:46:39.561234Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "\n",
    "model_name = \"my_mnist_model\"\n",
    "model_version = \"0001\"\n",
    "model_path = Path(model_name) / model_version\n",
    "model.save(model_path, save_format=\"tf\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001B[1m11490434/11490434\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 0us/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 583us/step - accuracy: 0.7016 - loss: 1.0881 - val_accuracy: 0.9066 - val_loss: 0.3607\n",
      "Epoch 2/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 522us/step - accuracy: 0.8979 - loss: 0.3657 - val_accuracy: 0.9206 - val_loss: 0.2905\n",
      "Epoch 3/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 509us/step - accuracy: 0.9143 - loss: 0.3039 - val_accuracy: 0.9288 - val_loss: 0.2570\n",
      "Epoch 4/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 518us/step - accuracy: 0.9244 - loss: 0.2694 - val_accuracy: 0.9334 - val_loss: 0.2342\n",
      "Epoch 5/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 496us/step - accuracy: 0.9311 - loss: 0.2448 - val_accuracy: 0.9380 - val_loss: 0.2170\n",
      "Epoch 6/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 485us/step - accuracy: 0.9361 - loss: 0.2257 - val_accuracy: 0.9428 - val_loss: 0.2033\n",
      "Epoch 7/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 495us/step - accuracy: 0.9403 - loss: 0.2102 - val_accuracy: 0.9468 - val_loss: 0.1918\n",
      "Epoch 8/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 499us/step - accuracy: 0.9437 - loss: 0.1970 - val_accuracy: 0.9502 - val_loss: 0.1819\n",
      "Epoch 9/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 512us/step - accuracy: 0.9467 - loss: 0.1856 - val_accuracy: 0.9550 - val_loss: 0.1732\n",
      "Epoch 10/10\n",
      "\u001B[1m1719/1719\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 542us/step - accuracy: 0.9494 - loss: 0.1754 - val_accuracy: 0.9562 - val_loss: 0.1657\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m model_version \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0001\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     24\u001B[0m model_path \u001B[38;5;241m=\u001B[39m Path(model_name) \u001B[38;5;241m/\u001B[39m model_version\n\u001B[0;32m---> 25\u001B[0m model\u001B[38;5;241m.\u001B[39msave(model_path, save_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_api.py:66\u001B[0m, in \u001B[0;36msave_model\u001B[0;34m(model, filepath, overwrite, **kwargs)\u001B[0m\n\u001B[1;32m     59\u001B[0m         logging\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[1;32m     60\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `save_format` argument is deprecated in Keras 3. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     61\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe recommend removing this argument as it can be inferred \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     62\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom the file path. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     63\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived: save_format=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_format\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     64\u001B[0m         )\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 66\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     67\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `save_format` argument is deprecated in Keras 3. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     68\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease remove this argument and pass a file path with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     69\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meither `.keras` or `.h5` extension.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     70\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived: save_format=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msave_format\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     71\u001B[0m         )\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs:\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     74\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe following argument(s) are not supported: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(kwargs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:47:14.947338Z",
     "start_time": "2024-06-28T11:47:14.942774Z"
    }
   },
   "cell_type": "code",
   "source": "sorted([str(path) for path in model_path.parent.glob(\"**/*\")])",
   "id": "818c753f02e82962",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:47:29.186626Z",
     "start_time": "2024-06-28T11:47:29.184191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())"
   ],
   "id": "a490e83bbfc8ff91",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:47:37.837766Z",
     "start_time": "2024-06-28T11:47:37.835323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "X_new = X_test[:3]\n",
    "request_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})"
   ],
   "id": "2f47a3e37bab66c7",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:47:41.198356Z",
     "start_time": "2024-06-28T11:47:41.195031Z"
    }
   },
   "cell_type": "code",
   "source": "request_json[:100] + \"...\" + request_json[-10:]",
   "id": "45ecf9e79bd75b6f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"signature_name\": \"serving_default\", \"instances\": [[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0..., 0, 0]]]}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:47:49.254515Z",
     "start_time": "2024-06-28T11:47:48.693136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "response = requests.post(server_url, data=request_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ],
   "id": "ecf438e41c9ce41e",
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/my_mnist_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x173b493d0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:203\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m     sock \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mcreate_connection(\n\u001B[1;32m    204\u001B[0m         (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dns_host, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport),\n\u001B[1;32m    205\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout,\n\u001B[1;32m    206\u001B[0m         source_address\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_address,\n\u001B[1;32m    207\u001B[0m         socket_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket_options,\n\u001B[1;32m    208\u001B[0m     )\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 85\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m sock\u001B[38;5;241m.\u001B[39mconnect(sa)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "\u001B[0;31mConnectionRefusedError\u001B[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:791\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    790\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 791\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[1;32m    792\u001B[0m     conn,\n\u001B[1;32m    793\u001B[0m     method,\n\u001B[1;32m    794\u001B[0m     url,\n\u001B[1;32m    795\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[1;32m    796\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    797\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    798\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    799\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[1;32m    800\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[1;32m    801\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[1;32m    802\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[1;32m    803\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[1;32m    804\u001B[0m )\n\u001B[1;32m    806\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:497\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 497\u001B[0m     conn\u001B[38;5;241m.\u001B[39mrequest(\n\u001B[1;32m    498\u001B[0m         method,\n\u001B[1;32m    499\u001B[0m         url,\n\u001B[1;32m    500\u001B[0m         body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    501\u001B[0m         headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    502\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    503\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[1;32m    504\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[1;32m    505\u001B[0m         enforce_content_length\u001B[38;5;241m=\u001B[39menforce_content_length,\n\u001B[1;32m    506\u001B[0m     )\n\u001B[1;32m    508\u001B[0m \u001B[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001B[39;00m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001B[39;00m\n\u001B[1;32m    510\u001B[0m \u001B[38;5;66;03m# With this behaviour, the received response is still readable.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:395\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mputheader(header, value)\n\u001B[0;32m--> 395\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mendheaders()\n\u001B[1;32m    397\u001B[0m \u001B[38;5;66;03m# If we're given a body we start sending that in chunks.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/http/client.py:1293\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1292\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1293\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_send_output(message_body, encode_chunked\u001B[38;5;241m=\u001B[39mencode_chunked)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/http/client.py:1052\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1052\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(msg)\n\u001B[1;32m   1054\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1055\u001B[0m \n\u001B[1;32m   1056\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/http/client.py:990\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m--> 990\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[1;32m    991\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:243\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 243\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_conn()\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n\u001B[1;32m    245\u001B[0m         \u001B[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:218\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 218\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NewConnectionError(\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to establish a new connection: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    220\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001B[39;00m\n",
      "\u001B[0;31mNewConnectionError\u001B[0m: <urllib3.connection.HTTPConnection object at 0x173b493d0>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[1;32m    487\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    488\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m    489\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[1;32m    490\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m    491\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    492\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    493\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    494\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    495\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[1;32m    496\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    497\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    498\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:845\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    843\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_e)\n\u001B[0;32m--> 845\u001B[0m retries \u001B[38;5;241m=\u001B[39m retries\u001B[38;5;241m.\u001B[39mincrement(\n\u001B[1;32m    846\u001B[0m     method, url, error\u001B[38;5;241m=\u001B[39mnew_e, _pool\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, _stacktrace\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mexc_info()[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    847\u001B[0m )\n\u001B[1;32m    848\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    514\u001B[0m     reason \u001B[38;5;241m=\u001B[39m error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause)\n\u001B[0;32m--> 515\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, reason) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreason\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    517\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/my_mnist_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x173b493d0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrequests\u001B[39;00m\n\u001B[1;32m      3\u001B[0m server_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp://localhost:8501/v1/models/my_mnist_model:predict\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m response \u001B[38;5;241m=\u001B[39m requests\u001B[38;5;241m.\u001B[39mpost(server_url, data\u001B[38;5;241m=\u001B[39mrequest_json)\n\u001B[1;32m      5\u001B[0m response\u001B[38;5;241m.\u001B[39mraise_for_status()\n\u001B[1;32m      6\u001B[0m response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/api.py:115\u001B[0m, in \u001B[0;36mpost\u001B[0;34m(url, data, json, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    104\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request.\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, data\u001B[38;5;241m=\u001B[39mdata, json\u001B[38;5;241m=\u001B[39mjson, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/adapters.py:519\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, _SSLError):\n\u001B[1;32m    516\u001B[0m         \u001B[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001B[39;00m\n\u001B[1;32m    517\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[0;32m--> 519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    521\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ClosedPoolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    522\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "\u001B[0;31mConnectionError\u001B[0m: HTTPConnectionPool(host='localhost', port=8501): Max retries exceeded with url: /v1/models/my_mnist_model:predict (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x173b493d0>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:47:56.461811Z",
     "start_time": "2024-06-28T11:47:56.447860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ],
   "id": "20f58559b0020810",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m y_proba \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(response[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      4\u001B[0m y_proba\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'response' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:50:24.822896Z",
     "start_time": "2024-06-28T11:50:24.804558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
    "\n",
    "request = PredictRequest()\n",
    "request.model_spec.name = model_name\n",
    "request.model_spec.signature_name = \"serving_default\"\n",
    "input_name = model.input_names[0]\n",
    "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))"
   ],
   "id": "75e188a91a5de6e4",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'input_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m request\u001B[38;5;241m.\u001B[39mmodel_spec\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m model_name\n\u001B[1;32m      5\u001B[0m request\u001B[38;5;241m.\u001B[39mmodel_spec\u001B[38;5;241m.\u001B[39msignature_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mserving_default\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 6\u001B[0m input_name \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39minput_names[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m      7\u001B[0m request\u001B[38;5;241m.\u001B[39minputs[input_name]\u001B[38;5;241m.\u001B[39mCopyFrom(tf\u001B[38;5;241m.\u001B[39mmake_tensor_proto(X_new))\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Sequential' object has no attribute 'input_names'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:50:37.327662Z",
     "start_time": "2024-06-28T11:50:36.942636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import grpc\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "response = predict_service.Predict(request, timeout=10.0)"
   ],
   "id": "ab3a6d67538154cf",
   "outputs": [
    {
     "ename": "_InactiveRpcError",
     "evalue": "<_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:Failed to pick subchannel {created_time:\"2024-06-28T20:50:37.203852+09:00\", children:[UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2024-06-28T20:50:37.20385+09:00\"}]}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31m_InactiveRpcError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m channel \u001B[38;5;241m=\u001B[39m grpc\u001B[38;5;241m.\u001B[39minsecure_channel(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocalhost:8500\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m predict_service \u001B[38;5;241m=\u001B[39m prediction_service_pb2_grpc\u001B[38;5;241m.\u001B[39mPredictionServiceStub(channel)\n\u001B[0;32m----> 6\u001B[0m response \u001B[38;5;241m=\u001B[39m predict_service\u001B[38;5;241m.\u001B[39mPredict(request, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10.0\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/grpc/_channel.py:946\u001B[0m, in \u001B[0;36m_UnaryUnaryMultiCallable.__call__\u001B[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001B[0m\n\u001B[1;32m    937\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    938\u001B[0m              request,\n\u001B[1;32m    939\u001B[0m              timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    942\u001B[0m              wait_for_ready\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    943\u001B[0m              compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    944\u001B[0m     state, call, \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blocking(request, timeout, metadata, credentials,\n\u001B[1;32m    945\u001B[0m                                   wait_for_ready, compression)\n\u001B[0;32m--> 946\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _end_unary_response_blocking(state, call, \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/grpc/_channel.py:849\u001B[0m, in \u001B[0;36m_end_unary_response_blocking\u001B[0;34m(state, call, with_call, deadline)\u001B[0m\n\u001B[1;32m    847\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m state\u001B[38;5;241m.\u001B[39mresponse\n\u001B[1;32m    848\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 849\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _InactiveRpcError(state)\n",
      "\u001B[0;31m_InactiveRpcError\u001B[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused\"\n\tdebug_error_string = \"UNKNOWN:Failed to pick subchannel {created_time:\"2024-06-28T20:50:37.203852+09:00\", children:[UNKNOWN:failed to connect to all addresses; last error: UNKNOWN: Failed to connect to remote host: Connection refused {grpc_status:14, created_time:\"2024-06-28T20:50:37.20385+09:00\"}]}\"\n>"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "y_proba = tf.make_ndarray(outputs_proto)"
   ],
   "id": "9905366de7e4e7cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:53:41.778716Z",
     "start_time": "2024-06-28T11:53:41.765442Z"
    }
   },
   "cell_type": "code",
   "source": "y_proba.round(2)",
   "id": "5c325353c54c54af",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_proba' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m y_proba\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'y_proba' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output_name = model.output_names[0]\n",
    "outputs_proto = response.outputs[output_name]\n",
    "shape = [dim.size for dim in outputs_proto.tensor_shape.dim]\n",
    "y_proba = np.array(outputs_proto.float_val).reshape(shape)\n",
    "y_proba.round(2)"
   ],
   "id": "7f60ff305688656e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8),\n",
    "    tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ],
   "id": "6f0c75988359cd65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model_version = \"0002\"\n",
    "model_path = Path(model_name) / model_version\n",
    "model.save(model_path, save_format=\"tf\")"
   ],
   "id": "61a61b31fe8432b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sorted([str(path) for path in model_path.parent.glob(\"**/*\")])",
   "id": "f8b8486e68a78119"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "\n",
    "response = requests.post(server_url, data=request_json)\n",
    "response.raise_for_status()\n",
    "response = response.json()"
   ],
   "id": "f39b09c88021897"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "response.keys()",
   "id": "118814b06aa819ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ],
   "id": "52a4a5e127c21eaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def upload_directory(bucket, dirpath):\n",
    "    dirpath = Path(dirpath)\n",
    "    for filepath in dirpath.glob(\"**/*\"):\n",
    "        if filepath.is_file():\n",
    "            blob = bucket.blob(filepath.relative_to(dirpath.parent).as_posix())\n",
    "            blob.upload_from_filename(filepath)\n",
    "\n",
    "\n",
    "upload_directory(bucket, \"my_mnist_model\")"
   ],
   "id": "6b9a071b4fcd2137"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from concurrent import futures\n",
    "\n",
    "\n",
    "def upload_file(bucket, filepath, blob_path):\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.upload_from_filename(filepath)\n",
    "\n",
    "\n",
    "def upload_directory(bucket, dirpath, prefix=None, max_workers=50):\n",
    "    dirpath = Path(dirpath)\n",
    "    prefix = prefix or dirpath.name\n",
    "    with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_filepath = {\n",
    "            executor.submit(\n",
    "                upload_file,\n",
    "                bucket, filepath,\n",
    "                f\"{prefix}/{filepath.relative_to(dirpath).as_posix()}\"\n",
    "            ): filepath\n",
    "            for filepath in sorted(dirpath.glob(\"**/*\"))\n",
    "            if filepath.is_file()\n",
    "        }\n",
    "        for future in futures.as_completed(future_to_filepath):\n",
    "            filepath = future_to_filepath[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "            except Exception as ex:\n",
    "                print(f\"{filepath!s:60} 업로드 에러: {ex}\")\n",
    "            else:\n",
    "                print(f\"{filepath!s:60} 업로드 완료\", end=\"\\r\")\n",
    "\n",
    "    print(f\"{dirpath!s:60} 업로드 완료\")"
   ],
   "id": "22a3f73d79e5a100"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=\"mnist-endpoint\")\n",
    "\n",
    "endpoint.deploy(\n",
    "    mnist_model,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=5,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=1\n",
    ")"
   ],
   "id": "4117c9e7b44d4fde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "response = endpoint.predict(instances=X_new.tolist())",
   "id": "d98559a811429298"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "np.round(response.predictions, 2)"
   ],
   "id": "34f65432c3e2bb23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "endpoint.undeploy_all()\n",
    "endpoint.delete()"
   ],
   "id": "be206f49e5633e89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_path = Path(\"my_mnist_batch\")\n",
    "batch_path.mkdir(exist_ok=True)\n",
    "with open(batch_path / \"my_mnist_batch.jsonl\", \"w\") as jsonl_file:\n",
    "    for image in X_test[:100].tolist():\n",
    "        jsonl_file.write(json.dumps(image))\n",
    "        jsonl_file.write(\"\\n\")\n",
    "\n",
    "upload_directory(bucket, batch_path)"
   ],
   "id": "e29fdf5a4027208a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_prediction_job = mnist_model.batch_predict(\n",
    "    job_display_name=\"my_batch_prediction_job\",\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    starting_replica_count=1,\n",
    "    max_replica_count=5,\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=1,\n",
    "    gcs_source=[f\"gs://{bucket_name}/{batch_path.name}/my_mnist_batch.jsonl\"],\n",
    "    gcs_destination_prefix=f\"gs://{bucket_name}/my_mnist_predictions/\",\n",
    "    sync=True\n",
    ")"
   ],
   "id": "f515f757df2da63e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "batch_prediction_job.output_info",
   "id": "c4dc676a5c818f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_probas = []\n",
    "for blob in batch_prediction_job.iter_outputs():\n",
    "    print(blob.name)\n",
    "    if \"prediction.results\" in blob.name:\n",
    "        for line in blob.download_as_text().splitlines():\n",
    "            y_proba = json.loads(line)[\"prediction\"]\n",
    "            y_probas.append(y_proba)"
   ],
   "id": "621964d7db321f03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_pred = np.argmax(y_probas, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test[:100]) / 100"
   ],
   "id": "c77fbc2f5a047b4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "accuracy",
   "id": "2542012196385e7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "mnist_model.delete()",
   "id": "67231ff2e6dbb28c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for prefix in [\"my_mnist_model/\", \"my_mnist_batch/\", \"my_mnist_predictions/\"]:\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    for blob in blobs:\n",
    "        blob.delete()\n",
    "\n",
    "batch_prediction_job.delete()"
   ],
   "id": "59e4911f4d51be30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(str(model_path))\n",
    "tflite_model = converter.convert()\n",
    "with open(\"my_converted_savedmodel.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ],
   "id": "9037f338fa23d66c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "converter = tf.lite.TFLiteConverter.from_keras_model(model)",
   "id": "cac83fe765f0fe15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "converter.optimizations = [tf.lite.Optimize.DEFAULT]",
   "id": "bfab127dac536e20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tflite_model = converter.convert()\n",
    "with open(\"my_converted_keras_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ],
   "id": "4cf028fea6037c43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "physical_gpus"
   ],
   "id": "6b4befd236a53bdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logical_gpus = tf.config.list_logical_devices(\"GPU\")\n",
    "logical_gpus"
   ],
   "id": "3a028e308b663498"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "a = tf.Variable([1., 2., 3.])\n",
    "a.device"
   ],
   "id": "dc3da832e0b3a987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "b = tf.Variable([1, 2, 3])\n",
    "b.device"
   ],
   "id": "5a5689749282d260"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    c = tf.Variable([1., 2., 3.])\n",
    "\n",
    "c.device"
   ],
   "id": "2ab63a9b6001aa61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with tf.device(\"/gpu:1234\"):\n",
    "    d = tf.Variable([1., 2., 3.])\n",
    "\n",
    "d.device"
   ],
   "id": "1b487fdb8f577033"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tf.config.set_soft_device_placement(False)\n",
    "\n",
    "try:\n",
    "    with tf.device(\"/gpu:1000\"):\n",
    "        d = tf.Variable([1., 2., 3.])\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)\n",
    "\n",
    "tf.config.set_soft_device_placement(True)"
   ],
   "id": "5e9d31c82aadba0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape([28, 28, 1], input_shape=[28, 28], dtype=tf.uint8),\n",
    "        tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ])"
   ],
   "id": "5fd251fab3cb4fb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "batch_size = 100\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), batch_size=batch_size)"
   ],
   "id": "f610eb5d0a57ca9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "type(model.weights[0])",
   "id": "7d499513b0d02922"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.predict(X_new).round(2)",
   "id": "11f73e13a8478c9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.save(\"my_mirrored_model\", save_format=\"tf\")\n",
    "model = tf.keras.models.load_model(\"my_mirrored_model\")\n",
    "type(model.weights[0])"
   ],
   "id": "8eacdab02d52a276"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.models.load_model(\"my_mirrored_model\")"
   ],
   "id": "b7243a37553bae7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "type(model.weights[0])",
   "id": "bd04456a1bdaacb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])",
   "id": "3cd41e087b8468f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "strategy = tf.distribute.MirroredStrategy(\n",
    "    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
   ],
   "id": "22c8b3c33d954720"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "strategy = tf.distribute.experimental.CentralStorageStrategy()",
   "id": "6fe9a3629670665d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cluster_spec = {\n",
    "    \"worker\": [\n",
    "        \"machine-a.example.com:2222\",\n",
    "        \"machine-b.example.com:2222\"\n",
    "    ],\n",
    "    \"ps\": [\"machine-a.example.com:2221\"]\n",
    "}"
   ],
   "id": "c80ae3b52c80fc66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    \"cluster\": cluster_spec,\n",
    "    \"task\": {\"type\": \"worker\", \"index\": 0}\n",
    "})"
   ],
   "id": "ee406361f11b06df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "resolver.cluster_spec()"
   ],
   "id": "d77f3ad45154aff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resolver.task_type",
   "id": "472cedc0092aa98e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "resolver.task_id",
   "id": "1fa3ef15686c76b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:05:15.420091Z",
     "start_time": "2024-06-28T12:05:10.759967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "print(f\"Starting task {resolver.task_type} #{resolver.task_id}\")\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape([28, 28, 1], input_shape=[28, 28], dtype=tf.uint8),\n",
    "        tf.keras.layers.Rescaling(scale=1 / 255),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10)\n",
    "\n",
    "if resolver.task_id == 0:\n",
    "    model.save(\"my_mnist_multiworker_model\", save_format=\"tf\")\n",
    "else:\n",
    "    tmpdir = tempfile.mkdtemp()\n",
    "    model.save(tmpdir, save_format=\"tf\")\n",
    "    tf.io.gfile.rmtree(tmpdir)"
   ],
   "id": "9648ae03b05c03d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n",
      "Starting task None #None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/reshaping/reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n",
      "2024-06-28 21:05:11.070923: W tensorflow/core/framework/dataset.cc:959] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m  65/1719\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m1:22\u001B[0m 50ms/step - accuracy: 0.1541 - loss: 2.2917"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 31\u001B[0m\n\u001B[1;32m     14\u001B[0m     model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mSequential([\n\u001B[1;32m     15\u001B[0m         tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mReshape([\u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m1\u001B[39m], input_shape\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m28\u001B[39m], dtype\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39muint8),\n\u001B[1;32m     16\u001B[0m         tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mRescaling(scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m255\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     25\u001B[0m         tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(units\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     26\u001B[0m     ])\n\u001B[1;32m     27\u001B[0m     model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse_categorical_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     28\u001B[0m                   optimizer\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mSGD(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-2\u001B[39m),\n\u001B[1;32m     29\u001B[0m                   metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 31\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train, validation_data\u001B[38;5;241m=\u001B[39m(X_valid, y_valid), epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resolver\u001B[38;5;241m.\u001B[39mtask_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     34\u001B[0m     model\u001B[38;5;241m.\u001B[39msave(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmy_mnist_multiworker_model\u001B[39m\u001B[38;5;124m\"\u001B[39m, save_format\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:325\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39menumerate_epoch():\n\u001B[1;32m    324\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m--> 325\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(iterator)\n\u001B[1;32m    326\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(\n\u001B[1;32m    327\u001B[0m         step, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pythonify_logs(logs)\n\u001B[1;32m    328\u001B[0m     )\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_training:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    830\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 833\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    835\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    836\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    876\u001B[0m \u001B[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001B[39;00m\n\u001B[1;32m    877\u001B[0m \u001B[38;5;66;03m# run the first trace but we should fail if variables are created.\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m results \u001B[38;5;241m=\u001B[39m tracing_compilation\u001B[38;5;241m.\u001B[39mcall_function(\n\u001B[1;32m    879\u001B[0m     args, kwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config\n\u001B[1;32m    880\u001B[0m )\n\u001B[1;32m    881\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[1;32m    882\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating variables on a non-first call to a function\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    883\u001B[0m                    \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m decorated with tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m function\u001B[38;5;241m.\u001B[39m_call_flat(  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    140\u001B[0m     flat_inputs, captured_inputs\u001B[38;5;241m=\u001B[39mfunction\u001B[38;5;241m.\u001B[39mcaptured_inputs\n\u001B[1;32m    141\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1318\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1319\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1320\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1321\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1322\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inference_function\u001B[38;5;241m.\u001B[39mcall_preflattened(args)\n\u001B[1;32m   1323\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1324\u001B[0m     args,\n\u001B[1;32m   1325\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1326\u001B[0m     executing_eagerly)\n\u001B[1;32m   1327\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001B[0m, in \u001B[0;36mAtomicFunction.call_preflattened\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_preflattened\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 216\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcall_flat(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    217\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001B[0m, in \u001B[0;36mAtomicFunction.call_flat\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    250\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mcall_function(\n\u001B[1;32m    252\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname,\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    254\u001B[0m         \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mflat_outputs),\n\u001B[1;32m    255\u001B[0m     )\n\u001B[1;32m    256\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    257\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    258\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    261\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1498\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1500\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute(\n\u001B[1;32m   1501\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1502\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[1;32m   1503\u001B[0m       inputs\u001B[38;5;241m=\u001B[39mtensor_inputs,\n\u001B[1;32m   1504\u001B[0m       attrs\u001B[38;5;241m=\u001B[39mattrs,\n\u001B[1;32m   1505\u001B[0m       ctx\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1506\u001B[0m   )\n\u001B[1;32m   1507\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1508\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1509\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1510\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1514\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1515\u001B[0m   )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     52\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 53\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m pywrap_tfe\u001B[38;5;241m.\u001B[39mTFE_Py_Execute(ctx\u001B[38;5;241m.\u001B[39m_handle, device_name, op_name,\n\u001B[1;32m     54\u001B[0m                                       inputs, attrs, num_outputs)\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     56\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()\n",
    "\n",
    "if resolver.task_type == \"chief\":\n",
    "    model_dir = os.getenv(\"AIP_MODEL_DIR\")\n",
    "    tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "    checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\n",
    "else:\n",
    "    tmp_dir = Path(tempfile.mkdtemp())\n",
    "    model_dir = tmp_dir / \"model\"\n",
    "    tensorboard_log_dir = tmp_dir / \"logs\"\n",
    "    checkpoint_dir = tmp_dir / \"ckpt\"\n",
    "\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(tensorboard_log_dir), tf.keras.callbacks.ModelCheckpoint(checkpoint_dir)]\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape([28, 28, 1], input_shape=[28, 28], dtype=tf.uint8),\n",
    "        tf.keras.layers.Lambda(lambda X: X / 255),\n",
    "        tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n",
    "        tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(units=10, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-2),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, callbacks=callbacks)\n",
    "model.save(model_dir, save_format=\"tf\")"
   ],
   "id": "79c068007adbe05f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "custom_training_job = aiplatform.CustomTrainingJob(\n",
    "    display_name=\"my_custom_training_job\",\n",
    "    script_path=\"my_vertex_ai_training_task.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
    "    model_serving_container_image_uri=server_image,\n",
    "    requirements=[\"gcsfs==2022.3.0\"],\n",
    "    staging_bucket=f\"gs://{bucket_name}/staging\"\n",
    ")"
   ],
   "id": "8b4bc16aa21335ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mnist_model2 = custom_training_job.run(\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    replica_count=2,\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=2,\n",
    ")"
   ],
   "id": "39f6c92fc6404398"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mnist_model2.delete()\n",
    "custom_training_job.delete()\n",
    "blobs = bucket.list_blobs(prefix=f\"gs://{bucket_name}/staging/\")\n",
    "for blob in blobs:\n",
    "    blob.delete()"
   ],
   "id": "de3d910f824ecf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_hidden\", type=int, default=2)\n",
    "parser.add_argument(\"--n_neurons\", type=int, default=256)\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-2)\n",
    "parser.add_argument(\"--optimizer\", default=\"adam\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def build_model(args):\n",
    "    with tf.distribute.MirroredStrategy().scope():\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Flatten(input_shape=[28, 28], dtype=tf.uint8))\n",
    "        for _ in range(args.n_hidden):\n",
    "            model.add(tf.keras.layers.Dense(args.n_neurons, activation=\"relu\"))\n",
    "        model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "        opt = tf.keras.optimizers.get(args.optimizer)\n",
    "        opt.learning_rate = args.learning_rate\n",
    "        model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "import os\n",
    "\n",
    "model_dir = os.getenv(\"AIP_MODEL_DIR\")\n",
    "tensorboard_log_dir = os.getenv(\"AIP_TENSORBOARD_LOG_DIR\")\n",
    "checkpoint_dir = os.getenv(\"AIP_CHECKPOINT_DIR\")\n",
    "trial_id = os.getenv(\"CLOUD_ML_TRIAL_ID\")\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(tensorboard_log_dir)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5)\n",
    "callbacks = [tensorboard_cb, early_stopping_cb]\n",
    "\n",
    "model = build_model(args)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, callbacks=callbacks)\n",
    "model.save(model_dir, save_format=\"tf\")\n",
    "\n",
    "import hypertune\n",
    "\n",
    "hypertune = hypertune.HyperTune()\n",
    "hypertune.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag=\"accuracy\",\n",
    "    metric_value=max(history.history[\"val_accuracy\"]),\n",
    "    global_step=model.optimizer.iterations.numpy(),\n",
    ")"
   ],
   "id": "c8863a8d4bfb3ff0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trial_job = aiplatform.CustomJob.from_local_script(\n",
    "    display_name=\"my_search_trial_job\",\n",
    "    script_path=\"my_vertex_ai_trial.py\",\n",
    "    container_uri=\"gcr.io/cloud-aiplatform/training/tf-gpu.2-4:latest\",\n",
    "    staging_bucket=f\"gs://{bucket_name}/staging\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_K80\",\n",
    "    accelerator_count=2,\n",
    ")"
   ],
   "id": "c52ea1868e7c305c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_final_metric(trial, metric_id):\n",
    "    for metric in trial.final_measurement.metrics:\n",
    "        if metric.metric_id == metric_id:\n",
    "            return metric.value\n",
    "\n",
    "\n",
    "trials = hp_job.trials\n",
    "trial_accuracies = [get_final_metric(trial, \"accuracy\") for trial in trials]\n",
    "best_trial = trials[np.argmax(trial_accuracies)]"
   ],
   "id": "cf9db8bad5ac2b52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "max(trial_accuracies)",
   "id": "7a49d9ac66eaaecc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "best_trial.id",
   "id": "de63a90005898359"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "best_trial.parameters",
   "id": "ac985aabafbe9a89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mnist_path = Path(\"datasets/mnist\")\n",
    "mnist_path.mkdir(parents=True, exist_ok=True)\n",
    "idx = 0\n",
    "with open(mnist_path / \"import.csv\", \"w\") as import_csv:\n",
    "    for split, X, y in zip((\"training\", \"validation\", \"test\"),\n",
    "                           (X_train, X_valid, X_test),\n",
    "                           (y_train, y_valid, y_test)):\n",
    "        for image, label in zip(X, y):\n",
    "            print(f\"\\r{idx + 1}/70000\", end=\"\")\n",
    "            filename = f\"{idx:05d}.png\"\n",
    "            plt.imsave(mnist_path / filename, np.tile(image, 3))\n",
    "            line = f\"{split},gs://{bucket_name}/mnist/{filename},{label}\\n\"\n",
    "            import_csv.write(line)\n",
    "            idx += 1"
   ],
   "id": "e5f59aadfadd7683"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "upload_directory(bucket, mnist_path)",
   "id": "c73ca79c996225a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
