# 10장. 확률적 경사 하강법

# 1. 점진적인 학습

- 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방법
- 확률적 경사 하강법(Stochastic Gradient Descent) : 점진적 학습 알고리즘, 훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것
    - 에포크(Epoch) : 훈련 세트드르 한 번 모두 사용하는 과정
    - 미니배치 경사 하강법(Minibatch Gradient Descent) : 여러 개의 샘플을 사용해 경사 하강법을 수행하는 방식
    - 배치 경사 하강법(Batch Gradient Descent) : 한 번 경사로를 따라 이동하기 위해 전체 샘플 사용
- 손실 함수(Loss Function) : 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준
- 로지스틱 손실 함수(Logistic Loss Function) : 이진 크로스엔트로피 손실 함수(Binary Cross-Entropy Loss Function), 손실 함수 사용 시 로지스틱 회귀 모델 생성
    - 크로스엔트로피 손실 함수 → 다중 분류에서 사용

# 2. SGDClassifier

- `partial_fit()` : 모델 이어서 훈련하기

# 3. 에포크와 과대/과소적합

- 에포크 횟수 적음 → 모델 훈련 세트 덜 학습
- 에포크 횟수 많음 → 훈련 세트 완전히 학습
- 조기 종료(Early Stopping) : 과대적합이 시작하기 전에 훈련을 멈춤
- 서포트 벡터 머신(Support Vector Machine) : 힌지 손실(Hinge Loss), 머신러닝 알고리즘을 위한 손실 함수