# 1. 파이토치와 훈련 성능

## 1. 성능 문제

- CPU < GPU

## 2. 파이토치

- 간편한 딥러닝 API를 포함하여 머신러닝 알고리즘을 구현하고 실행하기 위한 확장성이 좋은 멀티플랫폼 프로그래밍 인터페이스
- CPU, GPU, TPU 실행 가능
- 일련의 노드로 구성된 계산 그래프 중심

## 3. 파이토치 학습 방법

- 텐서 생성 및 조작
- 데이터 로드, 순환
- 데이터셋 사용
- 신경망 모듈
- 모델 구축 및 훈련

# 2. 파이토치 처음 시작하기

## 1. 파이토치 설치

- `pip install torch torchvision` : 파이토치 설치

## 2. 파이토치에서 텐서 만들기

- `torch.tensor`
- `torch.from_numpy`

## 3. 텐서의 데이터 타입과 크기 조작

- `torch.to()` : 텐서의 데이터 타입 변경
- `torch.transpose()` : 텐서 전치하기
- `torch.reshape()` : 텐서 크기 변경
- `torch.squeeze()` : 불필요한 차원 삭제

## 4. 텐서에 수학 연산 적용

- `torch.rand()` : 랜덤값 적용
- `torch.mean()` : 평균
- `torch.sum()` : 합
- `torch.std()` : 표준편차
- `torch.matmul()` : 행렬 곱
- `torch.linalg.norm()` : 노름 계산

## 5. chunk(), stack(), cat() 함수

- `torch.chunk()` : 입력 텐서 → 동일한 크기의 텐서 리스트
- `torch.stack()` : 여러 개의 텐서 연결
- `torch.cat()` : 여러 개의 텐서 연결

# 3. 파이토치 입력 파이프라인 구축

## 1. 텐서에서 파이토치 DataLoader 만들기

- `torch.utils.data.DataLoader()` : DataLoader 클래스 객체 반환

## 2. 두 개의 텐서를 하나의 데이터셋으로 연결

- `__init__()` : 기존 배열 읽기, 파일 로드, 데이터 필터링 등 초기화 로직
- `__getitem__()` : 주어진 인덱스에 해당하는 샘플 반환

## 3. 셔플, 배치, 반복

- 셔플 : 훈련 데이터 무작위로 섞기
- 배치 : 데이터셋 분할

## 4. 로컬 디스크에 있는 파일에서 데이터셋 만들기

- `pathlib` : 파일 경로 불러오기

## 5. torchvision.datasets 라이브러리에서 데이터셋 로드

- 딥러닝 모델 훈련과 평가에 사용할 수 있는 다양한 이미지 데이터셋 제공

# 4. 파이토치로 신경망 모델 만들기

## 1. 파이토치 신경망 모듈(torch.nn)

- 신경망 구축 및 훈련을 위해 고안된 모듈

## 2. 선형 회귀 모델 만들기

- 코드 작성

## 3. torch.nn과 torch.optim 모듈로 모델 훈련하기

- `torch.nn` : 여러 가지 손실 함수 제공
- `torch.optim` : 계산된 그레이디언트를 기반으로 파라미터를 업데이트하는 데 널리 사용되는 최적화 알고리즘 지원

## 4. 붓꽃 데이터셋을 분류하는 다층 퍼셉트론 만들기

- 코드 작성

## 5. 테스트 데이터셋에서 모델 평가하기

- 코드 작성

## 6. 훈련된 모델 저장하고 로드하기

# 5. 다층 신경망의 활성화 함수 선택

## 1. 로지스틱 함수 요약

- 시그모이드 함수

## 2. 소프트맥스 함수를 사용한 다중 클래스 확률 예측

- 특정 샘플의 최종 입력이 z일 때 i번째 클래스에 속할 확률을 지수적으로 가중된 선형 함수 합으로 나누어 정규화한 것

## 3. 하이퍼볼릭 탄젠트로 출력 범위 넓히기

- 스케일이 조정된 로지스틱 함수
- 출력 범위 (-1, 1) 사이로 넓힘

## 4. 렐루 활성화 함수

- 입력 값이 양수 → 입력에 대한 렐루의 도함수는 항상 1