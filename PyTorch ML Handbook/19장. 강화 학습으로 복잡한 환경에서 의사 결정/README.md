# 1. 경험에서 배운다

## 1. 강화 학습 이해

- 지도 학습, 비지도 학습과 근본적으로 다름
- 상호 작용을 통해 학습함
- 모델이 보상 함수를 최대화하기 위해 환경과 상호 작용하면서 학습
- 모델, 에이전트 → 환경과 상호 작용하기 위해 일련의 행동 생성(에피소드)
- 에이전트 → 환경이 제공하는 보상 수집
- 시행착오를 통해 얻은 결과 → 에이전트 성공, 실패에 따라 보상 결정

## 2. 강화 학습 시스템의 에이전트-환경 인터페이스 정의

- 에이전트 : 행동을 통해 주변 환경과 상호 작용하고 의사 결정 방법을 배우는 객체
- 환경 : 에이전트를 제외한 모든 것
- 보상 신호 : 에이전트가 환경과 상호 작용하면서 받는 피드백
- 학습 과정 중 에이전트 → 여러 행동 시도

# 2. 강화 학습의 기초 이론

## 1. 마르코프 결정 과정(MDP)

- 동적 계획법으로 해결 가능
- 상태 크기 증가 → DP 해결 불가

## 2. 마르코프 결정 과정의 수학 공식

- 확률 분포 → 환경의 동역학 완벽 정의
- 모든 환경의 전이 확률 계산
- 상태-전이 확률
- 유향 순환 그래프로 표현

## 3. 강화 학습 용어

- 대가 : 에피소드 전에 기간을 통해 얻은 누적된 보상
- 정책 : 다음에 선택할 행동을 결정하는 함수, 결정적, 확률적 가능
- 가치 함수 : 상태-가치 함수, 각 상태의 좋음 측정

# 3. 강화 학습 알고리즘

## 1. 동적 계획법

- 환경 동역학에 대해 완벽하게 알고 있음 → 모든 전이 확률
- 에이전트 상태 → 마르크프 성질
- 다음 행동, 보상 → 오직 현재 상태와 현재 타임 스텝에서 선택한 행동에 의해서만 결정

## 2. 몬테카를로를 사용한 강화 학습

- 환경의 상태-전이 확률 모름
- 에이전트 → 매 단계마다 행동 선택 → 모의 에피소드 생성

## 3. 시간 차 학습

- 환경 동역학, 전이 확률 모름
- 총 대가를 계산하기 위해 에피소드가 끝날 때까지 대기
- 부트스트래핑

# 4. 첫 번째 강화 학습 알고리즘 구현

- 코드 작성

# 5. 심층 Q-러닝

- 코드 작성