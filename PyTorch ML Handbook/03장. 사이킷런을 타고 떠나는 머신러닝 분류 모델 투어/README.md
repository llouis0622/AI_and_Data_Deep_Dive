# 1. 분류 알고리즘 선택

- 특성 선택, 훈련 샘플 수집
- 성능 지표 선택
- 학습 알고리즘 선택 → 모델 훈련
- 모델 성능 평가
- 알고리즘 설정 변경 → 모델 튜닝

# 2. 사이킷런 첫걸음 : 퍼셉트론 훈련

- 코드 작성

# 3. 로지스틱 회귀(Logistic Regression)를 사용한 클래스 확률 모델링

## 1. 로지스틱 회귀의 이해와 조건부 확률

- 로지스틱 회귀 : 선형적으로 구분되는 클래스에 뛰어난 성능, 분류 알고리즘 중 하나
- 다항 로지스틱 회귀(Multinomial Logistic Regression), 소프트맥스 회귀(Softmax Regression)
- 오즈비(Odds Ratio) : 특정 이벤트가 발생할 확률 비
- 로지스틱 시그모이드 함수(Logistic Sigmoid Function) : Logit 함수를 거꾸로 뒤집은 함수, 시그모이드 함수(Sigmoid Function)

## 2. 로지스틱 손실 함수의 가중치 학습

- 로그 함수 적용 → 가능도가 매우 작을 때 일어나는 수치상의 언더플로 방지 가능
- 계수의 곱 → 계수의 합 변환 가능

## 3. 아달린 구현을 로지스틱 회귀 알고리즘으로 변경

- 코드 작성

## 4. 사이킷런을 사용하여 로지스틱 회귀 모델 훈련

- 코드 작성

## 5. 규제를 사용하여 과대적합 피하기

- 과대적합(Overfitting) : 모델이 훈련 데이터로는 잘 동작하지만 본 적 없는 데이터로는 잘 일반화되지 않는 현상
- 과소적합(Underfitting) : 훈련 데이터에 있는 패턴을 감지할 정도로 충분히 모델이 복잡하지 않은 것
- 규제(Regularization) : 과도한 파라미터 값을 제한하기 위해 추가적인 정보를 주입하는 것
- L2 규제, L1 규제

# 4. 서포트 벡터 머신(SVM, Support Vector Machine)을 사용한 최대 마진 분류

- 퍼셉트론의 확장
- SVM 최적화 대상 → 마진 최대화
- 마진 : 클래스를 구분하는 초평면과 초평면에 가장 가까운 훈련 샘플 사이의 거리

## 1. 최대 마진

- 큰 마진 → 일반화 오차 낮아짐
- 작은 마진 → 과대적합 쉬움

## 2. 슬랙 변수를 사용하여 비선형 분류 문제 다루기

- 소프트 마진 분류(Soft Margin Classification)
- 슬랙 변수 : 선형적으로 구분되지 않는 데이터에서 SVM 최적화 목적 함수에 있는 선형 제약 조건 완화

## 3. 사이킷런의 다른 구현

- 코드 작성

# 5. 커널 SVM을 사용하여 비선형 문제 풀기

## 1. 선형적으로 구분되지 않는 데이터를 위한 커널 방법(Kernel Method)

- 매핑 함수를 사용하여 원본 특성의 비선형 조합을 선형적으로 구분되는 고차원 공간에 투영

## 2. 커널 기법을 사용하여 고차원 공간에서 분할 초평면 찾기

- 커널 함수 : 두 포인트 사이 점곱을 계산하는 데 드는 높은 손실 절감
- 방사 기저 함수(RBF, Radial Basis Function) : 가우스 커널
- 커널 → 샘플 간의 유사도 함수로 해석

# 6. 결정 트리(Decision Tree) 학습

- 훈련 데이터에 있는 특성을 기반으로 샘플의 클래스 레이블을 추정할 수 있는 일련의 질문 학습
- 트리의 루트에서 시작 → 정보 이득(IG, Information Gain)이 최대가 되는 특성으로 데이터 분류
- 트리의 최대 깊이 제한 → 트리 가지치기(Pruning)

## 1. 정보 이득 최대화 : 자원을 최대로 활용

- 가장 정보가 풍부한 특성으로 노드 분류 → 트리 알고리즘으로 최적화할 목적 함수 정의
- 이진 결정 트리 → 지니 불순도(Gini Impurity), 엔트로피(Entropy), 분류 오차(Classification Error)

## 2. 결정 트리 만들기

- 특성 공간 → 사각 격자로 나눔 → 복잡한 결정 경계 생성 가능

## 3. 랜덤 포레스트(Random Forest)로 여러 개의 결정 트리 연결

- 결정 트리 기반 알고리즘, 여러 개의 결정 트리 평균
- n개의 랜덤한 부트스트랩 샘플 선택 → 부트스트랩 샘플에서 결정 트리 학습 → 앞의 단계 반복 → 각 트리의 예측을 모아 다수결 투표로 클래스 레이블 할당

# 7. K-최근접 이웃(KNN, K-Nearest Neighbor) : 게으른 학습 알고리즘

- 훈련 데이터에서 판별 함수를 학습하는 대신 훈련 데이터셋을 메모리에 저장
- 숫자 k와 거리 측정 기준 선택 → 분류하려는 샘플에서 k개의 최근접 이웃 찾기 → 다수결 투표를 통해 클래스 레이블 할당