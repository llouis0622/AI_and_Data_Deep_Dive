# 1. 어텐션 메커니즘이 추가된 RNN

## 1. RNN의 정보 검색을 돕는 어텐션

- seq2seq : 번역 전 하나의 은닉 상태로 전체 입력 문장 기억
- 어텐션 메커니즘 : RNN이 타임 스텝마다 모든 입력 원소 참조 가능

## 2. RNN을 위한 원본 어텐션 메커니즘

- 입력 시퀀스 → 각 원소에 가중치 할당
- 모델 입력 → 초점 식별
- 어텐션 가중치

## 3. 양방향 RNN으로 입력 처리하기

- 문맥 벡터 → 입력 벡터 정제된 버전
- 다른 모든 입력 원소에 대한 정보 통합
- 입력 시퀀스 → 정방향, 역방향 처리 → 두 개의 은닉 상태

## 4. 문맥 벡터에서 출력 생성하기

- 은닉 상태 외 문맥 벡터 입력
- 문맥 벡터 → 연결된 은닉 상태에 가중치 부여

## 5. 어텐션 가중치 계산하기

- 얼라인먼트 점수 정규화된 버전 : 주변의 입력이 출력과 얼마나 잘 맞는지 평가
- 입력의 양방향 어노테이션 계산
- 기존 RNN과 매우 비슷한 순환 구조 구성
- 입력과 출력 원소 사이의 관계를 나타내는 어텐션 가중치와 문맥 벡터 계산 담당

# 2. 셀프 어텐션 메커니즘 소개

## 1. 기본적인 형태의 셀프 어텐션

- 다른 모든 입력 원소에 대한 현재 입력 원소의 의존성 모델링
- 현재 원소와 시퀀스에 있는 다른 모든 원소 사이의 유사도 기반으로 중요도 가중치 계산
- 소프트맥스 함수를 사용해서 가중치 정규화
- 가중치와 각 시퀀스 원소를 곱해서 어텐션 값 계산

## 2. 훈련 가능한 셀프 어텐션 메커니즘 : 스케일드 점곱 어텐션

- 스케일드 점곱 어텐션(Scaled Dot-product Attention)
- 쿼리, 키, 값 시퀀스 추가
- 정규화되지 않은 어텐션 가중치를 소프트맥스 함수를 사용하여 정규화된 가중치로 바꾸는 것

# 3. 어텐션이 필요한 전부다 : 원본 트랜스포머 아키텍처

## 1. 멀티 헤드 어텐션으로 문맥 임베딩 인코딩하기

- 인코더 : 입력 시퀀스를 받아 연속적인 표현으로 매핑 → 다음 디코더로 전달
- 동일한 층 6개 쌓은 것 → 인코더
- 멀티 헤드 셀프 어텐션 : 세 개의 행렬 → 하나의 어텐션 헤드로 생각
- 투영 행렬 초기화 → 스케일드 점곱 어텐션과 비슷한 방식으로 투영된 시퀀스 계산
- 멀티 헤드 셀프 어텐션 : 스케일 점곱 어텐션 계산 병렬로 여러 번 반복 → 결과 결합

## 2. 언어 모델 학습 : 디코더와 마스크드 멀티 헤드 어텐션

- 원래의 어텐션 메커니즘 변형
- 특정 수의 단어를 마스킹하여 제한된 입력 시퀀스만 모델에 전달

## 3. 구현 세부 사항 : 위치 인코딩 및 층 정규화

- 위치 인코딩 : 입력 시퀀스의 순서에 대한 정보 감지
- 트랜스포머 → 인코더, 디코더 블록 시작 전 입력 임베딩에 작은 값의 벡터 추가 → 서로 다른 위치의 동일한 단어가 약간 다른 인코딩을 가질 수 있도록 함
- 층 정규화 : 각 훈련 샘플에 대해 독립적으로 모든 특성 값에 걸쳐 수행, 각 훈련 샘플에 대한 평균과 표준 편차 계산

# 4. 레이블이 없는 데이터를 활용하여 대규모 언어 모델 구축

## 1. 트랜스포머 모델 사전 훈련 및 미세 튜닝

- 자기 지도 학습 : 일반 텍스트 자체에서 지도 학습을 위한 레이블 생성
- 비지도 사전 훈련 : 데이터의 구조를 사용하여 레이블 생성
- 일반 텍스트를 활용한 다음 모델을 레이블이 있는 데이터셋에 미세 튜닝 → 특정 작업 수행

## 2. GPT로 레이블이 없는 데이터 활용하기

- 레이블이 없는 대량의 텍스트에서 사전 훈련
- 지도 학습 방식의 미세 튜닝
- GPT-1 : 사전 훈련 시 트랜스포머 디코더 구조 활용, 제로 샷 작업
- GPT-2 : 입력, 미세 튜닝 단계에서 더 이상 추가 수정 불필요
- GPT-3 : 원 샷, 인-컨텍스트 학습을 통한 퓨 샷 학습, 희소 어텐션 사용

## 3. GPT-2를 사용하여 새로운 텍스트 생성

- 코드 작성

## 4. BERT를 통한 양방향 사전 훈련

- 트랜스포머 인코더 기반 모델 구조
- 정보 양방향 처리 → 분류 작업에서의 높은 품질의 입력 인코딩 제공
- 다음 문장 예측 → BERT의 특별한 사전 훈련 작업

## 5. 두 장점을 합친 BART

- GPT + BERT
- 모델에 양방향 인코더와 왼쪽에서 오른쪽으로 자기회귀하는 디코더 함께 존재
- 토큰 마스킹, 토큰 삭제, 텍스트 채우기, 문장 섞기, 문서 회전
- 인코더를 제외한 모든 파라미터 동결 → 모델의 모든 파라미터 업데이트

# 5. 파이토치에서 BERT 모델 미세 튜닝하기

- 코드 작성