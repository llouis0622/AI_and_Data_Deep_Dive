# 1. 순차 데이터 소개

## 1. 순차 데이터 모델링

- 시퀀스 : 특성 순서 보유, 상호 독립적이지 않음
- 독립 동일 분포(IID, Independent and Identically Distributed) → 지도 학습 머신러닝 알고리즘

## 2. 순차 데이터 vs 시계열 데이터

- 시계열 데이터 : 각 샘플이 시간 차원에 연관
- 연속적인 타임스탬프를 따라 샘플 얻음
- 시간 차원 → 데이터 포인트 사이의 순서 결정

## 3. 시퀀스 표현

- 시퀀스 → 시계열 데이터
- 과거 정보 기억 → 새로운 샘플 처리

## 4. 시퀀스 모델링의 종류

- 다대일(Many-to-One) : 입력 데이터 시퀀스, 출력 데이터 고정 크기 벡터, 스칼라
- 일대다(One-to-Many) : 입력 데이터 일반적인 형태, 출력 데이터 시퀀스
- 다대다(Many-to-Many) : 입력, 출력 모두 시퀀스
- 동기적 다대다 모델링 : 각 프레임 레이블링, 비디오 분류
- 지연 다대다 모델링 : 한 언어 → 다른 언어 번역

# 2. 시퀀스 모델링을 위한 RNN

## 1. RNN 반복 구조 이해

- 은닉층 → 현재 타임 스텝의 입력층과 이전 타임 스텝의 은닉층으로부터 정보 받음
- 순환 에지
- 은닉 유닛 → 두 개의 다른 입력 받음

## 2. RNN의 활성화 출력 계산

- 유향 에지 → 가중치 행렬과 연관
- W_xh : 입력과 은닉층 사이의 가중치 행렬
- W_hh : 순환 에지에 연관된 가중치 행렬
- W_h0 : 은닉층과 출력층 사이의 가중치 행렬

## 3. 은닉 순환과 출력 순환

- 현재 타임 스텝에서 은닉층에 추가
- 현재 타임 스텝에서 출력층에 추가
- 은닉-은닉 순환, 출력-은닉 순환, 출력-출력 순환

## 4. 긴 시퀀스 학습의 어려움

- 그레이디언트 폭주(Exploding Gradient), 그레이디언트 소실(Vanishing Gradient)
- 그레이디언트 클리핑, TBPTT, LSTM

## 5. LSTM 셀

- 그레이디언트 소실 문제 극복
- 일반 RNN 은닉층을 표현, 대체하는 메모리 셀
- 삭제 게이트 : 메모리 셀이 무한정 성장하지 않도록 셀 상태 다시 설정
- 입력 게이트 : 셀 상태를 업데이트하는 역할
- 출력 게이트 : 은닉 유닛의 출력 값을 업데이트

# 3. 파이토치로 시퀀스 모델링을 위한 RNN 구현

## 1. 첫 번째 프로젝트 : IMDB 영화 리뷰 감성 분석

- 코드 작성

## 2. 두 번째 프로젝트 : 텐서플로로 글자 단위 언어 모델 구현

- 코드 작성