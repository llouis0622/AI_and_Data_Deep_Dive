# 1. 인공 뉴런 : 초기 머신러닝의 간단한 역사

## 1. 인공 뉴런의 수학적 정의

- 인공 뉴런 : 0과 1 두 개의 클래스가 있는 이진 분류 작업으로 볼 수 있음
- 입력 값 x, 상응하는 가중치 벡터 w 선형 조합 → 결정 함수 정의

## 2. 퍼셉트론 학습 규칙

- 가중치 → 0, 랜덤한 작은 값으로 초기화
- 각 훈련 샘플 → 출력 값 계산, 가중치, 절편 업데이트
- 가중치 벡터 → 모든 가중치 및 절편 유닛 동시 업데이트

# 2. 파이썬으로 퍼셉트론 학습 알고리즘 구현

## 1. 객체지향 퍼셉트론 API

## 2. 퍼셉트론 훈련

- 일대다(OvA, One-versus-All) : 이진 분류기 → 다중 클래스 문제 적용

# 3. 적응형 선형 뉴런과 학습의 수렴(ADALINE, Adaptive Linear Neuron)

- 단일층 신경망, 퍼셉트론 향상된 버전
- 연속 함수 → 손실 함수 정의 및 최소화
- 가중치 업데이트 → 선형 활성화 함수 사용

## 1. 경사 하강법으로 손실 함수 최소화

- 목적 함수 : 최소화하려는 손실 함수, 비용 함수
- 평균 제곱 오차(MSE, Mean Squared Error) → 모델 파라미터 학습을 위한 손실 함수
- 선형 활성화 함수 → 손실 함수 미분 가능
- 경사 하강법 → 손실 함수의 그레이디언트 반대 방향으로 파라미터 업데이트
- 배치 경사 하강법 : 훈련 데이터셋에 있는 모든 샘플 기반으로 가중치 업데이트 계산

## 3. 특성 스케일을 조정하여 경사 하강법 결과 향상

- 표준화(Standardization) : 각 특성의 평균을 0에 맞추고 특성의 표준 편차를 1로 만듦
- 모든 가중치에 적합한 학습률 찾기 쉬움

## 4. 대규모 머신러닝과 확률적 경사 하강법

- 완전 배치 경사 하강법 : 전체 훈련 데이터셋에서 계산한 그레이디언트 반대 방향으로 한 걸음씩 진행하여 손실 함수 최소화하는 방법
- 확률적 경사 하강법(Stochastic Gradient Descent) : 각 훈련 샘플에 대해 점진적 파라미터 업데이트, 온라인 학습 가능