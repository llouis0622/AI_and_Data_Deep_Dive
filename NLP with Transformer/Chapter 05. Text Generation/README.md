# 1. 일관성 있는 텍스트 생성의 어려움

- 모델 확률 출력을 텍스트로 변환 → 디코딩 방법 필요
    - 입력이 모델의 정방향 패스를 한 번 통과할 때보다 많은 계산 필요
    - 텍스트의 품질과 다양성은 하이퍼파라미터에 따라 달라짐
- 자기회귀 모델(autoregressive model), 코잘 언어 모델(causal language model)

# 2. 그리디 서치 디코딩

- 연속적인 모델 출력에서의 이산적인 토큰 얻기 → 각 타임스텝에서 확률이 가장 높은 토큰 탐욕적 선택

# 3. 빔 서치 디코딩

- 각 스텝에서 확률이 가장 높은 토큰을 디코딩하는 대신 확률이 가장 높은 상위 b개의 다음 토큰 추적
- 빔, 불완전 가설 개수

# 4. 샘플링 방법

- 각 타임스텝 내에 모델이 출력한 전체 어휘사전의 확률 분포에서 랜덤하게 샘플링
- 어휘사전의 분포를 잘나내는 방법

# 5. 탑-k 및 뉴클리어스 샘플링

- 각 타임스텝에서 샘플리에 사용할 토큰의 개수 줄임
- 동적인 컷오프 적용

# 6. 어떤 디코딩 방법이 최선인가

- 텍스트 생성 작업의 특성에 따라 다름
- 정밀한 작업 → 빔 서치, 그리디 서치
- 창의적인 텍스트 생성 → 탑-k, 뉴클리어스 샘플링