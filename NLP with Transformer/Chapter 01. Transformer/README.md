# 1. 인코더-디코더 프레임워크

- RNN : 입력 → 네트워크 통과 → 은닉 상태 벡터 출력
    - 각 스텝에서 상태에 대한 정보 → 시퀀스의 다음 작업으로 전달
    - 이전 스텝의 정보 추정 후 예측 생성
    - 인코더-디코더, 시퀀스-투-시퀀스

# 2. 어텐션 메커니즘

- 입력 시퀀스에서 은닉 상태를 만들지 않고 스텝마다 인코더에서 디코더가 참고할 은닉 상태를 출력함
- 어텐션 : 어떤 상태를 먼저 사용할지 우선순위를 정하는 메커니즘 필요
- 셀프 어텐션 : 신경망의 같은 층에 있는 모든 상태에 대해서 어텐션을 작동시키는 방식

# 3. NLP의 전이 학습

- 컴퓨터 비전 → 전이 학습 사용 → 새로운 작업 적용
- NLP → 감성 분류 작업에 비지도 사전 훈련에서 추출한 특성 사용
- 사전 훈련 → 언어 모델링(Language Modeling) : 이전 단어를 바탕으로 다음 단어를 예측하는 것
- 도메인 적응 → 언어 모델을 대규모 말뭉치에서 사전 훈련한 후 다음 단계로 도메인 내 말뭉치에 적응
- 미세 튜닝 → 언어 모델을 타깃 작업을 위한 분류 층과 함께 미세 튜닝
- GPT : 트랜스포머 아키텍처의 디코더 부분 + ULMFiT 같은 언어 모델링 방법 사용
- BERT : 트랜스포머 아키텍처의 인코더 부분 + 마스크드 언어 모델링 방법 사용

# 4. 허깅페이스 트랜스포머스

- 머신러닝 아키텍처 → 새로운 작업 적용
    - 모델 아키텍처 코드 구현 → 파이토치 or 텐서플로
    - 서버 → 사전 훈련된 가중치 로드
    - 입력 전처리, 모델 전달 → 해당 작업에 맞는 사후 처리 수행
    - 데이터로더 구현, 모델 훈련을 위해 손실 함수, 옵티마이저 정의
- 매우 다양한 트랜스포머 모델에 표준화된 인터페이스 제공
- 새로운 문제에 모델을 적용하는 코드와 도구 제공
- 파이토치, 텐서플로, JAX 지원, 프레임워크 전환 용이

# 5. 트랜스포머 애플리케이션

## 1. 텍스트 분류(Text Classification)

- 파이프라인(Pipeline) : 원시 텍스트를 미세 튜닝된 모델의 예측으로 변환하기 위해 필요한 모든 단계 추상화
- 감성 분석을 위해 설계된 모델 사용, 다중 분류, 다중 레이블 분류 지원

## 2. 개체명 인식(NER, Named Entity Recognition)

- 개체명 : 제품, 장소, 사람 같은 실제 객체
- 개체명을 텍스트에서 추출하는 작업

## 3. 질문 답변(Question Answering)

- 텍스트 구절과 함께 답을 얻고 싶은 질문을 모델에 전달, 모델은 답변 텍스트 반환
- 추출적 질문 답변 : 답변을 텍스트에서 직접 추출

## 4. 요약(Text Summarization)

- 긴 텍스트 → 관련 사실이 모두 포함된 간단한 버전 생성

## 5. 번역

- 텍스트를 생성해 출력하는 작업

## 6. 텍스트 생성

- 자동 완성 기능

# 6. 허깅페이스 생태계

- 라이브러리 : 코드 제공
- 허브 : 사전 훈련된 모델 가중치, 데이터셋, 평가 지표를 위한 스크립트 제공

## 1. 허깅페이스 허브

- 2만 개의 모델 호스팅
- 코드 한 줄로 로드 가능
- 모델 카드, 데이터셋 카드 제공

## 2. 허깅페이스 토크나이저

- 러스트 백엔드 사용
- 입력 정규화, 모델 출력 적절한 포맷으로 변환
- 전처리, 사후처리 모두 가능

## 3. 허깅페이스 데이터셋

- 데이터셋 표준 인터페이스 제공
- 메모리 매핑 → 램 부족 회피

## 4. 허깅페이스 액셀러레이트

- 사용자 정의 로직을 처리하는 일반적인 훈련 루프에 훈련 인프라에 필요한 추상화 층 추가

# 7. 트랜스포머 주요 도전 과제

- 언어
- 데이터 가용성
- 긴 문서 처리하기
- 불투명성
- 편향