# 1. 대규모 데이터셋 수집하기

## 1. 대규모 말뭉치 구축의 어려움

- 데이터셋 규모 증가 → 내용 제어 및 이해 기회 감소
- 대규묘 데이터셋 → 고도의 자동화로 생성
- 사전 훈련 데이터셋 → 차이 발생

## 2. 사용자 정의 코드 데이터셋 만들기

- 깃허브 저장소
    - 깃허브 REST API 사용
    - 구글 빅쿼리 등 공개 데이터셋 사용

## 3. 대용량 데이터셋 다루기

- 컴퓨터 램 크기 < 대용량 데이터셋 로딩 → 문제 발생
- 메모리 매핑 : 데이터셋 제로-카피, 제로-오버헤드로 메모리 매핑 사용
- 스트리밍 : 여러 종류의 압축, 비압축 파일 포맷 → 데이터셋으로 바로 처리 및 로드

## 4. 허깅페이스 허브에 데이터셋 추가하기

- 훈련 서버에서 쉽게 다운로드 가능
- 스트리밍 데이터셋 허브 데이터셋과 함께 사용
- 커뮤니티 공유 가능

# 2. 토크나이저 구축하기

## 1. 토크나이저 모델

- BPE
- WordPiece
- 유니그램

## 2. 토크나이저 성능 측정하기

- 부분단어 생산력 : 토큰화된 단어마다 생성되는 부분단어의 평균 개수 계산
- 연속 단어 비율 : 말뭉치에서 적어도 두 개의 부분 토큰으로 분할된 토큰화된 단어 비율
- 커버리지 측정값 : 토큰화된 말뭉치에서 알 수 없는 단어나 거의 사용되지 않는 토큰 비율

## 3. 파이썬 코드를 위한 토크나이저

- 오프셋 트래킹
- 단어 매핑

## 4. 토크나이저 훈련하기

- 목표 어휘사전 크기 지정
- 입력 문자열을 공급할 반복자 준비
- train_new_from_iterator() 메서드 호출

## 5. 허브에 사용자 정의 토크나이저 저장하기

- 토크나이저 저장 및 불러오기 가능

# 3. 밑바닥부터 모델 훈련하기

## 1. 사전 훈련 목표

- 코잘 언어 모델링 : 레이블이 없는 데이터셋을 사용하는 자기 지도 훈련
- 마스크드 언어 모델링 : 잡음 제거 목표, 자기 지도 훈련
- 시퀀스-투-시퀀스 훈련 : 레이블링된 데이터셋으로 사용할 수 있도록 쌍의 대규모 데이터셋 구축 작업

## 2. 모델 초기화

- 새 모델 생성
- 이전 모델 로드 및 미세 튜닝

## 3. 데이터로더 구축하기

- 시퀀스 모델 제공
- 청크 단위 분리

## 4. 훈련 루프 정의하기

- 데이터 병렬화
- 모델 저장
- 최적화
- 평가
- 그레이디언트 누적과 체크포인팅

## 5. 훈련 실행

- 훈련 스크립트 저장
- 훈련 실행 → 24시간 ~ 7일 이상 소요

# 4. 결과 및 분석

- 훈련 손실 및 검증 복잡도 분석
- 생성된 텍스트 품질 측정