# 1. 트랜스포머 아키텍처

- 원본 트랜스포머 : 인코더-디코더 구조 기반
- 인코더 : 입력 토큰 시퀀스를 임베딩 벡터 시퀀스로 변환, 은닉 상태, 문맥
- 디코더 : 은닉 상태 사용 → 출력 토큰의 시퀀스를 한 번에 하나씩 반복적으로 생성
- 인코더 유형 : 텍스트 시퀀스 입력 → 풍부한 수치 표현으로 변환, 양방향 어텐션
- 디코더 유형 : 코잘 어텐션, 자기회귀 어텐션
- 인코더-디코더 유형 : 한 텍스트의 시퀀스를 다른 시퀀스로 매핑

# 2. 인코더

## 1. 셀프 어텐션

- 텍스트 시퀀스 → 토큰 임베딩
- 가중치가 동일 집합에 있는 모든 은닉 상태에 대해 계산됨
- 전체 시퀀스 → 각 임베딩의 가중 평균 계산
- 어텐션 가중치 정규화
- 문맥 고려 임베딩
- 스케일드 점곱 어텐션(scaled dot-product attention)
    - 각 토큰 임베딩 → 쿼리, 키, 값 벡터로 투영
    - 어텐션 점수 계산 : 유사도 함수 사용
    - 어텐션 가중치 계산
    - 토큰 임베딩 업데이트
- 멀티 헤드 어텐션
    - 각 투영 집합 : 어텐션 헤드
    - 한 헤드의 소프트맥스가 유사도의 한 측면에만 초점을 맞춤

## 2. 피드 포워트 층(Feed Forward Layer)

- 간단한 두 개 층으로 구성된 완전 연결 신경망
- 커널 크기가 1인 1차원 합성곱

## 3. 층 정규화 추가하기

- 층 정규화(Layer Normalization) : 배치에 있는 각 입력을 평균이 0이고 단위 분산을 가지도록 정규화
- 스킵 연결(Skip Connection) : 처리하지 않은 텐서를 모델의 다음 층으로 전달 후 처리된 텐서와 더함
- 사후 층 정규화 : 스킵 연결 사이에 층 정규화 삽입, 학습률 웜업
- 사전 층 정규화 : 스킵 연결 안에 층 정규화 놓음, 안정적

## 4. 위치 임베딩

- 벡터에 나열된 값의 위치 패턴으로 토큰 임베딩 보강
- 학습 가능한 패턴 사용
- 절대 위치 표현 : 변조된 사인 및 코사인 신호로 구성된 정적 패턴을 사용해 토큰 위치 인코딩
- 상대 위치 표현 : 직관을 따라 토큰의 상태 위치 인코딩

## 5. 분류 헤드 추가하기

- 작업에 독립적인 바디 + 작업에 특화된 헤드
- 바디에 연결할 분류 헤드 필요

# 3. 디코더

- 마스크드 멀티 헤드 셀프 어텐션 층 : 타임스텝마다 지난 출력과 예측한 현재 토큰만 사용하여 토큰 생성
- 인코더-디코더 어텐션 층 : 디코더의 중간 표현을 쿼리처럼 사용해서 인코더 스택의 출력 키와 값 벡터에 멀티 헤드 어텐션 수행

# 4. 트랜스포머 유니버스

## 1. 트랜스포머 가계도

- 트랜스포머
    - 인코더 → BERT → roBERTa → XLM → ALBERT → ELECTRA → DeBERTa 등
    - 디코더 → GPT → GPT-2 → GPT-3 → GPT-Neo → GPT-J 등
    - T5 → BART → M2M-100 → BigBird