{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-23T07:09:36.044458Z",
     "start_time": "2024-07-23T07:08:47.089416Z"
    }
   },
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype\n",
    "\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "os.environ['JOBLIB_START_METHOD'] = 'forkserver'\n",
    "\n",
    "INPUT_PATH = r'../data/mercari'\n",
    "\n",
    "\n",
    "def dameraulevenshtein(seq1, seq2):\n",
    "    oneago = None\n",
    "    thisrow = list(range(1, len(seq2) + 1)) + [0]\n",
    "    for x in range(len(seq1)):\n",
    "        twoago, oneago, thisrow = (oneago, thisrow, [0] * len(seq2) + [x + 1])\n",
    "        for y in range(len(seq2)):\n",
    "            delcost = oneago[y] + 1\n",
    "            addcost = thisrow[y - 1] + 1\n",
    "            subcost = oneago[y - 1] + (seq1[x] != seq2[y])\n",
    "            thisrow[y] = min(delcost, addcost, subcost)\n",
    "            if (x > 0 and y > 0 and seq1[x] == seq2[y - 1] and seq1[x - 1] == seq2[y] and seq1[x] != seq2[y]):\n",
    "                thisrow[y] = min(thisrow[y], twoago[y - 2] + 1)\n",
    "    return thisrow[len(seq2) - 1]\n",
    "\n",
    "\n",
    "class SymSpell:\n",
    "    def __init__(self, max_edit_distance=3, verbose=0):\n",
    "        self.max_edit_distance = max_edit_distance\n",
    "        self.verbose = verbose\n",
    "        self.dictionary = {}\n",
    "        self.longest_word_length = 0\n",
    "\n",
    "    def get_deletes_list(self, w):\n",
    "        deletes = []\n",
    "        queue = [w]\n",
    "        for d in range(self.max_edit_distance):\n",
    "            temp_queue = []\n",
    "            for word in queue:\n",
    "                if len(word) > 1:\n",
    "                    for c in range(len(word)):\n",
    "                        word_minus_c = word[:c] + word[c + 1:]\n",
    "                        if word_minus_c not in deletes:\n",
    "                            deletes.append(word_minus_c)\n",
    "                        if word_minus_c not in temp_queue:\n",
    "                            temp_queue.append(word_minus_c)\n",
    "            queue = temp_queue\n",
    "\n",
    "        return deletes\n",
    "\n",
    "    def create_dictionary_entry(self, w):\n",
    "        new_real_word_added = False\n",
    "        if w in self.dictionary:\n",
    "            self.dictionary[w] = (self.dictionary[w][0], self.dictionary[w][1] + 1)\n",
    "        else:\n",
    "            self.dictionary[w] = ([], 1)\n",
    "            self.longest_word_length = max(self.longest_word_length, len(w))\n",
    "\n",
    "        if self.dictionary[w][1] == 1:\n",
    "            new_real_word_added = True\n",
    "            deletes = self.get_deletes_list(w)\n",
    "            for item in deletes:\n",
    "                if item in self.dictionary:\n",
    "                    self.dictionary[item][0].append(w)\n",
    "                else:\n",
    "                    self.dictionary[item] = ([w], 0)\n",
    "\n",
    "        return new_real_word_added\n",
    "\n",
    "    def create_dictionary_from_arr(self, arr, token_pattern=r'[a-z]+'):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        for line in arr:\n",
    "            words = re.findall(token_pattern, line.lower())\n",
    "            for word in words:\n",
    "                total_word_count += 1\n",
    "                if self.create_dictionary_entry(word):\n",
    "                    unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def create_dictionary(self, fname):\n",
    "        total_word_count = 0\n",
    "        unique_word_count = 0\n",
    "\n",
    "        with open(fname) as file:\n",
    "            for line in file:\n",
    "                words = re.findall('[a-z]+', line.lower())\n",
    "                for word in words:\n",
    "                    total_word_count += 1\n",
    "                    if self.create_dictionary_entry(word):\n",
    "                        unique_word_count += 1\n",
    "\n",
    "        print(\"total words processed: %i\" % total_word_count)\n",
    "        print(\"total unique words in corpus: %i\" % unique_word_count)\n",
    "        print(\"total items in dictionary (corpus words and deletions): %i\" % len(self.dictionary))\n",
    "        print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "        print(\"  length of longest word in corpus: %i\" % self.longest_word_length)\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_suggestions(self, string, silent=False):\n",
    "        if (len(string) - self.longest_word_length) > self.max_edit_distance:\n",
    "            if not silent:\n",
    "                print(\"no items in dictionary within maximum edit distance\")\n",
    "            return []\n",
    "\n",
    "        suggest_dict = {}\n",
    "        min_suggest_len = float('inf')\n",
    "\n",
    "        queue = [string]\n",
    "        q_dictionary = {}\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            q_item = queue[0]\n",
    "            queue = queue[1:]\n",
    "\n",
    "            if ((self.verbose < 2) and (len(suggest_dict) > 0) and\n",
    "                    ((len(string) - len(q_item)) > min_suggest_len)):\n",
    "                break\n",
    "\n",
    "            if (q_item in self.dictionary) and (q_item not in suggest_dict):\n",
    "                if self.dictionary[q_item][1] > 0:\n",
    "                    assert len(string) >= len(q_item)\n",
    "                    suggest_dict[q_item] = (self.dictionary[q_item][1], len(string) - len(q_item))\n",
    "\n",
    "                    if (self.verbose < 2) and (len(string) == len(q_item)):\n",
    "                        break\n",
    "                    elif (len(string) - len(q_item)) < min_suggest_len:\n",
    "                        min_suggest_len = len(string) - len(q_item)\n",
    "\n",
    "                for sc_item in self.dictionary[q_item][0]:\n",
    "                    if sc_item not in suggest_dict:\n",
    "                        assert len(sc_item) > len(q_item)\n",
    "                        assert len(q_item) <= len(string)\n",
    "\n",
    "                        if len(q_item) == len(string):\n",
    "                            assert q_item == string\n",
    "                            item_dist = len(sc_item) - len(q_item)\n",
    "\n",
    "                        assert sc_item != string\n",
    "\n",
    "                        item_dist = dameraulevenshtein(sc_item, string)\n",
    "\n",
    "                        if (self.verbose < 2) and (item_dist > min_suggest_len):\n",
    "                            pass\n",
    "                        elif item_dist <= self.max_edit_distance:\n",
    "                            assert sc_item in self.dictionary\n",
    "                            suggest_dict[sc_item] = (self.dictionary[sc_item][1], item_dist)\n",
    "                            if item_dist < min_suggest_len:\n",
    "                                min_suggest_len = item_dist\n",
    "\n",
    "                        if self.verbose < 2:\n",
    "                            suggest_dict = {k: v for k, v in suggest_dict.items() if v[1] <= min_suggest_len}\n",
    "\n",
    "            assert len(string) >= len(q_item)\n",
    "\n",
    "            if (self.verbose < 2) and ((len(string) - len(q_item)) > min_suggest_len):\n",
    "                pass\n",
    "            elif (len(string) - len(q_item)) < self.max_edit_distance and len(q_item) > 1:\n",
    "                for c in range(len(q_item)):\n",
    "                    word_minus_c = q_item[:c] + q_item[c + 1:]\n",
    "                    if word_minus_c not in q_dictionary:\n",
    "                        queue.append(word_minus_c)\n",
    "                        q_dictionary[word_minus_c] = None\n",
    "        if not silent and self.verbose != 0:\n",
    "            print(\"number of possible corrections: %i\" % len(suggest_dict))\n",
    "            print(\"  edit distance for deletions: %i\" % self.max_edit_distance)\n",
    "\n",
    "        as_list = suggest_dict.items()\n",
    "        outlist = sorted(as_list, key=lambda x: (x[1][1], -x[1][0]))\n",
    "\n",
    "        if self.verbose == 0:\n",
    "            return outlist[0]\n",
    "        else:\n",
    "            return outlist\n",
    "\n",
    "    def best_word(self, s, silent=False):\n",
    "        try:\n",
    "            return self.get_suggestions(s, silent)[0]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field, start_time=time()):\n",
    "        self.field = field\n",
    "        self.start_time = start_time\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe):\n",
    "        print(f'[{time() - self.start_time}] select {self.field}')\n",
    "        dt = dataframe[self.field].dtype\n",
    "        if is_categorical_dtype(dt):\n",
    "            return dataframe[self.field].cat.codes[:, None]\n",
    "        elif is_numeric_dtype(dt):\n",
    "            return dataframe[self.field][:, None]\n",
    "        else:\n",
    "            return dataframe[self.field]\n",
    "\n",
    "\n",
    "class DropColumnsByDf(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_df=1, max_df=1.0):\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        self.nnz_cols = ((m != 0).sum(axis=0) >= self.min_df).A1\n",
    "        if self.max_df < 1.0:\n",
    "            max_df = m.shape[0] * self.max_df\n",
    "            self.nnz_cols = self.nnz_cols & ((m != 0).sum(axis=0) <= max_df).A1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        m = X.tocsc()\n",
    "        return m[:, self.nnz_cols]\n",
    "\n",
    "\n",
    "def get_rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred)))\n",
    "\n",
    "\n",
    "def split_cat(text):\n",
    "    try:\n",
    "        cats = text.split(\"/\")\n",
    "        return cats[0], cats[1], cats[2], cats[0] + '/' + cats[1]\n",
    "    except:\n",
    "        print(\"no category\")\n",
    "        return 'other', 'other', 'other', 'other/other'\n",
    "\n",
    "\n",
    "def brands_filling(dataset):\n",
    "    vc = dataset['brand_name'].value_counts()\n",
    "    brands = vc[vc > 0].index\n",
    "    brand_word = r\"[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\"\n",
    "\n",
    "    many_w_brands = brands[brands.str.contains(' ')]\n",
    "    one_w_brands = brands[~brands.str.contains(' ')]\n",
    "\n",
    "    ss2 = SymSpell(max_edit_distance=0)\n",
    "    ss2.create_dictionary_from_arr(many_w_brands, token_pattern=r'.+')\n",
    "\n",
    "    ss1 = SymSpell(max_edit_distance=0)\n",
    "    ss1.create_dictionary_from_arr(one_w_brands, token_pattern=r'.+')\n",
    "\n",
    "    two_words_re = re.compile(r\"(?=(\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+))\")\n",
    "\n",
    "    def find_in_str_ss2(row):\n",
    "        for doc_word in two_words_re.finditer(row):\n",
    "            print(doc_word)\n",
    "            suggestion = ss2.best_word(doc_word.group(1), silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word.group(1)\n",
    "        return ''\n",
    "\n",
    "    def find_in_list_ss1(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss1.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "\n",
    "    def find_in_list_ss2(list):\n",
    "        for doc_word in list:\n",
    "            suggestion = ss2.best_word(doc_word, silent=True)\n",
    "            if suggestion is not None:\n",
    "                return doc_word\n",
    "        return ''\n",
    "\n",
    "    print(f\"Before empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "\n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_name]\n",
    "\n",
    "    n_desc = dataset[dataset['brand_name'] == '']['item_description'].str.findall(\n",
    "        pat=r\"^[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\\s[a-z0-9*/+\\-'’?!.,|&%®™ôèéü]+\")\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss2(row) for row in n_desc]\n",
    "\n",
    "    n_name = dataset[dataset['brand_name'] == '']['name'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in n_name]\n",
    "\n",
    "    desc_lower = dataset[dataset['brand_name'] == '']['item_description'].str.findall(pat=brand_word)\n",
    "    dataset.loc[dataset['brand_name'] == '', 'brand_name'] = [find_in_list_ss1(row) for row in desc_lower]\n",
    "\n",
    "    print(f\"After empty brand_name: {len(dataset[dataset['brand_name'] == ''].index)}\")\n",
    "\n",
    "    del ss1, ss2\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def preprocess_regex(dataset, start_time=time()):\n",
    "    karats_regex = r'(\\d)([\\s-]?)(karat|karats|carat|carats|kt)([^\\w])'\n",
    "    karats_repl = r'\\1k\\4'\n",
    "\n",
    "    unit_regex = r'(\\d+)[\\s-]([a-z]{2})(\\s)'\n",
    "    unit_repl = r'\\1\\2\\3'\n",
    "\n",
    "    dataset['name'] = dataset['name'].str.replace(karats_regex, karats_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(karats_regex, karats_repl)\n",
    "    print(f'[{time() - start_time}] Karats normalized.')\n",
    "\n",
    "    dataset['name'] = dataset['name'].str.replace(unit_regex, unit_repl)\n",
    "    dataset['item_description'] = dataset['item_description'].str.replace(unit_regex, unit_repl)\n",
    "    print(f'[{time() - start_time}] Units glued.')\n",
    "\n",
    "\n",
    "def preprocess_pandas(train, test, start_time=time()):\n",
    "    train = train[train.price > 0.0].reset_index(drop=True)\n",
    "    print('Train shape without zero price: ', train.shape)\n",
    "\n",
    "    nrow_train = train.shape[0]\n",
    "    y_train = np.log1p(train[\"price\"])\n",
    "    merge: pd.DataFrame = pd.concat([train, test])\n",
    "\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "\n",
    "    merge['has_category'] = (merge['category_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_category filled.')\n",
    "\n",
    "    merge['category_name'] = merge['category_name'].fillna('other/other/other').str.lower().astype(str)\n",
    "    merge['general_cat'], merge['subcat_1'], merge['subcat_2'], merge['gen_subcat1'] = zip(\n",
    "        *merge['category_name'].apply(lambda x: split_cat(x)))\n",
    "    print(f'[{time() - start_time}] Split categories completed.')\n",
    "\n",
    "    merge['has_brand'] = (merge['brand_name'].notnull()).astype('category')\n",
    "    print(f'[{time() - start_time}] Has_brand filled.')\n",
    "\n",
    "    merge['gencat_cond'] = merge['general_cat'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_1_cond'] = merge['subcat_1'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    merge['subcat_2_cond'] = merge['subcat_2'].map(str) + '_' + merge['item_condition_id'].astype(str)\n",
    "    print(f'[{time() - start_time}] Categories and item_condition_id concancenated.')\n",
    "\n",
    "    merge['name'] = merge['name'].fillna('').str.lower().astype(str)\n",
    "    merge['brand_name'] = merge['brand_name'].fillna('').str.lower().astype(str)\n",
    "    merge['item_description'] = merge['item_description'].fillna('').str.lower().replace(\n",
    "        to_replace='No description yet', value='')\n",
    "    print(f'[{time() - start_time}] Missing filled.')\n",
    "\n",
    "    preprocess_regex(merge, start_time)\n",
    "\n",
    "    brands_filling(merge)\n",
    "    print(f'[{time() - start_time}] Brand name filled.')\n",
    "\n",
    "    merge['name'] = merge['name'] + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Name concancenated.')\n",
    "\n",
    "    merge['item_description'] = merge['item_description'] + ' ' + merge['name'] + ' ' + merge['subcat_1'] + ' ' + merge[\n",
    "        'subcat_2'] + ' ' + merge['general_cat'] + ' ' + merge['brand_name']\n",
    "    print(f'[{time() - start_time}] Item description concatenated.')\n",
    "\n",
    "    merge.drop(['price', 'test_id', 'train_id'], axis=1, inplace=True)\n",
    "\n",
    "    return merge, y_train, nrow_train\n",
    "\n",
    "\n",
    "def intersect_drop_columns(train: csr_matrix, valid: csr_matrix, min_df=0):\n",
    "    t = train.tocsc()\n",
    "    v = valid.tocsc()\n",
    "    nnz_train = ((t != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_valid = ((v != 0).sum(axis=0) >= min_df).A1\n",
    "    nnz_cols = nnz_train & nnz_valid\n",
    "    res = t[:, nnz_cols], v[:, nnz_cols]\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mp.set_start_method('forkserver', True)\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    train = pd.read_table(os.path.join(INPUT_PATH, 'train.tsv'),\n",
    "                          engine='c',\n",
    "                          dtype={'item_condition_id': 'category',\n",
    "                                 'shipping': 'category'}\n",
    "                          )\n",
    "    test = pd.read_table(os.path.join(INPUT_PATH, 'test.tsv'),\n",
    "                         engine='c',\n",
    "                         dtype={'item_condition_id': 'category',\n",
    "                                'shipping': 'category'}\n",
    "                         )\n",
    "    print(f'[{time() - start_time}] Finished to load data')\n",
    "    print('Train shape: ', train.shape)\n",
    "    print('Test shape: ', test.shape)\n",
    "\n",
    "    submission: pd.DataFrame = test[['test_id']]\n",
    "\n",
    "    merge, y_train, nrow_train = preprocess_pandas(train, test, start_time)\n",
    "\n",
    "    meta_params = {'name_ngram': (1, 2),\n",
    "                   'name_max_f': 75000,\n",
    "                   'name_min_df': 10,\n",
    "\n",
    "                   'category_ngram': (2, 3),\n",
    "                   'category_token': '.+',\n",
    "                   'category_min_df': 10,\n",
    "\n",
    "                   'brand_min_df': 10,\n",
    "\n",
    "                   'desc_ngram': (1, 3),\n",
    "                   'desc_max_f': 150000,\n",
    "                   'desc_max_df': 0.5,\n",
    "                   'desc_min_df': 10}\n",
    "\n",
    "    stopwords = frozenset(['the', 'a', 'an', 'is', 'it', 'this', ])\n",
    "\n",
    "    vectorizer = FeatureUnion([\n",
    "        ('name', Pipeline([\n",
    "            ('select', ItemSelector('name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 2),\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('category_name', Pipeline([\n",
    "            ('select', ItemSelector('category_name', start_time=start_time)),\n",
    "            ('transform', HashingVectorizer(\n",
    "                ngram_range=(1, 1),\n",
    "                token_pattern='.+',\n",
    "                tokenizer=split_cat,\n",
    "                n_features=2 ** 27,\n",
    "                norm='l2',\n",
    "                lowercase=False\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2))\n",
    "        ])),\n",
    "        ('brand_name', Pipeline([\n",
    "            ('select', ItemSelector('brand_name', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('gencat_cond', Pipeline([\n",
    "            ('select', ItemSelector('gencat_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_1_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_1_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('subcat_2_cond', Pipeline([\n",
    "            ('select', ItemSelector('subcat_2_cond', start_time=start_time)),\n",
    "            ('transform', CountVectorizer(\n",
    "                token_pattern='.+',\n",
    "                min_df=2,\n",
    "                lowercase=False\n",
    "            )),\n",
    "        ])),\n",
    "        ('has_brand', Pipeline([\n",
    "            ('select', ItemSelector('has_brand', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('shipping', Pipeline([\n",
    "            ('select', ItemSelector('shipping', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_condition_id', Pipeline([\n",
    "            ('select', ItemSelector('item_condition_id', start_time=start_time)),\n",
    "            ('ohe', OneHotEncoder())\n",
    "        ])),\n",
    "        ('item_description', Pipeline([\n",
    "            ('select', ItemSelector('item_description', start_time=start_time)),\n",
    "            ('hash', HashingVectorizer(\n",
    "                ngram_range=(1, 3),\n",
    "                n_features=2 ** 27,\n",
    "                dtype=np.float32,\n",
    "                norm='l2',\n",
    "                lowercase=False,\n",
    "                stop_words=stopwords\n",
    "            )),\n",
    "            ('drop_cols', DropColumnsByDf(min_df=2)),\n",
    "        ]))\n",
    "    ], n_jobs=1)\n",
    "\n",
    "    sparse_merge = vectorizer.fit_transform(merge)\n",
    "    print(f'[{time() - start_time}] Merge vectorized')\n",
    "    print(sparse_merge.shape)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "    X = tfidf_transformer.fit_transform(sparse_merge)\n",
    "    print(f'[{time() - start_time}] TF/IDF completed')\n",
    "\n",
    "    X_train = X[:nrow_train]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    X_test = X[nrow_train:]\n",
    "    del merge\n",
    "    del sparse_merge\n",
    "    del vectorizer\n",
    "    del tfidf_transformer\n",
    "    gc.collect()\n",
    "\n",
    "    X_train, X_test = intersect_drop_columns(X_train, X_test, min_df=1)\n",
    "    print(f'[{time() - start_time}] Drop only in train or test cols: {X_train.shape[1]}')\n",
    "    gc.collect()\n",
    "\n",
    "    ridge = Ridge(solver='auto', fit_intercept=True, alpha=0.4, max_iter=200, normalize=False, tol=0.01)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(f'[{time() - start_time}] Train Ridge completed. Iterations: {ridge.n_iter_}')\n",
    "\n",
    "    predsR = ridge.predict(X_test)\n",
    "    print(f'[{time() - start_time}] Predict Ridge completed.')\n",
    "\n",
    "    submission.loc[:, 'price'] = np.expm1(predsR)\n",
    "    submission.loc[submission['price'] < 0.0, 'price'] = 0.0\n",
    "    submission.to_csv(\"submission_ridge.csv\", index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.116788864135742] Finished to load data\n",
      "Train shape:  (1482535, 8)\n",
      "Test shape:  (693359, 7)\n",
      "Train shape without zero price:  (1481661, 8)\n",
      "[7.510717868804932] Has_category filled.\n",
      "[11.00120759010315] Split categories completed.\n",
      "[11.069148778915405] Has_brand filled.\n",
      "[12.583812713623047] Categories and item_condition_id concancenated.\n",
      "[14.362658739089966] Missing filled.\n",
      "[14.989813804626465] Karats normalized.\n",
      "[15.658555746078491] Units glued.\n",
      "total words processed: 2671\n",
      "total unique words in corpus: 2671\n",
      "total items in dictionary (corpus words and deletions): 2671\n",
      "  edit distance for deletions: 0\n",
      "  length of longest word in corpus: 39\n",
      "total words processed: 2616\n",
      "total unique words in corpus: 2616\n",
      "total items in dictionary (corpus words and deletions): 2616\n",
      "  edit distance for deletions: 0\n",
      "  length of longest word in corpus: 15\n",
      "Before empty brand_name: 927861\n",
      "After empty brand_name: 252606\n",
      "[39.47184371948242] Brand name filled.\n",
      "[39.95408272743225] Name concancenated.\n",
      "[45.85782480239868] Item description concatenated.\n",
      "[48.185038805007935] select name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/9z2y6vy916ncn7tzc7rkg8140000gn/T/ipykernel_3006/1687496380.py:213: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dt):\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of HashingVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'an', 'is', 'this', 'it', 'the', 'a'}) instead.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidParameterError\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 511\u001B[0m\n\u001B[1;32m    427\u001B[0m stopwords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthe\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124man\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mit\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mthis\u001B[39m\u001B[38;5;124m'\u001B[39m, ])\n\u001B[1;32m    429\u001B[0m vectorizer \u001B[38;5;241m=\u001B[39m FeatureUnion([\n\u001B[1;32m    430\u001B[0m     (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, Pipeline([\n\u001B[1;32m    431\u001B[0m         (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mselect\u001B[39m\u001B[38;5;124m'\u001B[39m, ItemSelector(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, start_time\u001B[38;5;241m=\u001B[39mstart_time)),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    508\u001B[0m     ]))\n\u001B[1;32m    509\u001B[0m ], n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 511\u001B[0m sparse_merge \u001B[38;5;241m=\u001B[39m vectorizer\u001B[38;5;241m.\u001B[39mfit_transform(merge)\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtime()\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39mstart_time\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] Merge vectorized\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28mprint\u001B[39m(sparse_merge\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:313\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    312\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 313\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    314\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    315\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    316\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    317\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    319\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:1735\u001B[0m, in \u001B[0;36mFeatureUnion.fit_transform\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m   1732\u001B[0m             routed_params[name] \u001B[38;5;241m=\u001B[39m Bunch(transform\u001B[38;5;241m=\u001B[39m{})\n\u001B[1;32m   1733\u001B[0m             routed_params[name]\u001B[38;5;241m.\u001B[39mfit \u001B[38;5;241m=\u001B[39m params\n\u001B[0;32m-> 1735\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parallel_func(X, y, _fit_transform_one, routed_params)\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m results:\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;66;03m# All transformers are None\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39mzeros((X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m0\u001B[39m))\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:1757\u001B[0m, in \u001B[0;36mFeatureUnion._parallel_func\u001B[0;34m(self, X, y, func, routed_params)\u001B[0m\n\u001B[1;32m   1754\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_transformer_weights()\n\u001B[1;32m   1755\u001B[0m transformers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter())\n\u001B[0;32m-> 1757\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Parallel(n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs)(\n\u001B[1;32m   1758\u001B[0m     delayed(func)(\n\u001B[1;32m   1759\u001B[0m         transformer,\n\u001B[1;32m   1760\u001B[0m         X,\n\u001B[1;32m   1761\u001B[0m         y,\n\u001B[1;32m   1762\u001B[0m         weight,\n\u001B[1;32m   1763\u001B[0m         message_clsname\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeatureUnion\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1764\u001B[0m         message\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(name, idx, \u001B[38;5;28mlen\u001B[39m(transformers)),\n\u001B[1;32m   1765\u001B[0m         params\u001B[38;5;241m=\u001B[39mrouted_params[name],\n\u001B[1;32m   1766\u001B[0m     )\n\u001B[1;32m   1767\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m idx, (name, transformer, weight) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(transformers, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m   1768\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:74\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     69\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     70\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     71\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     73\u001B[0m )\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(iterable_with_config)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1085\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1076\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1077\u001B[0m     \u001B[38;5;66;03m# Only set self._iterating to True if at least a batch\u001B[39;00m\n\u001B[1;32m   1078\u001B[0m     \u001B[38;5;66;03m# was dispatched. In particular this covers the edge\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1082\u001B[0m     \u001B[38;5;66;03m# was very quick and its callback already dispatched all the\u001B[39;00m\n\u001B[1;32m   1083\u001B[0m     \u001B[38;5;66;03m# remaining jobs.\u001B[39;00m\n\u001B[1;32m   1084\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m-> 1085\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n\u001B[1;32m   1086\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_original_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1088\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdispatch_one_batch(iterator):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:901\u001B[0m, in \u001B[0;36mParallel.dispatch_one_batch\u001B[0;34m(self, iterator)\u001B[0m\n\u001B[1;32m    899\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    900\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 901\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dispatch(tasks)\n\u001B[1;32m    902\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:819\u001B[0m, in \u001B[0;36mParallel._dispatch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    818\u001B[0m     job_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs)\n\u001B[0;32m--> 819\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mapply_async(batch, callback\u001B[38;5;241m=\u001B[39mcb)\n\u001B[1;32m    820\u001B[0m     \u001B[38;5;66;03m# A job can complete so quickly than its callback is\u001B[39;00m\n\u001B[1;32m    821\u001B[0m     \u001B[38;5;66;03m# called before we get here, causing self._jobs to\u001B[39;00m\n\u001B[1;32m    822\u001B[0m     \u001B[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001B[39;00m\n\u001B[1;32m    823\u001B[0m     \u001B[38;5;66;03m# used (rather than .append) in the following line\u001B[39;00m\n\u001B[1;32m    824\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs\u001B[38;5;241m.\u001B[39minsert(job_idx, job)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001B[0m, in \u001B[0;36mSequentialBackend.apply_async\u001B[0;34m(self, func, callback)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_async\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, callback\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    207\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001B[39;00m\n\u001B[0;32m--> 208\u001B[0m     result \u001B[38;5;241m=\u001B[39m ImmediateResult(func)\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m callback:\n\u001B[1;32m    210\u001B[0m         callback(result)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001B[0m, in \u001B[0;36mImmediateResult.__init__\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m    594\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[1;32m    595\u001B[0m     \u001B[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001B[39;00m\n\u001B[1;32m    596\u001B[0m     \u001B[38;5;66;03m# arguments in memory\u001B[39;00m\n\u001B[0;32m--> 597\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresults \u001B[38;5;241m=\u001B[39m batch()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001B[0m, in \u001B[0;36mBatchedCalls.__call__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:288\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001B[39;00m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;66;03m# change the default number of processes to -1\u001B[39;00m\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m parallel_backend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_jobs):\n\u001B[0;32m--> 288\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m [func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    289\u001B[0m                 \u001B[38;5;28;01mfor\u001B[39;00m func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitems]\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:136\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    134\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:1310\u001B[0m, in \u001B[0;36m_fit_transform_one\u001B[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001B[0m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[1;32m   1309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(transformer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1310\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit_transform(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\n\u001B[1;32m   1311\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1312\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[1;32m   1313\u001B[0m             X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n\u001B[1;32m   1314\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[0;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:533\u001B[0m, in \u001B[0;36mPipeline.fit_transform\u001B[0;34m(self, X, y, **params)\u001B[0m\n\u001B[1;32m    490\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Fit the model and transform with the final estimator.\u001B[39;00m\n\u001B[1;32m    491\u001B[0m \n\u001B[1;32m    492\u001B[0m \u001B[38;5;124;03mFit all the transformers one after the other and sequentially transform\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    530\u001B[0m \u001B[38;5;124;03m    Transformed samples.\u001B[39;00m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    532\u001B[0m routed_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_method_params(method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m, props\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m--> 533\u001B[0m Xt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit(X, y, routed_params)\n\u001B[1;32m    535\u001B[0m last_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_final_estimator\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:406\u001B[0m, in \u001B[0;36mPipeline._fit\u001B[0;34m(self, X, y, routed_params)\u001B[0m\n\u001B[1;32m    404\u001B[0m     cloned_transformer \u001B[38;5;241m=\u001B[39m clone(transformer)\n\u001B[1;32m    405\u001B[0m \u001B[38;5;66;03m# Fit or load from cache the current transformer\u001B[39;00m\n\u001B[0;32m--> 406\u001B[0m X, fitted_transformer \u001B[38;5;241m=\u001B[39m fit_transform_one_cached(\n\u001B[1;32m    407\u001B[0m     cloned_transformer,\n\u001B[1;32m    408\u001B[0m     X,\n\u001B[1;32m    409\u001B[0m     y,\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    411\u001B[0m     message_clsname\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    412\u001B[0m     message\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_log_message(step_idx),\n\u001B[1;32m    413\u001B[0m     params\u001B[38;5;241m=\u001B[39mrouted_params[name],\n\u001B[1;32m    414\u001B[0m )\n\u001B[1;32m    415\u001B[0m \u001B[38;5;66;03m# Replace the transformer of the step with the fitted\u001B[39;00m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;66;03m# transformer. This is necessary when loading the transformer\u001B[39;00m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;66;03m# from the cache.\u001B[39;00m\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps[step_idx] \u001B[38;5;241m=\u001B[39m (name, fitted_transformer)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/joblib/memory.py:349\u001B[0m, in \u001B[0;36mNotMemorizedFunc.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 349\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/pipeline.py:1310\u001B[0m, in \u001B[0;36m_fit_transform_one\u001B[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001B[0m\n\u001B[1;32m   1308\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _print_elapsed_time(message_clsname, message):\n\u001B[1;32m   1309\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(transformer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1310\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit_transform(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit_transform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\n\u001B[1;32m   1311\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1312\u001B[0m         res \u001B[38;5;241m=\u001B[39m transformer\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfit\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))\u001B[38;5;241m.\u001B[39mtransform(\n\u001B[1;32m   1313\u001B[0m             X, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransform\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n\u001B[1;32m   1314\u001B[0m         )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:907\u001B[0m, in \u001B[0;36mHashingVectorizer.fit_transform\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    889\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    890\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Transform a sequence of documents to a document-term matrix.\u001B[39;00m\n\u001B[1;32m    891\u001B[0m \n\u001B[1;32m    892\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    905\u001B[0m \u001B[38;5;124;03m        Document-term matrix.\u001B[39;00m\n\u001B[1;32m    906\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 907\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1466\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1461\u001B[0m partial_fit_and_fitted \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1462\u001B[0m     fit_method\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpartial_fit\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m _is_fitted(estimator)\n\u001B[1;32m   1463\u001B[0m )\n\u001B[1;32m   1465\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m global_skip_validation \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m partial_fit_and_fitted:\n\u001B[0;32m-> 1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1471\u001B[0m     )\n\u001B[1;32m   1472\u001B[0m ):\n\u001B[1;32m   1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:666\u001B[0m, in \u001B[0;36mBaseEstimator._validate_params\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_params\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    659\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001B[39;00m\n\u001B[1;32m    660\u001B[0m \n\u001B[1;32m    661\u001B[0m \u001B[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    664\u001B[0m \u001B[38;5;124;03m    accepted constraints.\u001B[39;00m\n\u001B[1;32m    665\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 666\u001B[0m     validate_parameter_constraints(\n\u001B[1;32m    667\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parameter_constraints,\n\u001B[1;32m    668\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_params(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m),\n\u001B[1;32m    669\u001B[0m         caller_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    670\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:95\u001B[0m, in \u001B[0;36mvalidate_parameter_constraints\u001B[0;34m(parameter_constraints, params, caller_name)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     90\u001B[0m     constraints_str \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     91\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;28mstr\u001B[39m(c)\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39mconstraints[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m or\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     92\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     93\u001B[0m     )\n\u001B[0;32m---> 95\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m InvalidParameterError(\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_name\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m parameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcaller_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconstraints_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam_val\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     98\u001B[0m )\n",
      "\u001B[0;31mInvalidParameterError\u001B[0m: The 'stop_words' parameter of HashingVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'an', 'is', 'this', 'it', 'the', 'a'}) instead."
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
