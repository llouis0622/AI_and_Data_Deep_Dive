{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-18T08:01:12.266007Z",
     "start_time": "2024-07-18T08:01:06.491007Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, PReLU, GaussianDropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from matplotlib import rc\n",
    "from matplotlib import rc\n",
    "\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "FUDGE_FACTOR = 1.1200\n",
    "XGB_WEIGHT = 0.6200\n",
    "BASELINE_WEIGHT = 0.0100\n",
    "OLS_WEIGHT = 0.0620\n",
    "NN_WEIGHT = 0.0800\n",
    "XGB1_WEIGHT = 0.8000\n",
    "BASELINE_PRED = 0.0115"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T08:06:58.158050Z",
     "start_time": "2024-07-18T08:06:43.918894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Reading data from disk ...\")\n",
    "prop = pd.read_csv('../data/zilow/properties_2016.csv')\n",
    "train = pd.read_csv(\"../data/zilow/train_2016_v2.csv\")\n",
    "\n",
    "print(\"\\nProcessing data for LightGBM ...\")\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "df_train.fillna(df_train.median(), inplace=True)\n",
    "\n",
    "x_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc',\n",
    "                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "\n",
    "params = {}\n",
    "params['max_bin'] = 10\n",
    "params['learning_rate'] = 0.0021\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'regression'\n",
    "params['metric'] = 'l1'\n",
    "params['sub_feature'] = 0.345\n",
    "params['bagging_fraction'] = 0.85\n",
    "params['bagging_freq'] = 40\n",
    "params['num_leaves'] = 512\n",
    "params['min_data'] = 500\n",
    "params['min_hessian'] = 0.05\n",
    "params['verbose'] = 0\n",
    "params['feature_fraction_seed'] = 2\n",
    "params['bagging_seed'] = 3\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "print(\"\\nFitting LightGBM model ...\")\n",
    "clf = lgb.train(params, d_train, 430)\n",
    "\n",
    "del d_train\n",
    "gc.collect()\n",
    "del x_train\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nPrepare for LightGBM prediction ...\")\n",
    "print(\"   Read sample file ...\")\n",
    "sample = pd.read_csv('../data/zilow/sample_submission.csv')\n",
    "print(\"   ...\")\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "print(\"   Merge with property data ...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "print(\"   ...\")\n",
    "del sample, prop\n",
    "gc.collect()\n",
    "print(\"   ...\")\n",
    "x_test = df_test[train_columns]\n",
    "print(\"   ...\")\n",
    "del df_test\n",
    "gc.collect()\n",
    "print(\"   Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "print(\"   ...\")\n",
    "x_test = x_test.values.astype(np.float32, copy=False)\n",
    "\n",
    "print(\"\\nStart LightGBM prediction ...\")\n",
    "p_test = clf.predict(x_test)\n",
    "\n",
    "del x_test\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nUnadjusted LightGBM predictions:\")\n",
    "print(pd.DataFrame(p_test).head())\n",
    "\n",
    "print(\"\\nRe-reading properties file ...\")\n",
    "properties = pd.read_csv('../data/zilow/properties_2016.csv')\n",
    "\n",
    "print(\"\\nProcessing data for XGBoost ...\")\n",
    "for c in properties.columns:\n",
    "    properties[c] = properties[c].fillna(-1)\n",
    "    if properties[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(properties[c].values))\n",
    "        properties[c] = lbl.transform(list(properties[c].values))\n",
    "\n",
    "train_df = train.merge(properties, how='left', on='parcelid')\n",
    "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "x_test = properties.drop(['parcelid'], axis=1)\n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "train_df = train_df[train_df.logerror > -0.4]\n",
    "train_df = train_df[train_df.logerror < 0.419]\n",
    "x_train = train_df.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "y_train = train_df[\"logerror\"].values.astype(np.float32)\n",
    "y_mean = np.mean(y_train)\n",
    "\n",
    "print('After removing outliers:')\n",
    "print('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,\n",
    "    'alpha': 0.4,\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test)\n",
    "\n",
    "num_boost_rounds = 250\n",
    "print(\"num_boost_rounds=\" + str(num_boost_rounds))\n",
    "\n",
    "print(\"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print(\"\\nPredicting with XGBoost ...\")\n",
    "xgb_pred1 = model.predict(dtest)\n",
    "\n",
    "print(\"\\nFirst XGBoost predictions:\")\n",
    "print(pd.DataFrame(xgb_pred1).head())\n",
    "\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "xgb_params = {\n",
    "    'eta': 0.033,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "num_boost_rounds = 150\n",
    "print(\"num_boost_rounds=\" + str(num_boost_rounds))\n",
    "\n",
    "print(\"\\nTraining XGBoost again ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print(\"\\nPredicting with XGBoost again ...\")\n",
    "xgb_pred2 = model.predict(dtest)\n",
    "\n",
    "print(\"\\nSecond XGBoost predictions:\")\n",
    "print(pd.DataFrame(xgb_pred2).head())\n",
    "\n",
    "xgb_pred = XGB1_WEIGHT * xgb_pred1 + (1 - XGB1_WEIGHT) * xgb_pred2\n",
    "\n",
    "print(\"\\nCombined XGBoost predictions:\")\n",
    "print(pd.DataFrame(xgb_pred).head())\n",
    "\n",
    "del train_df\n",
    "del x_train\n",
    "del x_test\n",
    "del properties\n",
    "del dtest\n",
    "del dtrain\n",
    "del xgb_pred1\n",
    "del xgb_pred2\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\\nProcessing data for Neural Network ...\")\n",
    "print('\\nLoading train, prop and sample data...')\n",
    "train = pd.read_csv(\"../data/zilow/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop = pd.read_csv('../data/zilow/properties_2016.csv')\n",
    "sample = pd.read_csv('../data/zilow/sample_submission.csv')\n",
    "\n",
    "print('Fitting Label Encoder on properties...')\n",
    "for c in prop.columns:\n",
    "    prop[c] = prop[c].fillna(-1)\n",
    "    if prop[c].dtype == 'object':\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(prop[c].values))\n",
    "        prop[c] = lbl.transform(list(prop[c].values))\n",
    "\n",
    "print('Creating training set...')\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "df_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\n",
    "df_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\n",
    "df_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\n",
    "df_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\n",
    "df_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day\n",
    "\n",
    "print('Filling NA/NaN values...')\n",
    "df_train.fillna(-1.0)\n",
    "\n",
    "print('Creating x_train and y_train from df_train...')\n",
    "x_train = df_train.drop(\n",
    "    ['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode', 'fireplacecnt',\n",
    "     'fireplaceflag'], axis=1)\n",
    "y_train = df_train[\"logerror\"]\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "print(x_train.shape, y_train.shape)\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "print('Creating df_test...')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "\n",
    "print(\"Merging Sample with property data...\")\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "df_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')\n",
    "df_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\n",
    "df_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\n",
    "df_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\n",
    "df_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day\n",
    "x_test = df_test[train_columns]\n",
    "\n",
    "print('Shape of x_test:', x_test.shape)\n",
    "print(\"Preparing x_test...\")\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "\n",
    "print(\"\\nPreprocessing neural network data...\")\n",
    "imputer = SimpleImputer()\n",
    "imputer.fit(x_train.iloc[:, :])\n",
    "x_train = imputer.transform(x_train.iloc[:, :])\n",
    "imputer.fit(x_test.iloc[:, :])\n",
    "x_test = imputer.transform(x_test.iloc[:, :])\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "len_x = int(x_train.shape[1])\n",
    "print(\"len_x is:\", len_x)\n",
    "\n",
    "print(\"\\nSetting up neural network model...\")\n",
    "nn = Sequential()\n",
    "nn.add(Dense(units=400, kernel_initializer='normal', input_dim=len_x))\n",
    "nn.add(PReLU())\n",
    "nn.add(Dropout(.4))\n",
    "nn.add(Dense(units=160, kernel_initializer='normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(units=64, kernel_initializer='normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.5))\n",
    "nn.add(Dense(units=26, kernel_initializer='normal'))\n",
    "nn.add(PReLU())\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dropout(.6))\n",
    "nn.add(Dense(1, kernel_initializer='normal'))\n",
    "nn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))\n",
    "\n",
    "print(\"\\nFitting neural network model...\")\n",
    "nn.fit(np.array(x_train), np.array(y_train), batch_size=32, epochs=70, verbose=2)\n",
    "\n",
    "print(\"\\nPredicting with neural network model...\")\n",
    "y_pred_ann = nn.predict(x_test)\n",
    "\n",
    "print(\"\\nPreparing results for write...\")\n",
    "nn_pred = y_pred_ann.flatten()\n",
    "print(\"Type of nn_pred is \", type(nn_pred))\n",
    "print(\"Shape of nn_pred is \", nn_pred.shape)\n",
    "\n",
    "print(\"\\nNeural Network predictions:\")\n",
    "print(pd.DataFrame(nn_pred).head())\n",
    "\n",
    "del train\n",
    "del prop\n",
    "del sample\n",
    "del x_train\n",
    "del x_test\n",
    "del df_train\n",
    "del df_test\n",
    "del y_pred_ann\n",
    "gc.collect()\n",
    "\n",
    "np.random.seed(17)\n",
    "random.seed(17)\n",
    "\n",
    "print(\"\\n\\nProcessing data for OLS ...\")\n",
    "\n",
    "train = pd.read_csv(\"../data/zilow/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "properties = pd.read_csv(\"../data/zilow/properties_2016.csv\")\n",
    "submission = pd.read_csv(\"../data/zilow/sample_submission.csv\")\n",
    "print(len(train), len(properties), len(submission))\n",
    "\n",
    "\n",
    "def get_features(df):\n",
    "    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n",
    "    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df['transactiondate'] = df['transactiondate'].dt.quarter\n",
    "    df = df.fillna(-1.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def MAE(y, ypred):\n",
    "    return np.sum([abs(y[i] - ypred[i]) for i in range(len(y))]) / len(y)\n",
    "\n",
    "\n",
    "train = pd.merge(train, properties, how='left', on='parcelid')\n",
    "y = train['logerror'].values\n",
    "test = pd.merge(submission, properties, how='left', left_on='ParcelId', right_on='parcelid')\n",
    "properties = []\n",
    "\n",
    "exc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror', 'parcelid']\n",
    "col = [c for c in train.columns if c not in exc]\n",
    "\n",
    "train = get_features(train[col])\n",
    "test['transactiondate'] = '2016-01-01'\n",
    "test = get_features(test[col])\n",
    "\n",
    "print(\"\\nFitting OLS...\")\n",
    "reg = LinearRegression(n_jobs=-1)\n",
    "reg.fit(train, y)\n",
    "print('fit...')\n",
    "print(MAE(y, reg.predict(train)))\n",
    "train = []\n",
    "y = []\n",
    "\n",
    "test_dates = ['2016-10-01', '2016-11-01', '2016-12-01', '2017-10-01', '2017-11-01', '2017-12-01']\n",
    "test_columns = ['201610', '201611', '201612', '201710', '201711', '201712']\n",
    "\n",
    "print(\"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\")\n",
    "lgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT\n",
    "lgb_weight0 = lgb_weight / (1 - OLS_WEIGHT)\n",
    "xgb_weight0 = XGB_WEIGHT / (1 - OLS_WEIGHT)\n",
    "baseline_weight0 = BASELINE_WEIGHT / (1 - OLS_WEIGHT)\n",
    "nn_weight0 = NN_WEIGHT / (1 - OLS_WEIGHT)\n",
    "pred0 = 0\n",
    "pred0 += xgb_weight0 * xgb_pred\n",
    "pred0 += baseline_weight0 * BASELINE_PRED\n",
    "pred0 += lgb_weight0 * p_test\n",
    "pred0 += nn_weight0 * nn_pred\n",
    "\n",
    "print(\"\\nCombined XGB/LGB/NN/baseline predictions:\")\n",
    "print(pd.DataFrame(pred0).head())\n",
    "\n",
    "print(\"\\nPredicting with OLS and combining with XGB/LGB/NN/baseline predicitons: ...\")\n",
    "for i in range(len(test_dates)):\n",
    "    test['transactiondate'] = test_dates[i]\n",
    "    pred = FUDGE_FACTOR * (OLS_WEIGHT * reg.predict(get_features(test)) + (1 - OLS_WEIGHT) * pred0)\n",
    "    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n",
    "    print('predict...', i)\n",
    "\n",
    "print(\"\\nCombined XGB/LGB/NN/baseline/OLS predictions:\")\n",
    "print(submission.head())\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\nWriting results to disk ...\")\n",
    "submission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n",
    "\n",
    "print(\"\\nFinished ...\")"
   ],
   "id": "e76dcf36274c53d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from disk ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3g/9z2y6vy916ncn7tzc7rkg8140000gn/T/ipykernel_15531/1072462269.py:2: DtypeWarning: Columns (22,32,34,49,55) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  prop = pd.read_csv('../data/zilow/properties_2016.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for LightGBM ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert [['2016-01-01' '2016-01-01' '2016-01-01' ... '2016-12-30' '2016-12-30'\n  '2016-12-30']] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m         prop[c] \u001B[38;5;241m=\u001B[39m prop[c]\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     10\u001B[0m df_train \u001B[38;5;241m=\u001B[39m train\u001B[38;5;241m.\u001B[39mmerge(prop, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m'\u001B[39m, on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparcelid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m df_train\u001B[38;5;241m.\u001B[39mfillna(df_train\u001B[38;5;241m.\u001B[39mmedian(), inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     13\u001B[0m x_train \u001B[38;5;241m=\u001B[39m df_train\u001B[38;5;241m.\u001B[39mdrop([\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparcelid\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogerror\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtransactiondate\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpropertyzoningdesc\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     14\u001B[0m                          \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpropertycountylandusecode\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfireplacecnt\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfireplaceflag\u001B[39m\u001B[38;5;124m'\u001B[39m], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     15\u001B[0m y_train \u001B[38;5;241m=\u001B[39m df_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlogerror\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11706\u001B[0m, in \u001B[0;36mDataFrame.median\u001B[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  11698\u001B[0m \u001B[38;5;129m@doc\u001B[39m(make_doc(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedian\u001B[39m\u001B[38;5;124m\"\u001B[39m, ndim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m))\n\u001B[1;32m  11699\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmedian\u001B[39m(\n\u001B[1;32m  11700\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  11704\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  11705\u001B[0m ):\n\u001B[0;32m> 11706\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mmedian(axis, skipna, numeric_only, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m  11707\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, Series):\n\u001B[1;32m  11708\u001B[0m         result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedian\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:12431\u001B[0m, in \u001B[0;36mNDFrame.median\u001B[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  12424\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmedian\u001B[39m(\n\u001B[1;32m  12425\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m  12426\u001B[0m     axis: Axis \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  12429\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m  12430\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Series \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mfloat\u001B[39m:\n\u001B[0;32m> 12431\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stat_function(\n\u001B[1;32m  12432\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmedian\u001B[39m\u001B[38;5;124m\"\u001B[39m, nanops\u001B[38;5;241m.\u001B[39mnanmedian, axis, skipna, numeric_only, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m  12433\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:12377\u001B[0m, in \u001B[0;36mNDFrame._stat_function\u001B[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001B[0m\n\u001B[1;32m  12373\u001B[0m nv\u001B[38;5;241m.\u001B[39mvalidate_func(name, (), kwargs)\n\u001B[1;32m  12375\u001B[0m validate_bool_kwarg(skipna, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskipna\u001B[39m\u001B[38;5;124m\"\u001B[39m, none_allowed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m> 12377\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reduce(\n\u001B[1;32m  12378\u001B[0m     func, name\u001B[38;5;241m=\u001B[39mname, axis\u001B[38;5;241m=\u001B[39maxis, skipna\u001B[38;5;241m=\u001B[39mskipna, numeric_only\u001B[38;5;241m=\u001B[39mnumeric_only\n\u001B[1;32m  12379\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11562\u001B[0m, in \u001B[0;36mDataFrame._reduce\u001B[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001B[0m\n\u001B[1;32m  11558\u001B[0m     df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mT\n\u001B[1;32m  11560\u001B[0m \u001B[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001B[39;00m\n\u001B[1;32m  11561\u001B[0m \u001B[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001B[39;00m\n\u001B[0;32m> 11562\u001B[0m res \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mreduce(blk_func)\n\u001B[1;32m  11563\u001B[0m out \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39m_constructor_from_mgr(res, axes\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39maxes)\u001B[38;5;241m.\u001B[39miloc[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m  11564\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m out_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m out\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboolean\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1500\u001B[0m, in \u001B[0;36mBlockManager.reduce\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m   1498\u001B[0m res_blocks: \u001B[38;5;28mlist\u001B[39m[Block] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m-> 1500\u001B[0m     nbs \u001B[38;5;241m=\u001B[39m blk\u001B[38;5;241m.\u001B[39mreduce(func)\n\u001B[1;32m   1501\u001B[0m     res_blocks\u001B[38;5;241m.\u001B[39mextend(nbs)\n\u001B[1;32m   1503\u001B[0m index \u001B[38;5;241m=\u001B[39m Index([\u001B[38;5;28;01mNone\u001B[39;00m])  \u001B[38;5;66;03m# placeholder\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:404\u001B[0m, in \u001B[0;36mBlock.reduce\u001B[0;34m(self, func)\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreduce\u001B[39m(\u001B[38;5;28mself\u001B[39m, func) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[Block]:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001B[39;00m\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001B[39;00m\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m--> 404\u001B[0m     result \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues)\n\u001B[1;32m    406\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    407\u001B[0m         res_values \u001B[38;5;241m=\u001B[39m result\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11481\u001B[0m, in \u001B[0;36mDataFrame._reduce.<locals>.blk_func\u001B[0;34m(values, axis)\u001B[0m\n\u001B[1;32m  11479\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([result])\n\u001B[1;32m  11480\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m> 11481\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m op(values, axis\u001B[38;5;241m=\u001B[39maxis, skipna\u001B[38;5;241m=\u001B[39mskipna, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:147\u001B[0m, in \u001B[0;36mbottleneck_switch.__call__.<locals>.f\u001B[0;34m(values, axis, skipna, **kwds)\u001B[0m\n\u001B[1;32m    145\u001B[0m         result \u001B[38;5;241m=\u001B[39m alt(values, axis\u001B[38;5;241m=\u001B[39maxis, skipna\u001B[38;5;241m=\u001B[39mskipna, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 147\u001B[0m     result \u001B[38;5;241m=\u001B[39m alt(values, axis\u001B[38;5;241m=\u001B[39maxis, skipna\u001B[38;5;241m=\u001B[39mskipna, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:787\u001B[0m, in \u001B[0;36mnanmedian\u001B[0;34m(values, axis, skipna, mask)\u001B[0m\n\u001B[1;32m    785\u001B[0m     inferred \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39minfer_dtype(values)\n\u001B[1;32m    786\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inferred \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstring\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmixed\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 787\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot convert \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvalues\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to numeric\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    788\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    789\u001B[0m     values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mTypeError\u001B[0m: Cannot convert [['2016-01-01' '2016-01-01' '2016-01-01' ... '2016-12-30' '2016-12-30'\n  '2016-12-30']] to numeric"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
