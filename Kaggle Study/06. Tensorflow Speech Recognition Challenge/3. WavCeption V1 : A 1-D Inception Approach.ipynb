{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-17T05:57:52.304254Z",
     "start_time": "2024-07-17T05:57:47.399425Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import IPython\n",
    "from numpy.fft import rfft, irfft\n",
    "import random\n",
    "import itertools\n",
    "from scipy.io import wavfile\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "import tensorflow as tf\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "rc('font', family='AppleGothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T05:59:18.752185Z",
     "start_time": "2024-07-17T05:59:18.745467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ms(x):\n",
    "    return (np.abs(x) ** 2.0).mean()\n",
    "\n",
    "\n",
    "def normalize(y, x=None):\n",
    "    if x is not None:\n",
    "        x = ms(x)\n",
    "    else:\n",
    "        x = 1.0\n",
    "    return y * np.sqrt(x / ms(y))\n",
    "\n",
    "\n",
    "def white_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    return state.randn(N)\n",
    "\n",
    "\n",
    "def pink_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)) + 1.)\n",
    "    y = (irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "\n",
    "def blue_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = np.sqrt(np.arange(len(X)))\n",
    "    y = (irfft(X * S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "\n",
    "def brown_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = (np.arange(len(X)) + 1)\n",
    "    y = (irfft(X / S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)\n",
    "\n",
    "\n",
    "def violet_noise(N, state=None):\n",
    "    state = np.random.RandomState() if state is None else state\n",
    "    uneven = N % 2\n",
    "    X = state.randn(N // 2 + 1 + uneven) + 1j * state.randn(N // 2 + 1 + uneven)\n",
    "    S = (np.arange(len(X)))\n",
    "    y = (irfft(X * S)).real\n",
    "    if uneven:\n",
    "        y = y[:-1]\n",
    "    return normalize(y)"
   ],
   "id": "a3442b54d1659d77",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T05:59:52.449796Z",
     "start_time": "2024-07-17T05:59:52.445926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_tensorflow_configuration(device=\"0\", memory_fraction=1):\n",
    "    device = str(device)\n",
    "    config = tf.ConfigProto()\n",
    "    config.allow_soft_placement = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = memory_fraction\n",
    "    config.gpu_options.visible_device_list = device\n",
    "    return (config)\n",
    "\n",
    "\n",
    "def start_tensorflow_session(device=\"0\", memory_fraction=1):\n",
    "    return (tf.Session(config=get_tensorflow_configuration(device=device, memory_fraction=memory_fraction)))\n",
    "\n",
    "\n",
    "def get_summary_writer(session, logs_path, project_id, version_id):\n",
    "    path = os.path.join(logs_path, \"{}_{}\".format(project_id, version_id))\n",
    "    if os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    summary_writer = tf.summary.FileWriter(path, graph_def=session.graph_def)\n",
    "    return (summary_writer)"
   ],
   "id": "c575698a058d8506",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:01:11.528524Z",
     "start_time": "2024-07-17T06:01:11.523036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _norm_path(path):\n",
    "    def normalize_path(*args, **kwargs):\n",
    "        return os.path.normpath(path(*args, **kwargs))\n",
    "\n",
    "    return normalize_path\n",
    "\n",
    "\n",
    "def _assure_path_exists(path):\n",
    "    def assure_exists(*args, **kwargs):\n",
    "        p = path(*args, **kwargs)\n",
    "        assert os.path.exists(p), \"the following path does not exist: '{}'\".format(p)\n",
    "        return p\n",
    "\n",
    "    return assure_exists\n",
    "\n",
    "\n",
    "def _is_output_path(path):\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence_or_create_it(*args, **kwargs):\n",
    "        if not os.path.exists(path(*args, **kwargs)):\n",
    "            \"Path does not exist... creating it: {}\".format(path(*args, **kwargs))\n",
    "            os.makedirs(path(*args, **kwargs))\n",
    "        return path(*args, **kwargs)\n",
    "\n",
    "    return check_existence_or_create_it\n",
    "\n",
    "\n",
    "def _is_input_path(path):\n",
    "    @_norm_path\n",
    "    @_assure_path_exists\n",
    "    def check_existence(*args, **kwargs):\n",
    "        return path(*args, **kwargs)\n",
    "\n",
    "    return check_existence\n",
    "\n",
    "\n",
    "@_is_input_path\n",
    "def get_train_path():\n",
    "    path = \"../data/speech/train\"\n",
    "    return path\n",
    "\n",
    "\n",
    "@_is_input_path\n",
    "def get_test_path():\n",
    "    path = \"../data/speech/test\"\n",
    "    return path\n",
    "\n",
    "\n",
    "@_is_input_path\n",
    "def get_train_audio_path():\n",
    "    path = os.path.join(get_train_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "\n",
    "@_is_input_path\n",
    "def get_scoring_audio_path():\n",
    "    path = os.path.join(get_test_path(), \"audio\")\n",
    "    return path\n",
    "\n",
    "\n",
    "@_is_output_path\n",
    "def get_submissions_path():\n",
    "    path = \"../working/output\"\n",
    "    return path\n",
    "\n",
    "\n",
    "@_is_output_path\n",
    "def get_silence_path():\n",
    "    path = \"../working/silence\"\n",
    "    return path"
   ],
   "id": "513de0fa36ecf6f6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:01:14.182842Z",
     "start_time": "2024-07-17T06:01:14.179857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "\n",
    "def batching(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ],
   "id": "243670a0c1545398",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:01:40.710506Z",
     "start_time": "2024-07-17T06:01:40.706042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_wav(filepath, pad=True):\n",
    "    sample_rate, x = wavfile.read(filepath)\n",
    "    target = os.path.split(os.path.split(filepath)[0])[1]\n",
    "    assert sample_rate == 16000\n",
    "    if pad:\n",
    "        return np.pad(x, (0, 16000 - len(x)), mode=\"constant\") / 32768, target\n",
    "    else:\n",
    "        return x / 32768, target\n",
    "\n",
    "\n",
    "def get_batcher(list_of_paths, batch_size, label_encoder=None, scoring=False):\n",
    "    for filepaths in batching(list_of_paths, batch_size):\n",
    "        wavs, targets = zip(*list(map(read_wav, filepaths)))\n",
    "        if scoring:\n",
    "            yield np.expand_dims(np.row_stack(wavs), 2), filepaths\n",
    "        else:\n",
    "            if label_encoder is None:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.row_stack(targets)\n",
    "            else:\n",
    "                yield np.expand_dims(np.row_stack(wavs), 2), np.expand_dims(\n",
    "                    label_encoder.transform(np.squeeze(targets)), 1)"
   ],
   "id": "80a63bbe326f383b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:02:59.604290Z",
     "start_time": "2024-07-17T06:02:59.594435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BatchNorm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.999, name=\"batch_norm\"):\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "\n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x,\n",
    "                                            decay=self.momentum,\n",
    "                                            updates_collections=None,\n",
    "                                            epsilon=self.epsilon,\n",
    "                                            scale=True,\n",
    "                                            is_training=train,\n",
    "                                            scope=self.name)\n",
    "\n",
    "\n",
    "def inception_1d(x, is_train, depth, norm_function, activ_function, name):\n",
    "    with tf.variable_scope(name):\n",
    "        x_norm = norm_function(name=\"norm_input\")(x, train=is_train)\n",
    "\n",
    "        branch_conv_1_1 = tf.layers.conv1d(inputs=x_norm, filters=16 * depth, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_1_1\")\n",
    "        branch_conv_1_1 = norm_function(name=\"norm_conv_1_1\")(branch_conv_1_1, train=is_train)\n",
    "        branch_conv_1_1 = activ_function(branch_conv_1_1, \"activation_1_1\")\n",
    "\n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_1\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_1\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_1\")\n",
    "\n",
    "        branch_conv_3_3 = tf.layers.conv1d(inputs=branch_conv_3_3, filters=32 * depth, kernel_size=3,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_3_3_2\")\n",
    "        branch_conv_3_3 = norm_function(name=\"norm_conv_3_3_2\")(branch_conv_3_3, train=is_train)\n",
    "        branch_conv_3_3 = activ_function(branch_conv_3_3, \"activation_3_3_2\")\n",
    "\n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_1\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_1\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_1\")\n",
    "\n",
    "        branch_conv_5_5 = tf.layers.conv1d(inputs=branch_conv_5_5, filters=32 * depth, kernel_size=5,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_5_5_2\")\n",
    "        branch_conv_5_5 = norm_function(name=\"norm_conv_5_5_2\")(branch_conv_5_5, train=is_train)\n",
    "        branch_conv_5_5 = activ_function(branch_conv_5_5, \"activation_5_5_2\")\n",
    "\n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=x_norm, filters=16, kernel_size=1,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_1\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_1\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_1\")\n",
    "\n",
    "        branch_conv_7_7 = tf.layers.conv1d(inputs=branch_conv_7_7, filters=32 * depth, kernel_size=5,\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                           padding=\"same\", name=\"conv_7_7_2\")\n",
    "        branch_conv_7_7 = norm_function(name=\"norm_conv_7_7_2\")(branch_conv_7_7, train=is_train)\n",
    "        branch_conv_7_7 = activ_function(branch_conv_7_7, \"activation_7_7_2\")\n",
    "\n",
    "        branch_maxpool_3_3 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\",\n",
    "                                                     name=\"maxpool_3\")\n",
    "        branch_maxpool_3_3 = norm_function(name=\"norm_maxpool_3_3\")(branch_maxpool_3_3, train=is_train)\n",
    "        branch_maxpool_3_3 = tf.layers.conv1d(inputs=branch_maxpool_3_3, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_3\")\n",
    "\n",
    "        branch_maxpool_5_5 = tf.layers.max_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\",\n",
    "                                                     name=\"maxpool_5\")\n",
    "        branch_maxpool_5_5 = norm_function(name=\"norm_maxpool_5_5\")(branch_maxpool_5_5, train=is_train)\n",
    "        branch_maxpool_5_5 = tf.layers.conv1d(inputs=branch_maxpool_5_5, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_maxpool_5\")\n",
    "\n",
    "        branch_avgpool_3_3 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=3, strides=1, padding=\"same\",\n",
    "                                                         name=\"avgpool_3\")\n",
    "        branch_avgpool_3_3 = norm_function(name=\"norm_avgpool_3_3\")(branch_avgpool_3_3, train=is_train)\n",
    "        branch_avgpool_3_3 = tf.layers.conv1d(inputs=branch_avgpool_3_3, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_3\")\n",
    "\n",
    "        branch_avgpool_5_5 = tf.layers.average_pooling1d(inputs=x_norm, pool_size=5, strides=1, padding=\"same\",\n",
    "                                                         name=\"avgpool_5\")\n",
    "        branch_avgpool_5_5 = norm_function(name=\"norm_avgpool_5_5\")(branch_avgpool_5_5, train=is_train)\n",
    "        branch_avgpool_5_5 = tf.layers.conv1d(inputs=branch_avgpool_5_5, filters=16, kernel_size=1,\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                              padding=\"same\", name=\"conv_avgpool_5\")\n",
    "\n",
    "        output = tf.concat([branch_conv_1_1, branch_conv_3_3, branch_conv_5_5, branch_conv_7_7, branch_maxpool_3_3,\n",
    "                            branch_maxpool_5_5, branch_avgpool_3_3, branch_avgpool_5_5], axis=-1)\n",
    "        return output"
   ],
   "id": "c3345ec5318752a0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:03:09.137199Z",
     "start_time": "2024-07-17T06:03:08.751029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filepaths_noise = glob.glob(os.path.join(get_train_audio_path(), \"_background_noise_\", \"*.wav\"))\n",
    "\n",
    "noise = np.concatenate(list(map(lambda x: read_wav(x, False)[0], filepaths_noise)))\n",
    "noise = np.concatenate([noise, noise[::-1]])\n",
    "synthetic_noise = np.concatenate([white_noise(N=16000 * 30, state=np.random.RandomState(655321)),\n",
    "                                  blue_noise(N=16000 * 30, state=np.random.RandomState(655321)),\n",
    "                                  pink_noise(N=16000 * 30, state=np.random.RandomState(655321)),\n",
    "                                  brown_noise(N=16000 * 30, state=np.random.RandomState(655321)),\n",
    "                                  violet_noise(N=16000 * 30, state=np.random.RandomState(655321)),\n",
    "                                  np.zeros(16000 * 60)])\n",
    "synthetic_noise /= np.max(np.abs(synthetic_noise))\n",
    "synthetic_noise = np.concatenate([synthetic_noise, (synthetic_noise + synthetic_noise[::-1]) / 2])\n",
    "all_noise = np.concatenate([noise, synthetic_noise])"
   ],
   "id": "d9ca5170864a8e47",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the following path does not exist: '../data/speech/train'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m filepaths_noise \u001B[38;5;241m=\u001B[39m glob\u001B[38;5;241m.\u001B[39mglob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_train_audio_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_background_noise_\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*.wav\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      3\u001B[0m noise \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate(\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m x: read_wav(x, \u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m], filepaths_noise)))\n\u001B[1;32m      4\u001B[0m noise \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([noise, noise[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]])\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36m_norm_path.<locals>.normalize_path\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnormalize_path\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mnormpath(path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "Cell \u001B[0;32mIn[4], line 9\u001B[0m, in \u001B[0;36m_assure_path_exists.<locals>.assure_exists\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21massure_exists\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 9\u001B[0m     p\u001B[38;5;241m=\u001B[39mpath(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(p), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe following path does not exist: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "Cell \u001B[0;32mIn[4], line 30\u001B[0m, in \u001B[0;36m_is_input_path.<locals>.check_existence\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;129m@_norm_path\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;129m@_assure_path_exists\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_existence\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[0;32mIn[4], line 45\u001B[0m, in \u001B[0;36mget_train_audio_path\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;129m@_is_input_path\u001B[39m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_train_audio_path\u001B[39m():\n\u001B[0;32m---> 45\u001B[0m     path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_train_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m path\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36m_norm_path.<locals>.normalize_path\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnormalize_path\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mnormpath(path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m, in \u001B[0;36m_assure_path_exists.<locals>.assure_exists\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21massure_exists\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m      9\u001B[0m     p\u001B[38;5;241m=\u001B[39mpath(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(p), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe following path does not exist: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "\u001B[0;31mAssertionError\u001B[0m: the following path does not exist: '../data/speech/train'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:03:22.816911Z",
     "start_time": "2024-07-17T06:03:22.764921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)\n",
    "\n",
    "path = get_silence_path()\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "for noise_clip_no in tqdm(range(8000)):\n",
    "    if noise_clip_no <= 4000:\n",
    "        idx = np.random.randint(0, len(noise) - 16000)\n",
    "        clip = noise[idx:(idx + 16000)]\n",
    "    else:\n",
    "        idx = np.random.randint(0, len(synthetic_noise) - 16000)\n",
    "        clip = synthetic_noise[idx:(idx + 16000)]\n",
    "    wavfile.write(os.path.join(path, \"{0:04d}.wav\".format(noise_clip_no)), 16000,\n",
    "                  ((32767 * clip / np.max(np.abs(clip))).astype(np.int16)))"
   ],
   "id": "58ab5301c027cc43",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m noise_clip_no \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m8000\u001B[39m)):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m noise_clip_no \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4000\u001B[39m:\n\u001B[0;32m---> 11\u001B[0m         idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(noise) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m16000\u001B[39m)\n\u001B[1;32m     12\u001B[0m         clip \u001B[38;5;241m=\u001B[39m noise[idx:(idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m16000\u001B[39m)]\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'noise' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:03:30.856165Z",
     "start_time": "2024-07-17T06:03:30.801982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filepaths = glob.glob(os.path.join(get_train_audio_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths += glob.glob(os.path.join(get_silence_path(), \"**/*.wav\"), recursive=True)\n",
    "filepaths = list(filter(lambda fp: \"_background_noise_\" not in fp, filepaths))\n",
    "validation_list = open(os.path.join(get_train_path(), \"validation_list.txt\")).readlines()\n",
    "test_list = open(os.path.join(get_train_path(), \"testing_list.txt\")).readlines()\n",
    "validation_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), validation_list))\n",
    "testing_list = list(map(lambda fn: os.path.join(get_train_audio_path(), fn.strip()), test_list))\n",
    "training_list = np.setdiff1d(filepaths, validation_list + testing_list).tolist()"
   ],
   "id": "f9cd525d93e83dfb",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the following path does not exist: '../data/speech/train'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m filepaths \u001B[38;5;241m=\u001B[39m glob\u001B[38;5;241m.\u001B[39mglob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_train_audio_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m**/*.wav\u001B[39m\u001B[38;5;124m\"\u001B[39m), recursive\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      2\u001B[0m filepaths \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m glob\u001B[38;5;241m.\u001B[39mglob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_silence_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m**/*.wav\u001B[39m\u001B[38;5;124m\"\u001B[39m), recursive\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m filepaths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mfilter\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m fp: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_background_noise_\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m fp, filepaths))\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36m_norm_path.<locals>.normalize_path\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnormalize_path\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mnormpath(path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "Cell \u001B[0;32mIn[4], line 9\u001B[0m, in \u001B[0;36m_assure_path_exists.<locals>.assure_exists\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21massure_exists\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 9\u001B[0m     p\u001B[38;5;241m=\u001B[39mpath(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(p), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe following path does not exist: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "Cell \u001B[0;32mIn[4], line 30\u001B[0m, in \u001B[0;36m_is_input_path.<locals>.check_existence\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;129m@_norm_path\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;129m@_assure_path_exists\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_existence\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[0;32mIn[4], line 45\u001B[0m, in \u001B[0;36mget_train_audio_path\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;129m@_is_input_path\u001B[39m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_train_audio_path\u001B[39m():\n\u001B[0;32m---> 45\u001B[0m     path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_train_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m path\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36m_norm_path.<locals>.normalize_path\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnormalize_path\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mnormpath(path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m, in \u001B[0;36m_assure_path_exists.<locals>.assure_exists\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21massure_exists\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m      9\u001B[0m     p\u001B[38;5;241m=\u001B[39mpath(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(p), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe following path does not exist: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "\u001B[0;31mAssertionError\u001B[0m: the following path does not exist: '../data/speech/train'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:03:38.422368Z",
     "start_time": "2024-07-17T06:03:38.408103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "random.seed(655321)\n",
    "random.shuffle(filepaths)\n",
    "random.shuffle(validation_list)\n",
    "random.shuffle(testing_list)\n",
    "random.shuffle(training_list)"
   ],
   "id": "cd79a63b2d01890d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m random\u001B[38;5;241m.\u001B[39mseed(\u001B[38;5;241m655321\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m random\u001B[38;5;241m.\u001B[39mshuffle(filepaths)\n\u001B[1;32m      3\u001B[0m random\u001B[38;5;241m.\u001B[39mshuffle(validation_list)\n\u001B[1;32m      4\u001B[0m random\u001B[38;5;241m.\u001B[39mshuffle(testing_list)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filepaths' is not defined"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:03:53.711672Z",
     "start_time": "2024-07-17T06:03:53.690186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "assert all(map(lambda fp: os.path.splitext(fp)[1] == \".wav\", filepaths))\n",
    "assert len(filepaths) == 64727 - 6 + 8000\n",
    "assert len(training_list) == len(filepaths) - 6798 - 6835\n",
    "assert len(validation_list) == 6798\n",
    "assert len(testing_list) == 6835\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), validation_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), testing_list))\n",
    "assert all(map(lambda fn: os.path.exists(os.path.join(fn)), training_list))\n",
    "assert set(validation_list + testing_list + training_list) == set(filepaths)\n",
    "assert len(np.intersect1d(validation_list, testing_list)) == 0\n",
    "assert len(np.intersect1d(training_list, testing_list)) == 0\n",
    "assert len(np.intersect1d(training_list, validation_list)) == 0"
   ],
   "id": "f8d516c9da0de407",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m fp: os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplitext(fp)[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.wav\u001B[39m\u001B[38;5;124m\"\u001B[39m, filepaths))\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(filepaths) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m64727\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m6\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m8000\u001B[39m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(training_list) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(filepaths) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m6798\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m6835\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filepaths' is not defined"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:04:01.475829Z",
     "start_time": "2024-07-17T06:04:01.459961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cardinal_classes = list(set(map(lambda fp: os.path.split(os.path.split(fp)[0])[1], filepaths)))\n",
    "le_classes = LabelEncoder().fit(cardinal_classes)\n",
    "Counter(map(lambda fp: os.path.split(os.path.split(fp)[0])[1], filepaths))"
   ],
   "id": "867a3f307deb9882",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cardinal_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m fp: os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(fp)[\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m1\u001B[39m], filepaths)))\n\u001B[1;32m      2\u001B[0m le_classes \u001B[38;5;241m=\u001B[39m LabelEncoder()\u001B[38;5;241m.\u001B[39mfit(cardinal_classes)\n\u001B[1;32m      3\u001B[0m Counter(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m fp: os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(fp)[\u001B[38;5;241m0\u001B[39m])[\u001B[38;5;241m1\u001B[39m], filepaths))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filepaths' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:04:19.726140Z",
     "start_time": "2024-07-17T06:04:19.700980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_gen_test = get_batcher(filepaths, 1000)\n",
    "batch_a_wav, batch_a_target = next(_gen_test)\n",
    "batch_b_wav, batch_b_target = next(_gen_test)\n",
    "_gen_test_le = get_batcher(filepaths, 1000, label_encoder=le_classes)\n",
    "batch_le_wav, batch_le_target = next(_gen_test_le)\n",
    "\n",
    "assert batch_a_wav.shape == (1000, 16000, 1)\n",
    "assert batch_le_wav.shape == (1000, 16000, 1)\n",
    "assert batch_a_wav.shape == batch_b_wav.shape == batch_le_wav.shape\n",
    "assert np.sum(np.abs(batch_a_wav - batch_b_wav)) != 0\n",
    "assert len(batch_a_target) == len(batch_b_target) == len(batch_le_target)\n",
    "assert any(batch_a_target != batch_b_target)\n",
    "assert all(batch_le_target == np.expand_dims(le_classes.transform(np.squeeze(batch_a_target)), 1))"
   ],
   "id": "18f2f9033fc8cf29",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filepaths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m _gen_test \u001B[38;5;241m=\u001B[39m get_batcher(filepaths, \u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m      2\u001B[0m batch_a_wav, batch_a_target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(_gen_test)\n\u001B[1;32m      3\u001B[0m batch_b_wav, batch_b_target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(_gen_test)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'filepaths' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:05:09.159045Z",
     "start_time": "2024-07-17T06:05:09.143784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NameSpacer:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "\n",
    "class Architecture:\n",
    "    def __init__(self, class_cardinality, seq_len=16000, name=\"architecture\"):\n",
    "        self.seq_len = seq_len\n",
    "        self.class_cardinality = class_cardinality\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.name = name\n",
    "        self.define_computation_graph()\n",
    "\n",
    "        #Aliases\n",
    "        self.ph = self.placeholders\n",
    "        self.op = self.optimizers\n",
    "        self.summ = self.summaries\n",
    "\n",
    "    def define_computation_graph(self):\n",
    "        # Reset graph\n",
    "        tf.reset_default_graph()\n",
    "        self.placeholders = NameSpacer(**self.define_placeholders())\n",
    "        self.core_model = NameSpacer(**self.define_core_model())\n",
    "        self.losses = NameSpacer(**self.define_losses())\n",
    "        self.optimizers = NameSpacer(**self.define_optimizers())\n",
    "        self.summaries = NameSpacer(**self.define_summaries())\n",
    "\n",
    "    def define_placeholders(self):\n",
    "        with tf.variable_scope(\"Placeholders\"):\n",
    "            wav_in = tf.placeholder(dtype=tf.float32, shape=(None, self.seq_len, 1), name=\"wav_in\")\n",
    "            is_train = tf.placeholder(dtype=tf.bool, shape=None, name=\"is_train\")\n",
    "            target = tf.placeholder(dtype=tf.int32, shape=(None, 1), name=\"target\")\n",
    "            acc_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"acc_dev\")\n",
    "            loss_dev = tf.placeholder(dtype=tf.float32, shape=None, name=\"loss_dev\")\n",
    "            return ({\"wav_in\": wav_in, \"target\": target, \"is_train\": is_train, \"acc_dev\":\n",
    "                acc_dev, \"loss_dev\": loss_dev})\n",
    "\n",
    "    def define_core_model(self):\n",
    "        with tf.variable_scope(\"Core_Model\"):\n",
    "            x = inception_1d(x=self.placeholders.wav_in, is_train=self.placeholders.is_train,\n",
    "                             norm_function=BatchNorm, activ_function=tf.nn.relu, depth=1,\n",
    "                             name=\"Inception_1_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_1_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=1, name=\"Inception_2_3\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_2\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_3_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_3\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=2, name=\"Inception_4_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_4\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_5_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_5\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=3, name=\"Inception_6_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_6\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_7_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_7\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_8_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_8\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_1\")\n",
    "            x = inception_1d(x=x, is_train=self.placeholders.is_train, norm_function=BatchNorm,\n",
    "                             activ_function=tf.nn.relu, depth=4, name=\"Inception_9_2\")\n",
    "            x = tf.layers.max_pooling1d(x, 2, 2, name=\"maxpool_9\")\n",
    "            x = tf.contrib.layers.flatten(x)\n",
    "            x = tf.layers.dense(BatchNorm(name=\"bn_dense_1\")(x, train=self.placeholders.is_train),\n",
    "                                128, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"dense_1\")\n",
    "            output = tf.layers.dense(BatchNorm(name=\"bn_dense_2\")(x, train=self.placeholders.is_train),\n",
    "                                     self.class_cardinality, activation=None,\n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                     name=\"output\")\n",
    "            return ({\"output\": output})\n",
    "\n",
    "    def define_losses(self):\n",
    "        with tf.variable_scope(\"Losses\"):\n",
    "            softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(self.placeholders.target),\n",
    "                                                                        logits=self.core_model.output,\n",
    "                                                                        name=\"softmax\")\n",
    "            return ({\"softmax\": softmax_ce})\n",
    "\n",
    "    def define_optimizers(self):\n",
    "        with tf.variable_scope(\"Optimization\"):\n",
    "            op = self.optimizer.minimize(self.losses.softmax)\n",
    "            return ({\"op\": op})\n",
    "\n",
    "    def define_summaries(self):\n",
    "        with tf.variable_scope(\"Summaries\"):\n",
    "            ind_max = tf.squeeze(tf.cast(tf.argmax(self.core_model.output, axis=1), tf.int32))\n",
    "            target = tf.squeeze(self.placeholders.target)\n",
    "            acc = tf.reduce_mean(tf.cast(tf.equal(ind_max, target), tf.float32))\n",
    "            loss = tf.reduce_mean(self.losses.softmax)\n",
    "            train_scalar_probes = {\"accuracy\": acc,\n",
    "                                   \"loss\": loss}\n",
    "            train_performance_scalar = [tf.summary.scalar(k, tf.reduce_mean(v), family=self.name)\n",
    "                                        for k, v in train_scalar_probes.items()]\n",
    "            train_performance_scalar = tf.summary.merge(train_performance_scalar)\n",
    "\n",
    "            dev_scalar_probes = {\"acc_dev\": self.placeholders.acc_dev,\n",
    "                                 \"loss_dev\": self.placeholders.loss_dev}\n",
    "            dev_performance_scalar = [tf.summary.scalar(k, v, family=self.name) for k, v in dev_scalar_probes.items()]\n",
    "            dev_performance_scalar = tf.summary.merge(dev_performance_scalar)\n",
    "            return ({\"accuracy\": acc, \"loss\": loss, \"s_tr\": train_performance_scalar, \"s_de\": dev_performance_scalar})"
   ],
   "id": "afe10bd02939e302",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:05:15.128232Z",
     "start_time": "2024-07-17T06:05:15.114221Z"
    }
   },
   "cell_type": "code",
   "source": "net = Architecture(class_cardinality=len(cardinal_classes), name=\"wavception\")",
   "id": "e5c2c628b24142c3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cardinal_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m net \u001B[38;5;241m=\u001B[39m Architecture(class_cardinality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(cardinal_classes), name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwavception\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cardinal_classes' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:05:21.329031Z",
     "start_time": "2024-07-17T06:05:21.306883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sess = start_tensorflow_session(device=\"1\")\n",
    "sw = get_summary_writer(sess, \"~/.logs_tensorboard/\", \"wavception\", \"V1\")\n",
    "c = 0"
   ],
   "id": "d90338c2bc4bb0cc",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m sess \u001B[38;5;241m=\u001B[39m start_tensorflow_session(device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m sw \u001B[38;5;241m=\u001B[39m get_summary_writer(sess, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m~/.logs_tensorboard/\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwavception\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mV1\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# Adjust your tensorboard logs path here\u001B[39;00m\n\u001B[1;32m      3\u001B[0m c \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "Cell \u001B[0;32mIn[3], line 11\u001B[0m, in \u001B[0;36mstart_tensorflow_session\u001B[0;34m(device, memory_fraction)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstart_tensorflow_session\u001B[39m(device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m, memory_fraction\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (tf\u001B[38;5;241m.\u001B[39mSession(config\u001B[38;5;241m=\u001B[39mget_tensorflow_configuration(device\u001B[38;5;241m=\u001B[39mdevice, memory_fraction\u001B[38;5;241m=\u001B[39mmemory_fraction)))\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:05:31.845807Z",
     "start_time": "2024-07-17T06:05:31.832265Z"
    }
   },
   "cell_type": "code",
   "source": "sess.run(tf.global_variables_initializer())",
   "id": "f4e1ce70de1aac75",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m sess\u001B[38;5;241m.\u001B[39mrun(tf\u001B[38;5;241m.\u001B[39mglobal_variables_initializer())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sess' is not defined"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:05:39.803355Z",
     "start_time": "2024-07-17T06:05:39.801058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(655321)\n",
    "random.seed(655321)"
   ],
   "id": "4bf974e5f26f45d5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:05:52.457620Z",
     "start_time": "2024-07-17T06:05:52.429789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(50000):\n",
    "    random.shuffle(training_list)\n",
    "    batcher = get_batcher(training_list, 16, le_classes)\n",
    "    for i, (batch_x, batch_y) in enumerate(batcher):\n",
    "        _, loss, acc, s = sess.run([net.op.op, net.losses.softmax, net.summ.accuracy, net.summ.s_tr],\n",
    "                                   feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y,\n",
    "                                              net.ph.is_train: True})\n",
    "        print(\"[{0:04d}|{1:04d}] Accuracy train: {2:.2f}%\".format(epoch, i, acc * 100))\n",
    "        sw.add_summary(s, c)\n",
    "\n",
    "        if c % 1000 == 0:\n",
    "            accuracies_dev = []\n",
    "            losses_dev = []\n",
    "            batcher = get_batcher(validation_list, 16, le_classes)\n",
    "            for i, (batch_x, batch_y) in enumerate(batcher):\n",
    "                acc, loss = sess.run([net.summ.accuracy, net.summ.loss],\n",
    "                                     feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y,\n",
    "                                                net.ph.is_train: False})\n",
    "                accuracies_dev.append(acc)\n",
    "                losses_dev.append(loss)\n",
    "            s = sess.run(net.summ.s_de, feed_dict={net.ph.acc_dev: np.mean(accuracies_dev),\n",
    "                                                   net.ph.loss_dev: np.mean(losses_dev)})\n",
    "            sw.add_summary(s, c)\n",
    "        c += 1"
   ],
   "id": "1d5ca9add3ce69e8",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m50000\u001B[39m):\n\u001B[0;32m----> 2\u001B[0m     random\u001B[38;5;241m.\u001B[39mshuffle(training_list)\n\u001B[1;32m      3\u001B[0m     batcher \u001B[38;5;241m=\u001B[39m get_batcher(training_list, \u001B[38;5;241m16\u001B[39m, le_classes)\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (batch_x, batch_y) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batcher):\n",
      "\u001B[0;31mNameError\u001B[0m: name 'training_list' is not defined"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:06:01.509944Z",
     "start_time": "2024-07-17T06:06:01.494645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracies = []\n",
    "batcher = get_batcher(testing_list, 64, le_classes)\n",
    "for i, (batch_x, batch_y) in tqdm(enumerate(batcher)):\n",
    "    acc = sess.run(net.summ.accuracy, feed_dict={net.ph.wav_in: batch_x, net.ph.target: batch_y,\n",
    "                                                 net.ph.is_train: False})\n",
    "    accuracies.append(acc)"
   ],
   "id": "b58a96b2fcb819b1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testing_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 2\u001B[0m batcher \u001B[38;5;241m=\u001B[39m get_batcher(testing_list, \u001B[38;5;241m64\u001B[39m, le_classes)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (batch_x, batch_y) \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(batcher)):\n\u001B[1;32m      4\u001B[0m     acc \u001B[38;5;241m=\u001B[39m sess\u001B[38;5;241m.\u001B[39mrun(net\u001B[38;5;241m.\u001B[39msumm\u001B[38;5;241m.\u001B[39maccuracy, feed_dict\u001B[38;5;241m=\u001B[39m{net\u001B[38;5;241m.\u001B[39mph\u001B[38;5;241m.\u001B[39mwav_in: batch_x, net\u001B[38;5;241m.\u001B[39mph\u001B[38;5;241m.\u001B[39mtarget: batch_y,\n\u001B[1;32m      5\u001B[0m                                                  net\u001B[38;5;241m.\u001B[39mph\u001B[38;5;241m.\u001B[39mis_train: \u001B[38;5;28;01mFalse\u001B[39;00m})\n",
      "\u001B[0;31mNameError\u001B[0m: name 'testing_list' is not defined"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:06:08.763280Z",
     "start_time": "2024-07-17T06:06:08.719510Z"
    }
   },
   "cell_type": "code",
   "source": "scoring_list = glob.glob(os.path.join(get_scoring_audio_path(), \"*.wav\"), recursive=True)",
   "id": "e18ce413b2ce9037",
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "the following path does not exist: '../data/speech/test'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m scoring_list \u001B[38;5;241m=\u001B[39m glob\u001B[38;5;241m.\u001B[39mglob(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_scoring_audio_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*.wav\u001B[39m\u001B[38;5;124m\"\u001B[39m), recursive\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36m_norm_path.<locals>.normalize_path\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnormalize_path\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mnormpath(path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "Cell \u001B[0;32mIn[4], line 9\u001B[0m, in \u001B[0;36m_assure_path_exists.<locals>.assure_exists\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21massure_exists\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 9\u001B[0m     p\u001B[38;5;241m=\u001B[39mpath(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(p), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe following path does not exist: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "Cell \u001B[0;32mIn[4], line 30\u001B[0m, in \u001B[0;36m_is_input_path.<locals>.check_existence\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;129m@_norm_path\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;129m@_assure_path_exists\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcheck_existence\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 30\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[0;32mIn[4], line 50\u001B[0m, in \u001B[0;36mget_scoring_audio_path\u001B[0;34m()\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;129m@_is_input_path\u001B[39m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_scoring_audio_path\u001B[39m():\n\u001B[0;32m---> 50\u001B[0m     path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_test_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m path\n",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m, in \u001B[0;36m_norm_path.<locals>.normalize_path\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnormalize_path\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mnormpath(path(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n",
      "Cell \u001B[0;32mIn[4], line 10\u001B[0m, in \u001B[0;36m_assure_path_exists.<locals>.assure_exists\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21massure_exists\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m      9\u001B[0m     p\u001B[38;5;241m=\u001B[39mpath(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 10\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(p), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe following path does not exist: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(p)\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p\n",
      "\u001B[0;31mAssertionError\u001B[0m: the following path does not exist: '../data/speech/test'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:06:14.785059Z",
     "start_time": "2024-07-17T06:06:14.771386Z"
    }
   },
   "cell_type": "code",
   "source": "batcher = get_batcher(scoring_list, 80, le_classes, scoring=True)",
   "id": "bd3e7f9316f8984d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scoring_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m batcher \u001B[38;5;241m=\u001B[39m get_batcher(scoring_list, \u001B[38;5;241m80\u001B[39m, le_classes, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'scoring_list' is not defined"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:06:21.272337Z",
     "start_time": "2024-07-17T06:06:21.254855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fns = []\n",
    "prds = []\n",
    "for i, (batch_x, filepaths) in tqdm(enumerate(batcher)):\n",
    "    pred = sess.run(net.core_model.output, feed_dict={net.ph.wav_in: batch_x, net.ph.is_train: False})\n",
    "    fns.extend(map(lambda f: os.path.split(f)[1], filepaths))\n",
    "    prds.extend(map(lambda f: np.argmax(pred, axis=1).tolist(), pred))"
   ],
   "id": "eef88ed12c460d2c",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batcher' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m fns \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      2\u001B[0m prds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (batch_x, filepaths) \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(batcher)):\n\u001B[1;32m      4\u001B[0m     pred \u001B[38;5;241m=\u001B[39m sess\u001B[38;5;241m.\u001B[39mrun(net\u001B[38;5;241m.\u001B[39mcore_model\u001B[38;5;241m.\u001B[39moutput, feed_dict\u001B[38;5;241m=\u001B[39m{net\u001B[38;5;241m.\u001B[39mph\u001B[38;5;241m.\u001B[39mwav_in: batch_x, net\u001B[38;5;241m.\u001B[39mph\u001B[38;5;241m.\u001B[39mis_train: \u001B[38;5;28;01mFalse\u001B[39;00m})\n\u001B[1;32m      5\u001B[0m     fns\u001B[38;5;241m.\u001B[39mextend(\u001B[38;5;28mmap\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m f: os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(f)[\u001B[38;5;241m1\u001B[39m], filepaths))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'batcher' is not defined"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-17T06:06:27.682406Z",
     "start_time": "2024-07-17T06:06:27.664834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame({\"fname\": fns, \"label\": prds})\n",
    "df.label = le_classes.inverse_transform(df.label)\n",
    "df.loc[~df.label.isin(\n",
    "    [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"silence\"]), \"label\"] = \"unknown\"\n",
    "df.to_csv(os.path.join(get_submissions_path(), \"submission.csv\"), index=False)"
   ],
   "id": "47629e7df4a842f3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'le_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfname\u001B[39m\u001B[38;5;124m\"\u001B[39m: fns, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m: prds})\n\u001B[0;32m----> 2\u001B[0m df\u001B[38;5;241m.\u001B[39mlabel \u001B[38;5;241m=\u001B[39m le_classes\u001B[38;5;241m.\u001B[39minverse_transform(df\u001B[38;5;241m.\u001B[39mlabel)\n\u001B[1;32m      3\u001B[0m df\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;241m~\u001B[39mdf\u001B[38;5;241m.\u001B[39mlabel\u001B[38;5;241m.\u001B[39misin(\n\u001B[1;32m      4\u001B[0m     [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myes\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mup\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdown\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgo\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msilence\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munknown\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m df\u001B[38;5;241m.\u001B[39mto_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(get_submissions_path(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubmission.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m), index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'le_classes' is not defined"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
